[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cao XuÃ¢n Lá»™c",
    "section": "",
    "text": "Xin chÃ o, mÃ¬nh lÃ  Lá»™c, sinh nÄƒm 2003 vÃ  lÃ  má»™t chÃ ng trai Ä‘áº¿n tá»« máº£nh Ä‘áº¥t Ä‘áº§y náº¯ng vÃ  giÃ³ - PhÃº YÃªn, Viá»‡t Nam. MÃ¬nh cÃ³ báº±ng cá»­ nhÃ¢n trÆ°á»ng Äáº¡i há»c Kinh Táº¿ - TÃ i ChÃ­nh (UEF) vÃ  chuyÃªn ngÃ nh cá»§a mÃ¬nh lÃ  Logistics vÃ  quáº£n lÃ½ chuá»—i cung á»©ng.\nLÃ  ngÆ°á»i cÃ³ niá»m Ä‘am mÃª máº¡nh máº½ vá»›i R, mÃ¬nh cÃ³ sá»Ÿ thÃ­ch viáº¿t post vá» viá»‡c phÃ¢n tÃ­ch dá»¯ liá»‡u vá»›i R Ä‘á»ƒ á»©ng dá»¥ng vÃ o cÃ¡c cÃ´ng viá»‡c, bÃ i toÃ¡n thÆ°á»ng gáº·p trong Supply Chain. NgoÃ i ra, sá»Ÿ thÃ­ch cá»§a mÃ¬nh lÃ  nghe sÃ¡ch nÃ³i vÃ  Ä‘i bá»™!\nCÃ¢u slogan mÃ  mÃ¬nh thÃ­ch nháº¥t lÃ : â€œDonâ€™t fear the risk, fear the opportunity lost!â€ vÃ  Ä‘Ã³ cÅ©ng lÃ  cÃ¡ch mÃ¬nh sá»‘ng vÃ  lÃ m viá»‡c Ä‘áº¿n bÃ¢y giá» ğŸ’ğŸ’ğŸ’.\nHi vá»ng cÃ¡c báº¡n sáº½ thÃ­ch bÃ i viáº¿t cá»§a mÃ¬nh!\n    \n    \n    Go to Next Page\n    \n    \n        \n            Go to Next Page\n            â”"
  },
  {
    "objectID": "torch.html",
    "href": "torch.html",
    "title": "RNN and LSTM model",
    "section": "",
    "text": "á» Ä‘Ã¢y ta sáº½ há»c vá» mÃ´ hÃ¬nh machine learning Ä‘Æ°á»£c á»©ng dá»¥ng nhiá»u nháº¥t trong viá»‡c phÃ¢n tÃ­ch dá»¯ liá»‡u thá»i gian lÃ  RNN vÃ  LSTM."
  },
  {
    "objectID": "torch.html#mÃ´-hÃ¬nh-rnn",
    "href": "torch.html#mÃ´-hÃ¬nh-rnn",
    "title": "RNN and LSTM model",
    "section": "1 MÃ´ hÃ¬nh RNN:",
    "text": "1 MÃ´ hÃ¬nh RNN:\n\n1.1 Äá»‹nh nghÄ©a:\nÄiá»ƒm chung lÃ  cáº£ hai mÃ´ hÃ¬nh Ä‘á»u thuá»™c phÃ¢n lá»›p Deep learning - nghÄ©a lÃ  há»c mÃ¡y sÃ¢u vá»›i Ä‘áº·c Ä‘iá»ƒm chung lÃ  phÃ¢n chia dá»¯ liá»‡u thÃ nh nhiá»u lá»›p vÃ  báº¯t Ä‘áº§u â€œhá»câ€ dáº§n qua tá»«ng lá»›p Ä‘á»ƒ Ä‘Æ°a ra káº¿t quáº£ cuá»‘i cÃ¹ng. á» hÃ¬nh dÆ°á»›i Ä‘Ã¢y, \\(X_o\\) Ä‘áº¡i diá»‡n cho dá»¯ liá»‡u Ä‘áº§u vÃ o, \\(h_t\\) lÃ  output Ä‘áº§u ra cá»§a tá»«ng step vÃ  \\(A\\) lÃ  nhá»¯ng gÃ¬ Ä‘Ã£ â€œhá»câ€ Ä‘Æ°á»£c táº¡i step Ä‘Ã³ vÃ  Ä‘Æ°á»£c truyá»n cho step tiáº¿p theo. Trong tÃ i liá»‡u chuáº©n thÃ¬ há» thÆ°á»ng kÃ­ hiá»‡u lÃ  \\(X_t\\), \\(Y_t\\), \\(h_{t-1}\\).\n\n  \n  \n  \n  \n    HÃ¬nh 1: Minh há»a vá» sá»± phÃ¢n chia dá»¯ liá»‡u thÃ nh nhiá»u lá»›p\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nKhi nhÃ¬n hÃ¬nh thÃ¬ báº¡n cÃ³ thá»ƒ bá»‘i rá»‘i chÆ°a hiá»ƒu cÃ¡c kÃ­ tá»± vÃ  hÃ¬nh áº£nh thÃ¬ báº¡n cÃ³ thá»ƒ tÆ°á»Ÿng tÆ°á»£ng há»c mÃ¡y nhÆ° 1 Ä‘á»©a tráº» vÃ  Ä‘á»ƒ nÃ³ cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c cÃ¢u: â€œHÃ´m nay con Ä‘i há»câ€ thÃ¬ nÃ³ pháº£i há»c tá»«ng chá»¯ cÃ¡i nhÆ°: a,b,c,â€¦ trÆ°á»›c rÃ²i má»›i ghÃ©p thÃ nh tá»« Ä‘Æ¡n nhÆ°: â€œHÃ´mâ€,â€œNayâ€,â€¦ rá»“i ghÃ©p thÃ nh cÃ¢u trÃªn.\nVáº­y giáº£ sá»­ nhÆ° hÃ´m nay há»c Ä‘Æ°á»£c tá»« â€œHÃ´mâ€ thÃ¬ nÃ³ sáº½ báº¯t Ä‘áº§u ghi nhá»› tá»« Ä‘Ã£ há»c vÃ o trong \\(A\\). Náº¿u sau nÃ y ta cáº§n há»c mÃ¡y hiá»ƒu cÃ¢u â€œHÃ´m sau con Ä‘i chÆ¡iâ€ thÃ¬ tá»‘c Ä‘á»™ há»c cá»§a há»c mÃ¡y sáº½ nhanh lÃªn vÃ¬ thay vÃ¬ nÃ³ pháº£i há»c 5 chá»¯ Ä‘Æ¡n nhÆ° thÃ´ng thÆ°á»ng thÃ¬ nÃ³ chá»‰ cáº§n há»c 4 chá»¯ cÃ²n láº¡i trá»« chá»¯ â€œhÃ´mâ€. Váº­y báº¡n Ä‘Ã£ hiá»ƒu Ã½ tÆ°á»Ÿng ná»n táº£ng cá»§a RNN rá»“i ha!\n\n\n1.2 NguyÃªn lÃ­ hoáº¡t Ä‘á»™ng:\nÄáº§u tiÃªn, RNN sáº½ tÃ­nh toÃ¡n hidden state lÃ  \\(h_t\\) vá»›i cÃ´ng thá»©c lÃ :\n\\[\n   \\mathbf{h}_t = \\text{activation}(\\mathbf{W}_\\text{hh} \\mathbf{h}_{t-1} + \\mathbf{W}_\\text{xh} \\mathbf{x}_t + \\mathbf{b}_\\text{h})\n\\]\nSau Ä‘Ã³, \\(h_t\\) sáº½ Ä‘Æ°á»£c lÃ m input cho cÃ¡c state sau vÃ  dá»±a vÃ o Ä‘Ã³ Ä‘á»ƒ tÃ­nh output vá»›i cÃ´ng thá»©c lÃ :\n\\[\ny_t = W_y \\cdot h_t + b_y\n\\]\nVÃ­ dá»¥: MÃ¬nh muá»‘n dá»± Ä‘oÃ¡n hÃ nh Ä‘á»™ng trong cÃ¢u nÃ³i â€œI am reading bookâ€ báº±ng mÃ´ hÃ¬nh RNN nhÆ° sau:\n\nBÆ°á»›c 1: Chuyá»ƒn Ä‘á»•i thÃ nh dáº¡ng sá»‘ báº±ng embedding layer:\n\nMÃ¬nh sáº½ gÃ¡n tá»«ng tá»« Ä‘Æ¡n sang dáº¡ng sá»‘ nhÆ°:\n\nâ€œIâ€ -&gt; \\(x_1\\)\nâ€œamâ€ -&gt; \\(x_2\\)\nâ€œreadingâ€ -&gt; \\(x_3\\)\nâ€œbookâ€ -&gt; \\(x_4\\)\nBÆ°á»›c 2: ThÃªm hidden layer vÃ  báº¯t Ä‘áº§u tÃ­nh toÃ¡n:\n\nCho input: â€œIâ€\n\\[\n   h_1 = \\tanh(W_x \\cdot x_1 + W_h \\cdot h_0 + b)\n\\]\nCho input: â€œamâ€\n\\[\n   h_2 = \\tanh(W_x \\cdot x_2 + W_h \\cdot h_1 + b)\n\\]\nCho input: â€œreadingâ€\n\\[\n   h_3 = \\tanh(W_x \\cdot x_3 + W_h \\cdot h_2 + b)\n\\]\nCho input: â€œbookâ€\n\\[\n   h_4 = \\tanh(W_x \\cdot x_4 + W_h \\cdot h_3 + b)\n\\]\n\nBÆ°á»›c 3: TÃ­nh toÃ¡n output: DÃ¹ng hÃ m activation softmax Ä‘á»ƒ phÃ¢n lá»›p theo xÃ¡c suáº¥t.\n\n\\[\n\\hat{y} = \\text{softmax}(W_y \\cdot h_4 + b_y)\n\\]\nNáº¿u muá»‘n hiá»ƒu thÃªm vá» cÃ¡ch hoáº¡t Ä‘á»™ng RNN, báº¡n cÃ³ thá»ƒ tham kháº£o link nÃ y: Recurrent Neural Network: Tá»« RNN Ä‘áº¿n LSTM.\n\n\n1.3 Váº¥n Ä‘á» lá»›n cá»§a RNN:\nRNN cÃ³ 1 váº¥n Ä‘á» lá»›n lÃ  Vanishing Gradient nghÄ©a lÃ  mÃ´ hÃ¬nh sáº½ khÃ´ng cÃ²n â€œhá»câ€ thÃªm Ä‘Æ°á»£c ná»¯a cho dÃ¹ tÄƒng sá»‘ epochs. NguyÃªn nhÃ¢n vÃ¬ sao nhÆ° váº­y thÃ¬ báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n chá»©ng minh cá»§a anh Tuáº¥n.\n\n  \n  \n  \n  \n    HÃ¬nh 2: Vanishing Gradient Problem\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nVáº¥n Ä‘á» nÃ y sáº½ lÃ m network khÃ³ update weight dáº«n tá»›i thá»i gian há»c lÃ¢u vÃ  khÃ³ Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c output. Báº¡n cÃ³ thá»ƒ hiá»ƒu Ä‘Æ¡n giáº£n nhÆ° viá»‡c báº¡n há»c liÃªn tá»¥c dáº«n tá»›i quÃ¡ táº£i vÃ  RNN cÅ©ng khÃ´ng nhÆ° váº­y. Do Ä‘Ã³, RNN chá»‰ há»c cÃ¡c thÃ´ng tin tá»« state gáº§n vÃ  Ä‘Ã³ lÃ  lÃ­ do ra Ä‘á»i LSTM - Long short term memory.\n\n\n\n\n\n\nWarning\n\n\n\nLÆ°u Ã½: Äiá»u nÃ y khÃ´ng cÃ³ nghÄ©a LSTM luÃ´n tá»‘t hÆ¡n RNN vÃ¬ cÃ³ nhá»¯ng bÃ i toÃ¡n vá»›i Ä‘áº§u vÃ o Ä‘Æ¡n giáº£n thÃ¬ mÃ´ hÃ¬nh chá»‰ cáº§n há»c cÃ¡c step Ä‘áº§u lÃ  Ä‘Ã£ â€œhá»câ€ Ä‘áº§y Ä‘á»§ thÃ´ng tin cáº§n thiáº¿t. MÃ´ hÃ¬nh LSTM phá»• biáº¿n vá»›i cÃ¡c bÃ i toÃ¡n phá»©c táº¡p nhÆ° tá»± Ä‘á»™ng dá»‹ch ngÃ´n ngá»¯, ghi chÃ©p láº¡i theo giá»ng nÃ³iâ€¦\n\n\n\n\n1.4 MÃ´ hÃ¬nh LSTM:\nCÃ³ thá»ƒ xem mÃ´ hÃ¬nh LSTM nhÆ° biáº¿n thá»ƒ cá»§a RNN. Vá» cáº¥u trÃºc, LSTM phá»©c táº¡p hÆ¡n RNN:\n\n  \n  \n  \n  \n    HÃ¬nh 3: So sÃ¡nh mÃ´ hÃ¬nh RNN vÃ  LSTM\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nCáº¥u trÃºc cÆ¡ báº£n gá»“m:\n\nCá»•ng quÃªn (Forget Gate): cÃ³ tÃ¡c dá»¥ng quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o cáº§n bá»‹ quÃªn trong tráº¡ng thÃ¡i Ã´ nhá»›.\nCá»•ng nháº­p (Input Gate): XÃ¡c Ä‘á»‹nh thÃ´ng tin nÃ o cáº§n Ä‘Æ°á»£c ghi vÃ o tráº¡ng thÃ¡i Ã´ nhá»›.\nCá»•ng xuáº¥t (Output Gate): Quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o sáº½ Ä‘Æ°á»£c xuáº¥t ra tá»« tráº¡ng thÃ¡i Ã´ nhá»› Ä‘á»ƒ áº£nh hÆ°á»Ÿng Ä‘áº¿n dá»± Ä‘oÃ¡n tiáº¿p theo.\n\nBáº¡n cÃ³ thá»ƒ kham kháº£o thÃªm bÃ i viáº¿t cá»§a dominhhai vá» cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a RNN vÃ  LSTM Ä‘á»ƒ hiá»ƒu thÃªm.\nTiáº¿p theo, ta sáº½ báº¯t Ä‘áº§u xÃ¢y dá»±ng thá»­ mÃ´ hÃ¬nh trong R."
  },
  {
    "objectID": "torch.html#xÃ¢y-dá»±ng-mÃ´-hÃ¬nh",
    "href": "torch.html#xÃ¢y-dá»±ng-mÃ´-hÃ¬nh",
    "title": "RNN and LSTM model",
    "section": "2 XÃ¢y dá»±ng mÃ´ hÃ¬nh:",
    "text": "2 XÃ¢y dá»±ng mÃ´ hÃ¬nh:\n\n2.1 Load dá»¯ liá»‡u:\nÄáº§u tiÃªn ta sáº½ load dá»¯ liá»‡u láº¡i nhÆ° trÆ°á»›c. á» Ä‘Ã¢y, Ä‘á»ƒ Ä‘Æ¡n giáº£n, mÃ¬nh chá»‰ xÃ¢y dá»±ng mÃ´ hÃ¬nh cho product A thÃ´i.\nGiáº£ sá»­ cÃ´ng ty mÃ¬nh Ä‘ang kinh doanh 3 loáº¡i máº·t hÃ ng product A,product B,product C vÃ  Ä‘Ã¢y lÃ  biá»ƒu Ä‘á»“ thá»ƒ hiá»‡n nhu cáº§u cá»§a cáº£ 3 máº·t hÃ ng tá»« thÃ¡ng 5 tá»›i thÃ¡ng 10.\n\n\nCode\nlibrary(highcharter)\nsales_data |&gt; \n  select(-Weekday) |&gt; \n  pivot_longer(cols = c(Product_A, Product_B, Product_C),\n               names_to = \"Product\",\n               values_to = \"Sales\") |&gt; \n  hchart(\"line\", hcaes(x = Date, y = Sales, group = Product))\n\n\n\n\n\n\nNáº¿u ta phÃ¢n tich sÃ¢u vá» nhu cáº§u cá»§a tá»«ng máº·t hÃ ng theo thá»© trong tuáº§n, ta sáº½ tháº¥y ráº±ng máº·t hÃ ng A, B thÃ¬ bÃ¡n cháº¡y vÃ o thá»© 4 vÃ  thá»© 7, cÃ²n máº·t hÃ ng C thÃ¬ bÃ¡n cháº¡y vÃ o thá»© 2 vÃ  thá»© 3.\n\nProduct A:Product B:Product C:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThÃ´ng thÆ°á»ng dá»¯ liá»‡u Ä‘á»ƒ train model trong machine learning thÆ°á»ng cáº§n tráº£i qua bÆ°á»›c normalize data nghÄ©a lÃ  Ä‘Æ°a táº¥t cáº£ dá»¯ liá»‡u vá» chung 1 thÆ°á»›c Ä‘o vÃ  pháº¡m vi. NguyÃªn do vÃ¬ Ä‘iá»u nÃ y giÃºp nhiá»u thuáº­t toÃ¡n há»c mÃ¡y dá»… dÃ ng há»™i tá»¥ hÆ¡n. VÃ­ dá»¥, cÃ¡c thuáº­t toÃ¡n nhÆ° k-Nearest Neighbors (KNN) vÃ  Support Vector Machines (SVM) ráº¥t nháº¡y cáº£m vá»›i khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u nÃªn náº¿u dá»¯ liá»‡u khÃ´ng Ä‘Æ°á»£c chuáº©n hÃ³a, thuáº­t toÃ¡n cÃ³ thá»ƒ Æ°u tiÃªn cÃ¡c Ä‘áº·c trÆ°ng cÃ³ pháº¡m vi lá»›n hÆ¡n vÃ  bá» qua cÃ¡c Ä‘áº·c trÆ°ng cÃ³ pháº¡m vi nhá» hÆ¡n, dáº«n Ä‘áº¿n hiá»‡u suáº¥t kÃ©m. VÃ  cÃ´ng thá»©c phá»• biáº¿n nháº¥t cho chuáº©n hÃ³a lÃ :\n\\[\n\\text{Normalized Value} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n\\]\n\n\nCode\n# Create a data frame with the adjusted sales data\nsales_data &lt;- data.frame(\n  Date = dates,\n  Weekday = weekdays,\n  Product_A = product_a_sales,\n  Product_B = product_b_sales,\n  Product_C = product_c_sales\n)\n\n# Convert the sales data to a time series (ts) object for Product A\nproduct_a_ts &lt;- ts(sales_data$Product_A, start = c(2024, 5), \n                   frequency = 365)\n                   \n\n# Normalzie data:\ntime_series_data&lt;-scale(product_a_ts)\n\nlibrary(highcharter)\nhighchart() |&gt;\n  hc_add_series(data = as.numeric(time_series_data), type = \"line\", name = \"Sales of Product A\") |&gt;\n  hc_title(text = \"Normalized Time Series of Product A\") |&gt;\n  hc_xAxis(title = list(text = \"Date\")) |&gt;\n  hc_yAxis(title = list(text = \"Normalized Sales\")) |&gt;\n  hc_tooltip(shared = TRUE) |&gt;\n  hc_plotOptions(line = list(marker = list(enabled = FALSE)))\n\n\n\n\n\n\n\n\n2.2 Chia dá»¯ liá»‡u:\nVáº­y Ä‘á»ƒ train data, mÃ¬nh sáº½ chia bá»™ dá»¯ liá»‡u thÃ nh 3 pháº§n:\n\nTraining data: dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n vÃ  xÃ¢y dá»±ng mÃ´ hÃ¬nh.\nEvaluating data: Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh vá»«a huáº¥n luyá»‡n.\nTesting data: dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ láº¡i náº¿u muá»‘n mÃ´ hÃ¬nh há»c láº¡i dá»¯ liá»‡u\n\n\n\nCode\nsales_data &lt;- data.frame(\n  Date = dates,\n  Weekday = weekdays,\n  Product_A = product_a_sales,\n  Product_B = product_b_sales,\n  Product_C = product_c_sales\n)\n\n# Convert the sales data to a time series (ts) object for Product A\ntime_series_data &lt;- scale(ts(sales_data$Product_A, start = c(2024, 5), \n                   frequency = 365))\n                   \n\ncreate_supervised_data &lt;- function(series, n) {\n  series &lt;- as.vector(series)  # Convert time series object to vector\n  data &lt;- data.frame(series)    # Initialize data frame with the original series\n  \n  # Create lag columns\n  for (i in 1:n) {\n    lagged_column &lt;- lag(series, i)  # Get lagged values\n    data &lt;- cbind(data, lagged_column)  # Add lagged column to the data\n  }\n  \n  # Name the columns properly\n  colnames(data) &lt;- c(paste0('t-', n:1), 't+1')\n  \n  # Remove rows with NA values (those at the start of the series due to lagging)\n  data &lt;- na.omit(data)\n  \n  return(data)\n}\n\n# Prepare the data with 12 input lags and 1 output (next time step)\nsupervised_data &lt;- create_supervised_data(time_series_data,\n                                          n = 50)\n\n# Step 2: Split data into training and test sets\ntrain_size &lt;- round(0.7 * nrow(supervised_data))   # 70% for training\nval_size &lt;- round(0.1 * nrow(supervised_data))     # 10% for validation\ntest_size &lt;- nrow(supervised_data) - train_size - val_size  # 20% for testing\n\ntrain_data &lt;- supervised_data[1:train_size, ]\nval_data &lt;- supervised_data[(train_size + 1):(train_size + val_size), ]\ntest_data &lt;- supervised_data[(train_size + val_size + 1):nrow(supervised_data), ]\n\n# Correct column selection\nx_train &lt;- as.matrix(train_data[, 1:50])  # Input features (12 lags)\ny_train &lt;- as.matrix(train_data[, 't+1'])  # Target output (next time step)\n\nx_val &lt;- as.matrix(val_data[, 1:50])  # Input features for validation\ny_val &lt;- as.matrix(val_data[, 't+1'])  # Actual output for validation\n\nx_test &lt;- as.matrix(test_data[, 1:50])  # Input features for testing\ny_test &lt;- as.matrix(test_data[, 't+1'])  # Actual output for testing\n\n## Plot the result:\nlibrary(xts)\nn&lt;-quantile(sales_data$Date, \n            probs = c(0, 0.7, 0.8,1), \n            type = 1)\n\nm1&lt;-sales_data |&gt; \n  filter(Date &lt;= n[[2]])\nm2&lt;-sales_data |&gt; \n  filter(Date &lt;= n[[3]] & Date &gt; n[[2]])\nm3&lt;-sales_data |&gt; \n  filter(Date &lt;= n[[4]] & Date &gt; n[[3]])\n\ndemand_training&lt;-xts(x=m1$Product_A,\n                     order.by=m1$Date)\ndemand_testing&lt;-xts(x=m2$Product_A,\n                     order.by=m2$Date)\ndemand_forecasting&lt;-xts(x=m3$Product_A,\n                     order.by=m3$Date)\n\nlibrary(dygraphs)\nlines&lt;-cbind(demand_training,\n             demand_testing,\n             demand_forecasting)\ndygraph(lines,\n        main = \"Training and testing data\", \n        ylab = \"Quantity order (Unit: Millions)\") |&gt; \n  dySeries(\"demand_training\", label = \"Training data\") |&gt;\n  dySeries(\"demand_testing\", label = \"Testing data\") |&gt;\n  dySeries(\"demand_forecasting\", label = \"Forecasting data\") |&gt;\n  dyOptions(fillGraph = TRUE, fillAlpha = 0.4) |&gt; \n  dyRangeSelector(height = 20)\n\n\n\n\n\n\n\n\n2.3 MÃ´ hÃ¬nh RNN:\nSau Ä‘Ã³, ta sáº½ báº¯t Ä‘áº§u train model báº±ng cÃ¡ch táº¡o thÃªm 12 cá»™t giÃ¡ trá»‹ lÃ  giÃ¡ trá»‹ quÃ¡ khá»© cá»§a demand. Báº¡n sáº½ báº¯t Ä‘áº§u Ä‘á»‹nh nghÄ©a mÃ´ hÃ¬nh gá»“m:\n\nInput: dÃ¹ng hÃ m layer_input(shape = input_shape) vá»›i input_shape lÃ  sá»‘ lÆ°á»£ng predictor.\nLayer: lÃ  cÃ¡c hidden layer trong mÃ´ hÃ¬nh thÃªm vÃ o báº±ng hÃ m layer_dense(x, units = 64, activation = 'relu') vá»›i Ä‘á»‘i sá»‘ units thÆ°á»ng lÃ  bá»™i sá»‘ cá»§a 32 nhÆ° 32,64,256,â€¦\nOutput: dÃ¹ng hÃ m layer_dense(x, units = 1) Ä‘á»ƒ Ä‘á»‹nh nghÄ©a lÃ  Ä‘áº§u ra chá»‰ cÃ³ 1 giÃ¡ trá»‹.\n\nÄá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh truyá»n thá»‘ng nhÆ° linear regression thÃ¬ báº¡n Ä‘Ã£ quen vá»›i thÃ´ng sá»‘ \\(R^2\\) Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh, cÃ²n vá»›i mÃ´ hÃ¬nh Machine learning thÃ¬ dÃ¹ng khÃ¡i niá»‡m loss function - hÃ m máº¥t mÃ¡t. Vá» khÃ¡i niá»‡m, loss function sáº½ Ä‘o lÆ°á»ng chÃªnh lá»‡ch giá»¯a predicted vÃ  actual trong bá»™ training data nÃªn khi cÃ ng tÄƒng epochs nghÄ©a lÃ  tÄƒng sá»‘ láº§n há»c láº¡i dá»¯ liá»‡u thÃ¬ loss function sáº½ tÃ­nh ra giÃ¡ trá»‹ cÃ ng tháº¥p. NhÆ° mÃ´ hÃ¬nh trÃªn thÃ¬ mÃ¬nh Ä‘áº·t Ä‘á»‘i sá»‘ loss = mse nghÄ©a lÃ  sá»­ dá»¥ng Mean Squared Error Ä‘á»ƒ tá»‘i Æ°u quy trÃ¬nh há»c cá»§a há»c mÃ¡y. CÃ´ng thá»©c nhÆ° sau:\n\\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{pred}}(i) - y_{\\text{true}}(i))^2\n\\]\nCÃ²n Ä‘á»‘i sá»‘ metrics = c('mae') nghÄ©a lÃ  tiÃªu chÃ­ khÃ¡c Ä‘á»ƒ theo dÃµi vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh. Váº­y táº¡i sao cáº§n cÃ³ 2 tham sá»‘ Ä‘Ã¡nh giÃ¡ song song nhÆ° váº­y lÃ  vÃ¬ nhÆ° Ä‘Ã£ nÃ³i, náº¿u báº¡n cÃ ng tÄƒng epochs thÃ¬ giÃ¡ trá»‹ loss cÃ ng tháº¥p trong khi dÃ¹ng metrics sáº½ Ä‘Æ°a ra Ä‘Ã¡nh giÃ¡ khÃ¡ch quan hÆ¡n vá» mÃ´ hÃ¬nh mÃ  khÃ´ng phá»¥ thuá»™c vÃ o sá»‘ láº§n epochs. CÃ´ng thá»©c nhÆ° sau:\n\\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{pred}}(i) - y_{\\text{true}}(i)|\n\\]\nVáº­y khi cháº¡y code, R sáº½ return output nhÆ° biá»ƒu Ä‘á»“ dÆ°á»›i Ä‘Ã¢y lÃ  so sÃ¡nh tham sá»‘ cá»§a mse vÃ  mae giá»¯a training data vÃ  evaluating data. Ã tÆ°á»Ÿng lÃ  Ä‘Ã¡nh giÃ¡ thá»­ mÃ´ hÃ¬nh cÃ³ dá»± Ä‘oÃ¡n tá»‘t khÃ´ng khi cÃ³ dá»¯ liá»‡u má»›i vÃ o.\nTiáº¿p theo, ta sáº½ dÃ¹ng test data Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh vá»«a xÃ¢y dá»±ng. Káº¿t quáº£ cÃ³ váº» khÃ¡ á»•n vÃ¬ mÃ´ hÃ¬nh gáº§n nhÆ° theo sÃ¡t Ä‘Æ°á»£c dá»¯ liá»‡u cá»§a test data.\n\n\nCode\n# Step 6: Make predictions\nRNN_forecast &lt;- RNN_model |&gt; \n  predict(x_test)\n\n\n1/1 - 0s - 291ms/step\n\n\nCode\n# Step 7: Combine predicted and observed\nplot_data &lt;- data.frame(\n  actual = y_test,  # Actual values from the test set\n  forecast = RNN_forecast  # Forecasted values\n)\n\n# Step 8: Plot using Highcharts\nhighchart() |&gt;\n  hc_title(text = \"Time Series Forecasting with Highcharts\") |&gt;\n  hc_xAxis(\n    categories = plot_data$time,\n    title = list(text = \"Time\")\n  ) |&gt;\n  hc_yAxis(\n    title = list(text = \"Value\"),\n    plotLines = list(list(\n      value = 0,\n      width = 1,\n      color = \"gray\"\n    ))\n  ) |&gt;\n  hc_add_series(\n    name = \"Actual Data\",\n    data = plot_data$actual,\n    type = \"line\",\n    color = \"#1f77b4\"  # Blue color for actual data\n  ) |&gt;\n  hc_add_series(\n    name = \"Forecast\",\n    data = plot_data$forecast,\n    type = \"line\",\n    color = \"#ff7f0e\"  # Orange color for forecast data\n  ) |&gt;\n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) |&gt;\n  hc_legend(\n    enabled = TRUE\n  )\n\n\n\n\n\n\n\n\n2.4 MÃ´ hÃ¬nh LSTM:\nTiáº¿p theo, ta sáº½ xÃ¢y dá»±ng thá»­ mÃ´ hÃ¬nh LSTM. MÃ´ hÃ¬nh LSTM thÆ°á»ng bao gá»“m cÃ¡c lá»›p sau:\n\nLá»›p LSTM: ÄÃ¢y lÃ  lá»›p chÃ­nh, cÃ³ thá»ƒ cÃ³ má»™t hoáº·c nhiá»u lá»›p LSTM chá»“ng lÃªn nhau. Má»—i lá»›p LSTM cÃ³ thá»ƒ tráº£ vá» toÃ n bá»™ chuá»—i báº±ng return_sequences = TRUE hoáº·c chá»‰ tráº£ vá» giÃ¡ trá»‹ cuá»‘i cÃ¹ng báº±ng return_sequences = FALSE.\nLá»›p Dense: Sau khi thÃ´ng tin Ä‘Æ°á»£c xá»­ lÃ½ qua cÃ¡c lá»›p LSTM, nÃ³ sáº½ Ä‘Æ°á»£c Ä‘Æ°a qua cÃ¡c lá»›p Dense (lá»›p fully connected) Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng.\nLá»›p Dropout (tÃ¹y chá»n): Äá»ƒ trÃ¡nh overfitting, cÃ³ thá»ƒ thÃªm lá»›p dropout Ä‘á»ƒ táº¯t ngáº«u nhiÃªn má»™t sá»‘ nÆ¡-ron trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.\n\nVáº­y giá» ta sáº½ so sÃ¡nh vá»›i mÃ´ hÃ¬nh RNN trÆ°á»›c vá»›i mÃ´ hÃ¬nh LSTM qua 2 thÃ´ng sá»‘ Ä‘Ã£ chá»n mse vÃ  mae.\n\n\nCode\n# Extract metrics into a data frame\nresults_df &lt;- data.frame(\n  Model = c(\"RNN\", \"LSTM\"),\n  MSE = c(RNN_result[[1]],RNN_result[[2]]),\n  MAE = c(LSTM_result[[1]], LSTM_result[[2]])\n)\n\nlibrary(gt)\n# Create a gt table\nresults_df |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Model Performance Metrics\",\n    subtitle = \"Comparison of MSE and MAE for RNN and LSTM\"\n  ) |&gt;\n  fmt_number(\n    columns = vars(MSE, MAE),\n    decimals = 6\n  ) |&gt;\n  cols_label(\n    Model = \"Model Type\",\n    MSE = \"Mean Squared Error\",\n    MAE = \"Mean Absolute Error\"\n  ) |&gt;\n  tab_options(\n    table.font.size = 14,\n    heading.title.font.size = 16,\n    heading.subtitle.font.size = 14\n  )\n\n\n\n\n\n\n\n\nModel Performance Metrics\n\n\nComparison of MSE and MAE for RNN and LSTM\n\n\nModel Type\nMean Squared Error\nMean Absolute Error\n\n\n\n\nRNN\n0.622324\n0.790441\n\n\nLSTM\n0.637329\n0.735569\n\n\n\n\n\n\n\nKáº¿t quáº£ cho tháº¥y mÃ´ hÃ¬nh LSTM Ä‘Æ°a ra káº¿t quáº£ tá»‘t hÆ¡n RNN vá»›i Ä‘á»™ sai sá»‘ tháº¥p hÆ¡n nhÆ°ng náº¿u báº¡n Ä‘á»ƒ Ã½ thÃ¬ tháº¥y trong biá»ƒu Ä‘á»“ mÃ¬nh váº«n Ä‘á»ƒ % Ã¢m Ä‘á»ƒ dá»… phÃ¢n biá»‡t giá»¯a viá»‡c outstock vÃ  high inventory (bá»Ÿi vÃ¬ báº¡n Ä‘ang dá»± bÃ¡o cho nhu cáº§u cá»§a khÃ¡ch hÃ ng). Báº¡n cÃ³ thá»ƒ so sÃ¡nh thÃªm 1 bÆ°á»›c ná»¯a vá» tá»•ng chi phÃ­ giá»¯a 2 mÃ´ hÃ¬nh vá» outstock vÃ  holding cost Ä‘á»ƒ cÃ³ cÃ¡i nhÃ¬n tá»•ng quan nháº¥t.\n\n\nCode\nLSTM_forecast &lt;- LSTM_model |&gt; \n  predict(x_test)\n\n\n1/1 - 0s - 350ms/step\n\n\nCode\ncompare&lt;-data.frame(Date = 1:length(y_test),\n                    LSTM = round((LSTM_forecast - y_test)/y_test,3),\n                    RNN = round((RNN_forecast - y_test)/y_test,3)\n)\n\n# Create the highchart plot with percentage formatting for y-axis\nhighchart() |&gt;\n  hc_chart(type = \"line\") |&gt;\n  hc_title(text = \"Residual Comparison: LSTM vs RNN\") |&gt;\n  hc_xAxis(\n    categories = compare$Date,\n    title = list(text = \"Date\")\n  ) |&gt;\n  hc_yAxis(\n    title = list(text = \"Residuals\"),\n    labels = list(\n      formatter = JS(\"function() { return (this.value).toFixed(0) + '%'; }\")  # Format labels as percentages\n    ),\n    plotLines = list(\n      list(value = 0, color = \"gray\", width = 1, dashStyle = \"Dash\")\n    )\n  ) |&gt;\n  hc_add_series(\n    name = \"LSTM Residuals\",\n    data = compare$LSTM,\n    color = \"#1f77b4\"\n  ) |&gt;\n  hc_add_series(\n    name = \"RNN Residuals\",\n    data = compare$RNN,\n    color = \"#ff7f0e\"\n  ) |&gt;\n  hc_tooltip(shared = TRUE) |&gt;\n  hc_legend(enabled = TRUE)\n\n\n\n\n\n\n\n\n2.5 XÃ¡c Ä‘á»‹nh cáº¥u trÃºc mÃ´ hÃ¬nh:\nNáº¿u báº¡n Ä‘á»ƒ Ã½, thá»±c cháº¥t code cho mÃ´ hÃ¬nh cho nhÆ° mÃ¬nh Ä‘Ã£ trÃ¬nh bÃ y thÃ¬ khÃ¡ Ä‘Æ¡n giáº£n vÃ  Ä‘iá»u khÃ³ nháº¥t trong mÃ´ hÃ¬nh lÃ  xÃ¡c Ä‘á»‹nh sá»‘ lá»›p layer trong mÃ´ hÃ¬nh. NhÆ° bÃ i toÃ¡n time series forecasting thÃ¬ mÃ¬nh chá»‰ cáº§n 2,3 lá»›p layer Ä‘Æ¡n giáº£n lÃ  Ä‘Ã£ Ä‘áº¡t káº¿t quáº£ tá»‘t vá»›i sai sá»‘ ráº¥t tháº¥p (&lt; 0.03), cÃ²n vá»›i cÃ¡c bÃ i toÃ¡n phá»©c táº¡p hÆ¡n thÃ¬ sá»‘ layer sáº½ nhiá»u hÆ¡n.\nVáº­y quy táº¯c xÃ¡c Ä‘á»‹nh mÃ´ hÃ¬nh lÃ  nhÆ° tháº¿ nÃ o ? CÃ¢u tráº£ lá»i lÃ  khÃ´ng cÃ³ quy táº¯c nÃ o cáº£ vÃ  chá»‰ cÃ³ cÃ¡c tips mÃ  mÃ¬nh lá»¥m nháº·t trÃªn máº¡ng nhÆ° sau:\n\n2.5.1 Number of layer:\nSá»‘ layer nÃªn náº±m giá»¯a sá»‘ input vÃ  sá»‘ output. NhÆ° bÃ i thá»±c hÃ nh trÃªn thÃ¬ sá»‘ layer nÃªn náº±m trong khoáº£ng (1,12). Hoáº·c báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng hÃ m dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ xÃ¡c Ä‘á»‹nh.\n\\[\nN_h = \\frac{N_s}{\\alpha \\cdot (N_i + N_o)}\n\\]\nVá»›i cÃ¡c tham sá»‘ gá»“m:\n\n\\(N_h\\) lÃ  sá»‘ lÆ°á»£ng hidden neurons.\n\\(N_s\\) lÃ  sá»‘ lÆ°á»£ng máº«u trong training data.\n\\(\\alpha\\) lÃ  yáº¿u tá»‘ tá»· lá»‡ tÃ¹y Ã½ (thÆ°á»ng tá»« 2-10).\n\\(N_i\\) lÃ  sá»‘ lÆ°á»£ng nÆ¡-ron input\n\\(N_o\\) lÃ  sá»‘ lÆ°á»£ng nÆ¡-ron output.\n\nVÃ­ dá»¥ nhÆ° á»Ÿ mÃ´ hÃ¬nh trÃªn thÃ¬ sá»‘ hidden layer sáº½ khoáº£ng 1-4 layer lÃ  á»•n (NhÆ° trÃªn thÃ¬ mÃ¬nh dÃ¹ng 1 layer cho mÃ´ hÃ¬nh RNN, 2 layer cho mÃ´ hÃ¬nh LSTM)\n\n\n2.5.2 Choose acvtivation function:\nCÃ¡c hÃ m activation dÃ¹ng Ä‘á»ƒ tÃ­nh weighted sum vÃ  má»—i layer sáº½ cáº§n cÃ³ 1 hoáº·c nhiá»u hÃ m Ä‘á»ƒ tÃ­nh. Viá»‡c lá»±a chá»n hÃ m áº£nh hÆ°á»Ÿng lá»›n Ä‘áº¿n performance cá»§a mÃ´ hÃ¬nh, thÆ°á»ng sáº½ Ä‘Æ°á»£c chia thÃ nh 3 pháº§n lÃ :\n\nActivation for input layer: thÆ°á»ng ko dÃ¹ng hÃ m gÃ¬ cáº£. Báº¡n chá»‰ thá»±c hiá»‡n processing dá»¯ liá»‡u Ä‘á»ƒ training.\nActivation for Hidden Layers:\n\nThÃ´ng thÆ°á»ng, hÃ m Tanh thÃ¬ phÃ¹ há»£p cho dá»± bÃ¡o giÃ¡ trá»‹ liÃªn tá»¥c tá»« dá»¯ liá»‡u chuá»—i, ReLU giÃºp cho quÃ¡ trÃ¬nh training nhanh hÆ¡n vÃ  khÃ´ng gÃ¢y ra vanishing problem do khÃ´ng bá»‹ cháº·n, Softmax thÆ°á»ng dÃ¹ng á»Ÿ output layer cho bÃ i toÃ¡n classification, Sigmoid thÆ°á»ng dÃ¹ng cho há»“i quy logic. NgoÃ i ra, cÃ¡ch Ä‘Æ¡n giáº£n hÆ¡n lÃ  tÃ¹y vÃ o loáº¡i mÃ´ hÃ¬nh báº¡n Ä‘ang xÃ¢y dá»±ng Ä‘á»ƒ lá»±a chá»n, vÃ­ dá»¥ nhÆ° tips dÆ°á»›i Ä‘Ã¢y mÃ¬nh tÃ¬m hiá»ƒu Ä‘Æ°á»£c:\n\n  \n  \n  \n  \n    HÃ¬nh 4: Tips chá»n hÃ m activation cho hidden layer \n  \n  \n  \n  \n    Source: Link to Image\n  \n\n\nActivation for Output Layers:\n\nÄá»‘i vá»›i output layer, báº¡n sáº½ lá»±a chá»n hÃ m dá»±a trÃªn class cá»§a output mÃ  báº¡n Ä‘ang hÆ°á»›ng Ä‘áº¿n. CÃ¡c hÃ m thÃ´ng thÆ°á»ng sáº½ gá»“m:\n\nLinear: hay cÃ²n gá»i lÃ  â€œidentityâ€ (nhÃ¢n vá»›i 1.0) hoáº·c â€œno activationâ€ bá»Ÿi vÃ¬ hÃ m linear tuyáº¿n tÃ­nh khÃ´ng thay Ä‘á»•i weighted sum cá»§a input theo báº¥t ká»³ cÃ¡ch nÃ o vÃ  thay vÃ o Ä‘Ã³ tráº£ vá» giÃ¡ trá»‹ trá»±c tiáº¿p. HÃ m nÃ y thÆ°á»ng dÃ¹ng cho output dáº¡ng liÃªn tá»¥c.\nLogistic (Sigmoid): Ã¡p dá»¥ng cho output dáº¡ng [0,1] hay cÃ²n gá»i lÃ  binary classification (vÃ­ dá»¥ mÃ´ hÃ¬nh nháº±m Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh cÃ³/khÃ´ng trong viá»‡c Ä‘áº§u tÆ° vÃ o cá»• phiá»ƒu nÃ y cháº³ng háº¡n).\nSoftmax: HÃ m nÃ y sáº½ chuyá»ƒn Ä‘á»•i má»™t vector thÃ nh cÃ¡c giÃ¡ trá»‹ xÃ¡c suáº¥t cÃ³ tá»•ng báº±ng 1 (NÃ³ giá»‘ng nhÆ° tÃ¬m hÃ m máº­t Ä‘á»™ (PDF) cho má»™t biáº¿n). á»¨ng dá»¥ng Ä‘á»ƒ dÃ¡n nhÃ£n cho multiclass thay vÃ¬ 2 class nhÆ° hÃ m sigmoid bÃªn trÃªn. Má»—i nhÃ£n sáº½ cÃ³ 1 giÃ¡ trá»‹ xÃ¡c suáº¥t vÃ  dá»±a vÃ o Ä‘Ã³ dá»± Ä‘oÃ¡n kháº£ nÄƒng xáº£y ra cá»§a tá»«ng class.\n\n\n\n2.5.3 Number of neurons:\nSá»‘ lÆ°á»£ng nÆ¡-ron trong má»™t lá»›p quyáº¿t Ä‘á»‹nh lÆ°á»£ng thÃ´ng tin mÃ  máº¡ng cÃ³ thá»ƒ lÆ°u trá»¯. Nhiá»u nÆ¡-ron giÃºp máº¡ng há»c Ä‘Æ°á»£c cÃ¡c máº«u phá»©c táº¡p hÆ¡n, nhÆ°ng cÅ©ng lÃ m tÄƒng nguy cÆ¡ overfitting (quÃ¡ khá»›p) vÃ  yÃªu cáº§u nhiá»u tÃ i nguyÃªn tÃ­nh toÃ¡n hÆ¡n. Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u vá»›i má»™t sá»‘ lÆ°á»£ng nÆ¡-ron tÆ°Æ¡ng Ä‘á»‘i nhá», nhÆ° 128 hoáº·c 256â€¦"
  },
  {
    "objectID": "torch.html#káº¿t-luáº­n",
    "href": "torch.html#káº¿t-luáº­n",
    "title": "RNN and LSTM model",
    "section": "3 Káº¿t luáº­n:",
    "text": "3 Káº¿t luáº­n:\nNhÆ° váº­y, chÃºng ta Ä‘Ã£ Ä‘Æ°á»£c há»c vá» mÃ´ hÃ¬nh RNN vÃ  LSTM vÃ  cÃ¡ch xÃ¢y dá»±ng chÃºng trong R. Tiáº¿p theo, ta sáº½ há»c tiáº¿p vá» mÃ´ hÃ¬nh Transformer\n\n\n\n    \n    \n    Contact Me\n    \n    \n    \n\n\n    \n        Contact Me\n        \n            Your Email:\n            \n            Please enter a valid email address.\n            Send Email\n        \n        \n            \n                \n                     View Code on GitHub\n                \n            \n        \n        \n            \n                \n                     Visit my RPubs"
  }
]