[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cao Xuân Lộc",
    "section": "",
    "text": "Xin chào, mình là Lộc, sinh năm 2003 và là một chàng trai đến từ mảnh đất đầy nắng và gió - Phú Yên, Việt Nam. Mình có bằng cử nhân trường Đại học Kinh Tế - Tài Chính (UEF) và chuyên ngành của mình là Logistics và quản lý chuỗi cung ứng.\nLà người có niềm đam mê mạnh mẽ với R, mình có sở thích viết post về việc phân tích dữ liệu với R để ứng dụng vào các công việc, bài toán thường gặp trong Supply Chain. Ngoài ra, sở thích của mình là nghe sách nói và đi bộ!\nCâu slogan mà mình thích nhất là: “Don’t fear the risk, fear the opportunity lost!” và đó cũng là cách mình sống và làm việc đến bây giờ 💝💝💝.\nHi vọng các bạn sẽ thích bài viết của mình!\n    \n    \n    Go to Next Page\n    \n    \n        \n            Go to Next Page\n            ➔"
  },
  {
    "objectID": "torch.html",
    "href": "torch.html",
    "title": "RNN and LSTM model",
    "section": "",
    "text": "Ở đây ta sẽ học về mô hình machine learning được ứng dụng nhiều nhất trong việc phân tích dữ liệu thời gian là RNN và LSTM."
  },
  {
    "objectID": "torch.html#định-nghĩa",
    "href": "torch.html#định-nghĩa",
    "title": "RNN and LSTM model",
    "section": "1 Định nghĩa:",
    "text": "1 Định nghĩa:\n\n1.1 Mô hình RNN:\nĐiểm chung là cả hai mô hình đều thuộc phân lớp Deep learning - nghĩa là học máy sâu với đặc điểm chung là phân chia dữ liệu thành nhiều lớp và bắt đầu “học” dần qua từng lớp để đưa ra kết quả cuối cùng. Ở hình dưới đây, \\(X_o\\) đại diện cho dữ liệu đầu vào, \\(h_t\\) là output đầu ra của từng step và \\(A\\) là những gì đã “học” được tại step đó và được truyền cho step tiếp theo. Trong tài liệu chuẩn thì họ thường kí hiệu là \\(X_t\\), \\(Y_t\\), \\(h_{t-1}\\).\n\n  \n  \n  \n  \n    Hình 1: Minh họa về sự phân chia dữ liệu thành nhiều lớp\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nKhi nhìn hình thì bạn có thể bối rối chưa hiểu các kí tự và hình ảnh thì bạn có thể tưởng tượng học máy như 1 đứa trẻ và để nó có thể hiểu được câu: “Hôm nay con đi học” thì nó phải học từng chữ cái như: a,b,c,… trước ròi mới ghép thành từ đơn như: “Hôm”,“Nay”,… rồi ghép thành câu trên. Vậy giả sử như hôm nay học được từ “Hôm” thì nó sẽ bắt đầu ghi nhớ từ đã học vào trong \\(A\\). Nếu sau này ta cần học máy hiểu câu “Hôm sau con đi chơi” thì tốc độ học của học máy sẽ nhanh lên vì thay vì nó phải học 5 chữ đơn như thông thường thì nó chỉ cần học 4 chữ còn lại trừ chữ “hôm”. Vậy bạn đã hiểu ý tưởng nền tảng của RNN rồi ha!\nNếu muốn hiểu thêm về RNN, bạn có thể tham khảo link này: Recurrent Neural Network: Từ RNN đến LSTM.\nVà trong RNN có 1 vấn đề lớn là Vanishing Gradient nghĩa là mô hình sẽ không còn “học” thêm được nữa cho dù tăng số epochs. Theo phần chứng minh của anh Tuấn cho thấy RNN sẽ luôn xảy ra vấn đề đó cho dù bạn có xây dựng mô hình tốt như thế nào. Điều này có thể hiểu đơn giản như việc bạn học liên tục dẫn tới quá tải. Do đó, RNN chỉ học các thông tin \\(A\\) từ các step gần nhất và đó là lí do ra đời LSTM - Long short term memory.\n\n\n\n\n\n\nWarning\n\n\n\nLưu ý: Điều này không có nghĩa LSTM luôn tốt hơn RNN vì có những bài toán với đầu vào đơn giản thì mô hình chỉ cần học các step đầu là đã “học” đầy đủ thông tin cần thiết. Mô hình LSTM phổ biến với các bài toán phức tạp như tự động dịch ngôn ngữ, ghi chép lại theo giọng nói…\n\n\n\n\n1.2 Mô hình LSTM:\nCó thể xem mô hình LSTM như biến thể của RNN. Về cấu trúc, LSTM có ba cổng chính giúp nó xử lý và duy trì thông tin qua các bước thời gian:\n\nCổng quên (Forget Gate): Quyết định thông tin nào cần bị quên trong trạng thái ô nhớ.\nCổng nhập (Input Gate): Xác định thông tin nào cần được ghi vào trạng thái ô nhớ.\nCổng xuất (Output Gate): Quyết định thông tin nào sẽ được xuất ra từ trạng thái ô nhớ để ảnh hưởng đến dự đoán tiếp theo."
  },
  {
    "objectID": "torch.html#xây-dựng-mô-hình",
    "href": "torch.html#xây-dựng-mô-hình",
    "title": "RNN and LSTM model",
    "section": "2 Xây dựng mô hình:",
    "text": "2 Xây dựng mô hình:\n\n2.1 Load dữ liệu:\nĐầu tiên ta sẽ load dữ liệu lại như trước. Ở đây, để đơn giản, mình chỉ xây dựng mô hình cho product A thôi.\nGiả sử công ty mình đang kinh doanh 3 loại mặt hàng product A,product B,product C và đây là biểu đồ thể hiện nhu cầu của cả 3 mặt hàng từ tháng 5 tới tháng 10.\n\n\nCode\nlibrary(highcharter)\nsales_data |&gt; \n  select(-Weekday) |&gt; \n  pivot_longer(cols = c(Product_A, Product_B, Product_C),\n               names_to = \"Product\",\n               values_to = \"Sales\") |&gt; \n  hchart(\"line\", hcaes(x = Date, y = Sales, group = Product))\n\n\n\n\n\n\nNếu ta phân tich sâu về nhu cầu của từng mặt hàng theo thứ trong tuần, ta sẽ thấy rằng mặt hàng A, B thì bán chạy vào thứ 4 và thứ 7, còn mặt hàng C thì bán chạy vào thứ 2 và thứ 3.\n\nProduct A:Product B:Product C:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThông thường dữ liệu để train model trong machine learning thường cần trải qua bước normalize data nghĩa là đưa tất cả dữ liệu về chung 1 thước đo và phạm vi. Nguyên do vì điều này giúp nhiều thuật toán học máy dễ dàng hội tụ hơn. Ví dụ, các thuật toán như k-Nearest Neighbors (KNN) và Support Vector Machines (SVM) rất nhạy cảm với khoảng cách giữa các điểm dữ liệu nên nếu dữ liệu không được chuẩn hóa, thuật toán có thể ưu tiên các đặc trưng có phạm vi lớn hơn và bỏ qua các đặc trưng có phạm vi nhỏ hơn, dẫn đến hiệu suất kém. Và công thức phổ biến nhất cho chuẩn hóa là:\n\\[\n\\text{Normalized Value} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n\\]\n\n\nCode\n# Create a data frame with the adjusted sales data\nsales_data &lt;- data.frame(\n  Date = dates,\n  Weekday = weekdays,\n  Product_A = product_a_sales,\n  Product_B = product_b_sales,\n  Product_C = product_c_sales\n)\n\n# Convert the sales data to a time series (ts) object for Product A\nproduct_a_ts &lt;- ts(sales_data$Product_A, start = c(2024, 5), \n                   frequency = 365)\n                   \n\n# Normalzie data:\ntime_series_data&lt;-scale(product_a_ts)\n\nlibrary(highcharter)\nhighchart() %&gt;%\n  hc_add_series(data = as.numeric(time_series_data), type = \"line\", name = \"Sales of Product A\") %&gt;%\n  hc_title(text = \"Normalized Time Series of Product A\") %&gt;%\n  hc_xAxis(title = list(text = \"Date\")) %&gt;%\n  hc_yAxis(title = list(text = \"Normalized Sales\")) %&gt;%\n  hc_tooltip(shared = TRUE) %&gt;%\n  hc_plotOptions(line = list(marker = list(enabled = FALSE)))\n\n\n\n\n\n\n\n\n2.2 Chia dữ liệu:\nVậy để train data, mình sẽ chia bộ dữ liệu thành 3 phần:\n\nTraining data: dùng để huấn luyện và xây dựng mô hình.\nEvaluating data: đánh giá mô hình vừa huấn luyện.\nTesting data: dùng để đánh giá lại nếu muốn mô hình học lại dữ liệu\n\n\n\nCode\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(dplyr)\n\n# Function to create supervised learning format from time series\ncreate_supervised_data &lt;- function(series, n_in = 1, n_out = 1) {\n  series &lt;- as.vector(series)  # Convert time series object to vector\n  data &lt;- data.frame(series)\n  \n  # Use base R lag function for ts objects (lag() from stats package)\n  for (i in 1:n_in) {\n    data &lt;- cbind(data, stats::lag(series, -i))\n  }\n  \n  colnames(data) &lt;- c(paste0('t-', 1:n_in), 't+1')  # Correctly name columns\n  return(data)\n}\n\n# Prepare the data with 12 input lags and 1 output (next time step)\nsupervised_data &lt;- create_supervised_data(time_series_data,\n                                          n_in = 12, \n                                          n_out = 1)\n\n# Remove NA rows created by lag function\nsupervised_data &lt;- na.omit(supervised_data)\n\n# Step 2: Split data into training and test sets\ntrain_size &lt;- round(0.7 * nrow(supervised_data))   # 70% for training\nval_size &lt;- round(0.1 * nrow(supervised_data))     # 10% for validation\ntest_size &lt;- nrow(supervised_data) - train_size - val_size  # 20% for testing\n\ntrain_data &lt;- supervised_data[1:train_size, ]\nval_data &lt;- supervised_data[(train_size + 1):(train_size + val_size), ]\ntest_data &lt;- supervised_data[(train_size + val_size + 1):nrow(supervised_data), ]\n\n# Correct column selection\nx_train &lt;- as.matrix(train_data[, 1:12])  # Input features (12 lags)\ny_train &lt;- as.matrix(train_data[, 't+1'])  # Target output (next time step)\n\nx_val &lt;- as.matrix(val_data[, 1:12])  # Input features for validation\ny_val &lt;- as.matrix(val_data[, 't+1'])  # Actual output for validation\n\nx_test &lt;- as.matrix(test_data[, 1:12])  # Input features for testing\ny_test &lt;- as.matrix(test_data[, 't+1'])  # Actual output for testing\n\n\n## Plot the result:\nlibrary(xts)\nn&lt;-quantile(sales_data$Date, \n            probs = c(0, 0.7, 0.8,1), \n            type = 1)\n\nm1&lt;-sales_data %&gt;% \n  filter(Date &lt;= n[[2]])\nm2&lt;-sales_data %&gt;% \n  filter(Date &lt;= n[[3]] & Date &gt; n[[2]])\nm3&lt;-sales_data %&gt;% \n  filter(Date &lt;= n[[4]] & Date &gt; n[[3]])\n\ndemand_training&lt;-xts(x=m1$Product_A,\n                     order.by=m1$Date)\ndemand_testing&lt;-xts(x=m2$Product_A,\n                     order.by=m2$Date)\ndemand_forecasting&lt;-xts(x=m3$Product_A,\n                     order.by=m3$Date)\n\nlibrary(dygraphs)\nlines&lt;-cbind(demand_training,\n             demand_testing,\n             demand_forecasting)\ndygraph(lines,\n        main = \"Training and testing data\", \n        ylab = \"Quantity order (Unit: Millions)\") %&gt;% \n  dySeries(\"demand_training\", label = \"Training data\") %&gt;%\n  dySeries(\"demand_testing\", label = \"Testing data\") %&gt;%\n  dySeries(\"demand_forecasting\", label = \"Forecasting data\") %&gt;%\n  dyOptions(fillGraph = TRUE, fillAlpha = 0.4) %&gt;% \n  dyRangeSelector(height = 20)\n\n\n\n\n\n\n\n\n2.3 Mô hình RNN:\nSau đó, ta sẽ bắt đầu train model bằng cách tạo thêm 12 cột giá trị là giá trị quá khứ của demand. Bạn sẽ bắt đầu định nghĩa mô hình gồm:\n\nInput: dùng hàm layer_input(shape = input_shape) với input_shape là số lượng predictor.\nLayer: là các hidden layer trong mô hình thêm vào bằng hàm layer_dense(x, units = 64, activation = 'relu') với đối số units thường là bội số của 32 như 32,64,256,…\nOutput: dùng hàm layer_dense(x, units = 1) để định nghĩa là đầu ra chỉ có 1 giá trị.\n\n\n\nCode\n# Step 3: Build a simple transformer-like model\nRNN_model &lt;- function(input_shape) {\n  inputs &lt;- layer_input(shape = input_shape)\n\n  # Transformer Encoder Layer (simplified)\n  x &lt;- inputs\n  x &lt;- layer_dense(x, units = 64, activation = 'relu')  # Dense layer\n  x &lt;- layer_dense(x, units = 32, activation = 'relu')  # Another dense layer\n\n  # Output layer\n  x &lt;- layer_dense(x, units = 1)\n  \n  model &lt;- keras_model(inputs, x)\n  return(model)\n}\n\n# Example input shape (12 time steps input per sample)\ninput_shape &lt;- c(12)\n\nRNN_model &lt;- RNN_model(input_shape)\n\n\nĐối với các mô hình truyền thống như linear regression thì bạn đã quen với thông số \\(R^2\\) để đánh giá mô hình, còn với mô hình Machine learning thì dùng khái niệm loss function - hàm mất mát. Về khái niệm, loss function sẽ đo lường chênh lệch giữa predicted và actual trong bộ training data nên khi càng tăng epochs nghĩa là tăng số lần học lại dữ liệu thì loss function sẽ tính ra giá trị càng thấp. Như mô hình trên thì mình đặt đối số loss = mse nghĩa là sử dụng Mean Squared Error để tối ưu quy trình học của học máy. Công thức như sau:\n\\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{pred}}(i) - y_{\\text{true}}(i))^2\n\\]\nCòn đối số metrics = c('mae') nghĩa là tiêu chí khác để theo dõi và đánh giá mô hình. Vậy tại sao cần có 2 tham số đánh giá song song như vậy là vì như đã nói, nếu bạn càng tăng epochs thì giá trị loss càng thấp trong khi dùng metrics sẽ đưa ra đánh giá khách quan hơn về mô hình mà không phụ thuộc vào số lần epochs. Công thức như sau:\n\\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{pred}}(i) - y_{\\text{true}}(i)|\n\\]\nVậy khi chạy code, R sẽ return output như biểu đồ dưới đây là so sánh tham số của mse và mae giữa training data và evaluating data. Ý tưởng là đánh giá thử mô hình có dự đoán tốt không khi có dữ liệu mới vào.\nTiếp theo, ta sẽ dùng test data để đánh giá mô hình vừa xây dựng. Kết quả có vẻ khá tuyệt vì mô hình gần như theo sát được dữ liệu của test data.\n\n\nCode\n# Step 6: Make predictions\nRNN_forecast &lt;- RNN_model %&gt;% \n  predict(x_test)\n\n\n2/2 - 0s - 95ms/epoch - 48ms/step\n\n\nCode\n# Step 7: Combine predicted and observed\nplot_data &lt;- data.frame(\n  time = c(min(m3$Date)-days(1),m3$Date),  # Time for the test set\n  actual = y_test,  # Actual values from the test set\n  forecast = RNN_forecast  # Forecasted values\n)\n\n# Step 8: Plot using Highcharts\nhighchart() %&gt;%\n  hc_title(text = \"Time Series Forecasting with Highcharts\") %&gt;%\n  hc_xAxis(\n    categories = plot_data$time,\n    title = list(text = \"Time\")\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Value\"),\n    plotLines = list(list(\n      value = 0,\n      width = 1,\n      color = \"gray\"\n    ))\n  ) %&gt;%\n  hc_add_series(\n    name = \"Actual Data\",\n    data = plot_data$actual,\n    type = \"line\",\n    color = \"#1f77b4\"  # Blue color for actual data\n  ) %&gt;%\n  hc_add_series(\n    name = \"Forecast\",\n    data = plot_data$forecast,\n    type = \"line\",\n    color = \"#ff7f0e\"  # Orange color for forecast data\n  ) %&gt;%\n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) %&gt;%\n  hc_legend(\n    enabled = TRUE\n  )\n\n\n\n\n\n\n\n\n2.4 Mô hình LSTM:\nTiếp theo, ta sẽ xây dựng thử mô hình LSTM. Mô hình LSTM thường bao gồm các lớp sau:\n\nLớp LSTM: Đây là lớp chính, có thể có một hoặc nhiều lớp LSTM chồng lên nhau. Mỗi lớp LSTM có thể trả về toàn bộ chuỗi bằng return_sequences = TRUE hoặc chỉ trả về giá trị cuối cùng bằng return_sequences = FALSE.\nLớp Dense: Sau khi thông tin được xử lý qua các lớp LSTM, nó sẽ được đưa qua các lớp Dense (lớp fully connected) để đưa ra dự đoán cuối cùng.\nLớp Dropout (tùy chọn): Để tránh overfitting, có thể thêm lớp dropout để tắt ngẫu nhiên một số nơ-ron trong quá trình huấn luyện.\n\nVậy giờ ta sẽ so sánh với mô hình RNN trước với mô hình LSTM qua 2 thông số đã chọn mse và mae.\n\n\nCode\n# Extract metrics into a data frame\nresults_df &lt;- data.frame(\n  Model = c(\"RNN\", \"LSTM\"),\n  Metric = c(\"Loss\", \"metric\"),\n  MSE = c(RNN_result[[1]],RNN_result[[2]]),\n  MAE = c(LSTM_result[[1]], LSTM_result[[2]])\n)\n\nlibrary(gt)\n# Create a gt table\nresults_df %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Model Performance Metrics\",\n    subtitle = \"Comparison of MSE and MAE for RNN and LSTM\"\n  ) %&gt;%\n  fmt_number(\n    columns = vars(MSE, MAE),\n    decimals = 4\n  ) %&gt;%\n  cols_label(\n    Model = \"Model Type\",\n    MSE = \"Mean Squared Error\",\n    MAE = \"Mean Absolute Error\"\n  ) %&gt;%\n  tab_options(\n    table.font.size = 14,\n    heading.title.font.size = 16,\n    heading.subtitle.font.size = 14\n  )\n\n\n\n\n\n\n\n\nModel Performance Metrics\n\n\nComparison of MSE and MAE for RNN and LSTM\n\n\nModel Type\nMetric\nMean Squared Error\nMean Absolute Error\n\n\n\n\nRNN\nLoss\n0.0000\n0.0001\n\n\nLSTM\nmetric\n0.0019\n0.0055\n\n\n\n\n\n\n\nKết quả cho thấy mô hình RNN truyền thống đưa ra kết quả tốt hơn LSTM mặc dù sai số của LSTM đều &lt; 0.03 là không quá tệ nhưng tiêu chí vẫn là mô hình nào hiệu quả nhất.\n\n\nCode\nLSTM_forecast &lt;- LSTM_model %&gt;% \n  predict(x_test)\n\n\n2/2 - 1s - 745ms/epoch - 372ms/step\n\n\nCode\ncompare&lt;-data.frame(Date = c(min(m3$Date)-days(1),m3$Date),\n                    LSTM = round(LSTM_forecast - y_test,3),\n                    RNN = round(RNN_forecast - y_test,3)\n)\n\n# Create the highchart plot\nhighchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_title(text = \"Residual Comparison: LSTM vs RNN\") %&gt;%\n  hc_xAxis(\n    categories = compare$Date,\n    title = list(text = \"Date\")\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Residuals\"),\n    plotLines = list(\n      list(value = 0, color = \"gray\", width = 1, dashStyle = \"Dash\")\n    )\n  ) %&gt;%\n  hc_add_series(\n    name = \"LSTM Residuals\",\n    data = compare$LSTM,\n    color = \"#1f77b4\"\n  ) %&gt;%\n  hc_add_series(\n    name = \"RNN Residuals\",\n    data = compare$RNN,\n    color = \"#ff7f0e\"\n  ) %&gt;%\n  hc_tooltip(shared = TRUE) %&gt;%\n  hc_legend(enabled = TRUE)\n\n\n\n\n\n\n\n\n2.5 Xác định cấu trúc mô hình:\nNếu bạn để ý, thực chất code cho mô hình cho như mình đã trình bày thì khá đơn giản và điều khó nhất trong mô hình là xác định số lớp layer trong mô hình. Như bài toán time series forecasting thì mình chỉ cần 2,3 lớp layer đơn giản là đã đạt kết quả tốt với sai số rất thấp (&lt; 0.03), còn với các bài toán phức tạp hơn thì số layer sẽ nhiều hơn.\nVậy quy tắc xác định mô hình là như thế nào ? Câu trả lời là không có quy tắc nào cả và chỉ có các tips mà mình lụm nhặt trên mạng như sau:\n\nNumber of layer nên nằm giữa số input và số output. Như bài thực hành trên thì số layer nên nằm trong khoảng (1,12). Hoặc bạn có thể sử dụng hàm dưới đây để xác định.\n\n\\[\nN_h = \\frac{N_s}{\\alpha \\cdot (N_i + N_o)}\n\\]\nVới các tham số gồm:\n\n\\(N_h\\) là số lượng hidden neurons.\n\\(N_s\\) là số lượng mẫu trong training data.\n\\(\\alpha\\) là yếu tố tỷ lệ tùy ý (thường từ 2-10).\n\\(N_i\\) là số lượng nơ-ron input\n\\(N_o\\) là số lượng nơ-ron output.\n\n\nCác hàm acvtivation như Tanh thì phù hợp cho dự báo giá trị liên tục từ dữ liệu chuỗi, ReLU giúp cho quá trình training nhanh hơn và không gây ra vanishing problem do không bị chặn, Softmax thường dùng ở final layer cho bài toán classification, Sigmoid thường dùng cho hồi quy logic.\nNumber of neurons: Số lượng nơ-ron trong một lớp quyết định lượng thông tin mà mạng có thể lưu trữ. Nhiều nơ-ron giúp mạng học được các mẫu phức tạp hơn, nhưng cũng làm tăng nguy cơ overfitting (quá khớp) và yêu cầu nhiều tài nguyên tính toán hơn. Bạn có thể bắt đầu với một số lượng nơ-ron tương đối nhỏ, như 128 hoặc 256…"
  },
  {
    "objectID": "torch.html#kết-luận",
    "href": "torch.html#kết-luận",
    "title": "RNN and LSTM model",
    "section": "3 Kết luận:",
    "text": "3 Kết luận:\nNhư vậy, chúng ta đã được học về mô hình RNN và LSTM và cách xây dựng chúng trong R.\nNếu bạn có câu hỏi hay thắc mắc nào, đừng ngần ngại liên hệ với mình qua Gmail. Bên cạnh đó, nếu bạn muốn xem lại các bài viết trước đây của mình, hãy nhấn vào hai nút dưới đây để truy cập trang Rpubs hoặc mã nguồn trên Github. Rất vui được đồng hành cùng bạn, hẹn gặp lại! 😄😄😄\n\n\n\n    \n    \n    Contact Me\n    \n    \n    \n\n\n    \n        Contact Me\n        \n            Your Email:\n            \n            Please enter a valid email address.\n            Send Email\n        \n        \n            \n                \n                     View Code on GitHub\n                \n            \n        \n        \n            \n                \n                     Visit my RPubs"
  },
  {
    "objectID": "torch.html#mô-hình-rnn",
    "href": "torch.html#mô-hình-rnn",
    "title": "RNN and LSTM model",
    "section": "1 Mô hình RNN:",
    "text": "1 Mô hình RNN:\n\n1.1 Định nghĩa:\nĐiểm chung là cả hai mô hình đều thuộc phân lớp Deep learning - nghĩa là học máy sâu với đặc điểm chung là phân chia dữ liệu thành nhiều lớp và bắt đầu “học” dần qua từng lớp để đưa ra kết quả cuối cùng. Ở hình dưới đây, \\(X_o\\) đại diện cho dữ liệu đầu vào, \\(h_t\\) là output đầu ra của từng step và \\(A\\) là những gì đã “học” được tại step đó và được truyền cho step tiếp theo. Trong tài liệu chuẩn thì họ thường kí hiệu là \\(X_t\\), \\(Y_t\\), \\(h_{t-1}\\).\n\n  \n  \n  \n  \n    Hình 1: Minh họa về sự phân chia dữ liệu thành nhiều lớp\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nKhi nhìn hình thì bạn có thể bối rối chưa hiểu các kí tự và hình ảnh thì bạn có thể tưởng tượng học máy như 1 đứa trẻ và để nó có thể hiểu được câu: “Hôm nay con đi học” thì nó phải học từng chữ cái như: a,b,c,… trước ròi mới ghép thành từ đơn như: “Hôm”,“Nay”,… rồi ghép thành câu trên.\nVậy giả sử như hôm nay học được từ “Hôm” thì nó sẽ bắt đầu ghi nhớ từ đã học vào trong \\(A\\). Nếu sau này ta cần học máy hiểu câu “Hôm sau con đi chơi” thì tốc độ học của học máy sẽ nhanh lên vì thay vì nó phải học 5 chữ đơn như thông thường thì nó chỉ cần học 4 chữ còn lại trừ chữ “hôm”. Vậy bạn đã hiểu ý tưởng nền tảng của RNN rồi ha!\n\n\n1.2 Nguyên lí hoạt động:\nĐầu tiên, RNN sẽ tính toán hidden state là \\(h_t\\) với công thức là:\n\\[\n   \\mathbf{h}_t = \\text{activation}(\\mathbf{W}_\\text{hh} \\mathbf{h}_{t-1} + \\mathbf{W}_\\text{xh} \\mathbf{x}_t + \\mathbf{b}_\\text{h})\n\\] Sau đó, \\(h_t\\) sẽ được làm input cho các state sau và dựa vào đó để tính output với công thức là:\n\\[\ny_t = W_y \\cdot h_t + b_y\n\\]\nVí dụ: Mình muốn dự đoán hành động trong câu nói “I am reading book” bằng mô hình RNN như sau:\n\nBước 1: Chuyển đổi thành dạng số bằng embedding layer:\n\nMình sẽ gán từng từ đơn sang dạng số như:\n\n“I” -&gt; \\(x_1\\)\n“am” -&gt; \\(x_2\\)\n“reading” -&gt; \\(x_3\\)\n“book” -&gt; \\(x_4\\)\nBước 2: Thêm hidden layer và bắt đầu tính toán:\n\nCho input: “I” \\[\n   h_1 = \\tanh(W_x \\cdot x_1 + W_h \\cdot h_0 + b)\n\\]\nCho input: “am” \\[\n   h_2 = \\tanh(W_x \\cdot x_2 + W_h \\cdot h_1 + b)\n\\] Cho input: “reading” \\[\n   h_3 = \\tanh(W_x \\cdot x_3 + W_h \\cdot h_2 + b)\n\\]\nCho input: “book” \\[\n   h_4 = \\tanh(W_x \\cdot x_4 + W_h \\cdot h_3 + b)\n\\]\n\nBước 3: Tính toán output: Dùng hàm activation softmax để phân lớp theo xác suất.\n\n\\[\n\\hat{y} = \\text{softmax}(W_y \\cdot h_4 + b_y)\n\\] Nếu muốn hiểu thêm về cách hoạt động RNN, bạn có thể tham khảo link này: Recurrent Neural Network: Từ RNN đến LSTM.\n\n\n1.3 Vấn đề lớn của RNN:\nRNN có 1 vấn đề lớn là Vanishing Gradient nghĩa là mô hình sẽ không còn “học” thêm được nữa cho dù tăng số epochs. Nguyên nhân vì sao như vậy thì bạn có thể tham khảo phần chứng minh của anh Tuấn.\n\n  \n  \n  \n  \n    Hình 2: Vanishing Gradient Problem\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nVấn đề này sẽ làm network khó update weight dẫn tới thời gian học lâu và khó để đạt được output. Bạn có thể hiểu đơn giản như việc bạn học liên tục dẫn tới quá tải và RNN cũng không như vậy. Do đó, RNN chỉ học các thông tin từ state gần và đó là lí do ra đời LSTM - Long short term memory.\n\n\n\n\n\n\nWarning\n\n\n\nLưu ý: Điều này không có nghĩa LSTM luôn tốt hơn RNN vì có những bài toán với đầu vào đơn giản thì mô hình chỉ cần học các step đầu là đã “học” đầy đủ thông tin cần thiết. Mô hình LSTM phổ biến với các bài toán phức tạp như tự động dịch ngôn ngữ, ghi chép lại theo giọng nói…\n\n\n\n\n1.4 Mô hình LSTM:\nCó thể xem mô hình LSTM như biến thể của RNN. Về cấu trúc, LSTM phức tạp hơn RNN:\n\n  \n  \n  \n  \n    Hình 3: So sánh mô hình RNN và LSTM\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nCấu trúc cơ bản gồm:\n\nCổng quên (Forget Gate): có tác dụng quyết định thông tin nào cần bị quên trong trạng thái ô nhớ.\nCổng nhập (Input Gate): Xác định thông tin nào cần được ghi vào trạng thái ô nhớ.\nCổng xuất (Output Gate): Quyết định thông tin nào sẽ được xuất ra từ trạng thái ô nhớ để ảnh hưởng đến dự đoán tiếp theo.\n\nBạn có thể kham khảo thêm bài viết của dominhhai về cách hoạt động của RNN và LSTM để hiểu thêm.\nTiếp theo, ta sẽ bắt đầu xây dựng thử mô hình trong R."
  },
  {
    "objectID": "transfer.html",
    "href": "transfer.html",
    "title": "Mô hình Transformer",
    "section": "",
    "text": "Trong một nghiên cứu của (Jimeng Shi, Mahek Jain, and Giri Narasimhan 2022) về việc ứng dụng hàng loạt các mô hình thuộc phân lớp Deep learning và so sánh để chọn ra mô hình dự đoán tốt nhất chỉ số PM2.5 (là chỉ số đo lường lượng hạt bụi li ti có trong không khí với kích thước 2,5 micron trở xuống). Kết quả có bao gồm: “Mô hình Transformer dự đoán tốt nhất cho dự đoán long-term trong tương lai. LSTM và GRU vượt trội hơn RNN cho các dự đoán short-term.”\nVậy mô hình Transformer là gì ? Chúng ta sẽ học nó ở bài này."
  },
  {
    "objectID": "transfer.html#mô-hình-transformer",
    "href": "transfer.html#mô-hình-transformer",
    "title": "Mô hình Transformer",
    "section": "1 Mô hình Transformer:",
    "text": "1 Mô hình Transformer:\n\n1.1 Giới thiệu:\n\n\nCode\npacman::p_load(torch,\n               dplyr,\n               tidyverse)\n\n\nChắc các bạn đã quá quen thuộc với Chatgpt - một công cụ AI mạnh mẽ trong thời gian gần đây với lượng người sử dụng cực kì cao. Như biểu đồ dưới đây, từ khi launched Chatgpt chỉ tốn 5 ngày để đạt 1 triệu người sử dụng và ngoài ra theo thống kê đến tháng 2/2024, Chatgpt đã có tới 1.6 tỉ lượt thăm quan.\n\n  \n  \n  \n  \n    Hình 1: Thời gian để đạt 1 triệu người dùng của Chatgpt\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nÝ tưởng ban đầu của Chatgpt chính là dựa trên cấu trúc mô hình Transformer - 1 dạng Deep learning chỉ mới được giới thiệu với thế giới từ năm 2017 nhưng có sức ảnh hưởng rất lớn, nhất là trong lĩnh vực Generative AI.\nKhái niệm về mô hình này được giới thiệu lần đầu vào năm 2017 của các nhà nghiên cứu của Google trong bài tài liệu Attention is all you need. Mô hình này dựa trên ý tưởng là xác định các thành phần quan trọng trong sequence và cho phép mô hình đưa ra quyết định dựa trên sự phụ thuộc giữa các phần tử trong đầu vào, bất kể khoảng cách của chúng với nhau, quá trình này gọi là Attention mechanisms. Dựa vào đó, mô hình Transformer sẽ chuyển đổi một chuỗi input thành 1 chuỗi output khác nhưng vẫn đảm bảo giữ lại các đặc điểm quan trọng của sequence đó.\n\n  \n  \n  \n  \n    Hình 2: Input và output của mô hình\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nVí dụ với việc dịch thuật văn bản sẽ có những từ trong câu, câu trong đoạn văn đại diện cho ý nghĩa toàn câu, toàn đoạn văn. Hay với về việc phân tích demand trong time series, lượng mua hàng vào những ngày nghỉ, cuối tuần sẽ đưa ra insight tốt hơn vào các ngày bình thường. Như vậy, bạn thấy đó, mô hình Transformer phù hợp với các task thuộc dạng dịch văn bản, dự đoán chuỗi hành động liên tiếp của đối tượng,…\n\n\n1.2 So sánh với RNN, LSTM:\nNhư hình trên, bạn có thể thấy mô hình Transformer cũng gồm Encoder và Decoder giống như cách hoạt động của RNN, LSTM. Nhưng khác nhau ở chỗ, thay vì cơ chế đó hoạt động ở từng timestep liên tục nhau như RNN thì ở Transformer input được đẩy vào cùng 1 lúc (nghĩa là không còn học theo từng timestep nữa). Nhờ vậy, Transformer sẽ xác định được các thành phần quan trọng trong sequence và lựa chọn thông số cho chúng (Hiểu đơn giản như việc bạn cần nghe hết đoạn thoại của người đối diện thì mới hiểu được họ đang nói gì và chọn lọc các keyword để xác định ý chính của đoạn văn đó và đó là ý tưởng chính xây dựng lên mô hình này).\n\n  \n  \n  \n  \n    Hình 3: So sánh performance giữa mô hình Transformer và LSTM\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nNgoài ra, chính cơ chế Self-attention đã tạo sự khác biệt lớn cho mô hình Transformer so với các mô hình khác. Như hình dưới đây là nghiên cứu của về việc ứng dụng Deep learning để tạo phụ đề cho video. Nghiên cứu đã so sánh performance giữa 2 mô hình (i) Transformer-based model và (ii) LSTM-based model khi hyperparamater tuning. Kết quả cho thấy sự vượt trội của Transformer khi chỉ số accuracy lên tới 97%.\nTiếp theo, chúng ta sẽ tìm hiểu về các thành phần chính trong mô hình Transformer."
  },
  {
    "objectID": "transfer.html#các-thành-phần-cơ-bản-trong-transformer",
    "href": "transfer.html#các-thành-phần-cơ-bản-trong-transformer",
    "title": "Mô hình Transformer",
    "section": "2 Các thành phần cơ bản trong Transformer:",
    "text": "2 Các thành phần cơ bản trong Transformer:\nVề nguyên lí hoạt động, mình sẽ chia thành các phần như sau theo cách giải thích cá nhân để giúp mọi người dễ hiểu:\n\nThành phần 1: Tensor\n\nĐầu tiên, các bạn phải hiểu về tensor là gì? Thì nó là một đối tượng toán học nhằm tổng hợp hóa 1 hoặc nhiều chiều trong 1 object. Dạng đơn giản của tensor như là scalar (số đơn giản), vector (chuỗi các số),…\n\n  \n  \n  \n  \n    Hình 4: Tensor là gì\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nVà mục đích của việc chuyển đổi dữ liệu sang dạng tensor là để giúp cho việc tính toán trên GPU nhanh hơn và tăng tốc độ training machine learning model. Ngoài ra, vẫn có các thông tin khác hay về tensor trong R, bạn có thể kham khảo link này: Tensors.\n\nThành phần 2: Embedding và positional encoding\n\nKhi dữ liệu được đưa vào, nó sẽ trải qua bước embedding (cách để biểu diễn dữ liệu đa chiều trong không gian ít chiều). Nếu dữ liệu của bạn dạng hình ảnh hoặc dạng văn bản thì bước này rất cần thiết (vì các mô hình machine learning chỉ làm việc được với dữ liệu dạng số).\nNgoài ra, vì mô hình Transformer không có khả năng xử lý dữ liệu theo thứ tự tuần tự (khác với RNN hoặc LSTM), nó sẽ cần một chỉ báo để chỉ ra thứ tự của các bước trong chuỗi, gọi là Postitional encoding. Bạn có thể kham khảo bài viết của Mehreen Saeed. Và code trong R sẽ ví dự như sau:\n\n\nCode\npositional_encoding &lt;- function(seq_len, d, n = 10000) {\n  P &lt;- matrix(0, nrow = seq_len, ncol = d)\n  \n  for (k in 1:seq_len) {\n    for (i in 0:(d / 2 - 1)) {\n      denominator &lt;- n^(2 * i / d)\n      P[k, 2 * i + 1] &lt;- sin(k / denominator)\n      P[k, 2 * i + 2] &lt;- cos(k / denominator)\n    }\n  }\n  \n  return(P)\n}\n\n\n\nThành phần 3: Self-attention mechanism\n\nĐây là một cơ chế đặc biệt của Transformer, cho phép mô hình chú ý đến tất cả các bước thời gian trước đó trong chuỗi tại mỗi bước. Điều này giúp mô hình nắm bắt được các mối quan hệ dài hạn và sự liên hệ giữa các bước thời gian với nhau (giúp tránh gặp vấn đề ghi nhớ ngắn hạn như RNN). Bạn có thể xem Self-attention như là cấu trúc chung nhất, còn khi xây dựng mô hình người ta có thể biến tấu tùy vào nhu cầu.\nNhư ở Encoder thì sử dụng Multi-Head Attention có thể tính toán chú ý nhiều lần song song (khác với self -attention chỉ tính toán cho single sequence) . Mỗi “đầu” có thể chú ý đến những khía cạnh khác nhau của các mối quan hệ thời gian trong chuỗi. Ví dụ, một đầu có thể chú ý đến các mẫu ngắn hạn (ví dụ: sự dao động hàng ngày), trong khi một đầu khác có thể nắm bắt các xu hướng dài hạn (ví dụ: chu kỳ mùa). Trong R thì đã có sẵn hàm nn_multihead_attention() trong package torch.\nCòn đối với Decoder thì dùng Masked Multi-Head Attention để đảm bảo rằng khi dự báo giá trị tiếp theo trong chuỗi thời gian, mô hình chỉ có thể chú ý đến các bước thời gian trước đó mà không nhìn vào các bước thời gian tương lai. So sánh với Self-attention thì bạn cần thêm bước Masked score thôi. Trong R sẽ được code như sau:\n\n\n\n\n\n\nShow structure\nmask_self_attention &lt;- nn_module(\n  initialize = function(embed_dim, num_heads) {\n    self$embed_dim &lt;- embed_dim\n    self$num_heads &lt;- num_heads\n    self$head_dim &lt;- embed_dim / num_heads\n\n    if (embed_dim %% num_heads != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    # Linear layers for Q, K, V \n    self$query &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$key &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$value &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    \n    # Final linear layer after concatenating heads\n    self$out &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    \n  },\n  \n  forward = function(x) {\n    batch_size &lt;- x$size(1)\n    seq_leng &lt;- x$size(2)\n    \n    # Linear projections for Q, K, V\n    Q &lt;- self$query(x)  # (batch_size, seq_leng, embed_dim)\n    K &lt;- self$key(x)\n    V &lt;- self$value(x)\n    \n    # Reshape to separate heads: (batch_size, num_heads, seq_leng, head_dim)\n    Q &lt;- Q$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    K &lt;- K$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    V &lt;- V$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    \n    # Compute Matmul and scale:\n    d_k &lt;- self$head_dim\n    attention_scores &lt;- torch_matmul(Q, torch_transpose(K, -1, -2)) / sqrt(d_k)\n    \n    # Apply mask if provided\n    mask &lt;- torch_tril(torch_ones(c(seq_leng, seq_leng)))\n    \n    if (!is.null(mask)) {\n      \n      masked_attention_scores &lt;- attention_scores$masked_fill(mask == 0, -Inf)\n      \n    } else {\n      print(\"Warning: No mask provided\")\n    }\n    \n    # Compute attention weights\n    weights &lt;- nnf_softmax(masked_attention_scores, dim = -1)\n    \n    # Apply weights to V\n    attn_output &lt;- torch_matmul(weights, V)  \n    \n    # Reshape again:\n    attn_output &lt;- attn_output$transpose(2, 3)$contiguous()$view(c(batch_size, seq_leng, self$embed_dim))\n    \n    # Final linear layer\n    output &lt;- self$out(attn_output)\n    return(output)\n  }\n)\n\n\n\n\nNgoài ra, trong Decoder còn có Cross-attention nhưng nó hơi phức tạp nên mình sẽ giới thiệu sau.\n\nThành phần 4: Sub-layer\n\nBạn sẽ để ý thấy các phép tính toán trong mô hình sẽ luôn kèm theo bộ phận Add & Norm để lưu giữ residual và cộng vào output được tạo sau khi kết thúc các phép tính đó. Việc này giúp giảm thiểu vấn đề vanishing gradient đã đề cập ở trang trước và giúp cho mô hình học sâu hơn. Trong R bạn chỉ cần thêm lớp này bằng hàm nn_layer_norm().\n\nThành phần 5: Feed-Forward Neural Networks\n\nSau khi tính toán chú ý, đại diện của từng bước thời gian sẽ được đưa qua một mạng nơ-ron Feed-Forward (FFN), thường bao gồm: (i) Một phép biến đổi tuyến tính (lớp kết nối đầy đủ), (ii) Hàm kích hoạt ReLU và (iii) Một phép biến đổi tuyến tính nữa. Trong R sẽ code như này:\n\n\nCode\nfeed_forward &lt;- nn_sequential(\n      nn_linear(d_model, d_ff),\n      nn_relu(),\n      nn_linear(d_ff, d_model)\n    )"
  },
  {
    "objectID": "transfer.html#các-thành-phần-chính",
    "href": "transfer.html#các-thành-phần-chính",
    "title": "Mô hình Transformer",
    "section": "3 Các thành phần chính:",
    "text": "3 Các thành phần chính:\nSau khi hiểu rõ các thành phần cần thiết, ta sẽ ngó qua workflow đầy đủ của mô hình Transformer.Nếu bạn chưa hiểu thì có thể kham khảo link này datacamp\n\n  \n  \n  \n  \n    Hình 6: Workflow của mô hình Transformer\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nMình sẽ trình bày theo cách cá nhân để giúp mọi người hiểu rõ hơn:\n\nBước 1: Xử lí input: sẽ gồm bước Embedding dữ liệu sau đó cộng thêm Positional encoding. Lưu ý: input cho encoder và decoder là khác nhau, encoder sẽ nhận đầu vào là các biến dự báo (ví dụ: giá trị lag của time series,…) và decoder sẽ nhận đầu vào là biến target (là kết quả bạn mong muốn mô hình dự báo đúng).\nBước 2: Encoder output: Khi dữ liệu đi vào encoder block thì sẽ trải qua lớp multi-head attention* và feed forward và các lớp sub-layer normalization. Lưu ý: khi normalizing thì phải normalize (kết quả từ lớp trước + input ban đầu), bạn có thể nhìn ảnh dưới đây để dễ hiểu hơn.\n\n\n  \n  \n  \n  \n    Hình 7: Normalization và residual connection sau lớp Multi-Head Attention\n  \n  \n  \n  \n    Source: Link to Image\n  \n\n\nBước 3: Add encoder output to decoder: Sau khi Decoder thực hiện tính toán cho dữ liệu thông qua layer Mask multi-head attention và Normalization thì sẽ đến bước Cross-attention (Mặc dù ở hình trên hoặc các tài liệu khác mà bạn từng đọc sẽ để là layer multi-head attention nhưng thực chất layer cross-attention mới đúng).\n\nVậy cross-attention có gì đặc biệt? Ta sẽ nhìn sơ qua cấu trúc của nó thì sẽ nhận ra điểm khác biệt so với self-attention thông thường là cross-attention sẽ nhận dữ liệu từ 2 nguồn: (i) output của encoder gán cho Q và (ii) input của decoder gán cho V, K.\n\nCross attention:Self attention:\n\n\n\n  \n  \n  \n  \n    Hình 8: Workflow của Cross-attention\n  \n  \n  \n  \n    Source: Link to Image\n  \n\n\n\n\n  \n  \n  \n  \n    Hình 9: Workflow của Self-attention\n  \n  \n  \n  \n    Source: Link to Image\n  \n\n\n\n\nVề code trong R sẽ như sau:\n\n\nShow structure\ncross_attention &lt;- nn_module(\n  initialize = function(embed_dim, num_heads) {\n    self$embed_dim &lt;- embed_dim\n    self$num_heads &lt;- num_heads\n    self$head_dim &lt;- embed_dim / num_heads\n    \n    if (self$head_dim %% 1 != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    self$query &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$key &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$value &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$out &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n  },\n  \n  forward = function(decoder_input, encoder_output, mask = NULL) {\n    batch_size &lt;- decoder_input$size(1)\n    seq_leng_dec &lt;- decoder_input$size(2)\n    seq_leng_enc &lt;- encoder_output$size(2)\n    \n    Q &lt;- self$query(decoder_input)\n    K &lt;- self$key(encoder_output)\n    V &lt;- self$value(encoder_output)\n    \n    Q &lt;- Q$view(c(batch_size, seq_leng_dec, self$num_heads, self$head_dim))$transpose(2, 3)\n    K &lt;- K$view(c(batch_size, seq_leng_enc, self$num_heads, self$head_dim))$transpose(2, 3)\n    V &lt;- V$view(c(batch_size, seq_leng_enc, self$num_heads, self$head_dim))$transpose(2, 3)\n    \n    d_k &lt;- self$head_dim\n    attention_scores &lt;- torch_matmul(Q, torch_transpose(K, -1, -2)) / sqrt(d_k)\n    \n    weights &lt;- nnf_softmax(attention_scores, dim = -1)\n    \n    attn_output &lt;- torch_matmul(weights, V)\n    \n    attn_output &lt;- attn_output$transpose(2, 3)$contiguous()$view(c(batch_size, seq_leng_dec, self$embed_dim))\n    \n    output &lt;- self$out(attn_output)\n    return(output)\n  }\n)\n\n\nKết quả sau đó sẽ được đẩy qua layer feed forward và normalization để trả về output (giống như encoder).\n\nBước 4: Output of decoder: Cuối cùng, output của decoder sẽ qua 2 layer linear và softmax để tìm ra output có xác suất cao nhất (nghĩa là output đó sẽ có ý nghĩa nhất trong sequence để dự báo cho các step sau).\n\n\n  \n  \n  \n  \n    Hình 10: Output của mô hình\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nNhư vậy, chúng ta đã lướt sơ qua cách hoạt động và các lưu ý của mô hình Transformer. Tiếp theo, mình sẽ thử xây dựng trong R và dùng nó để xử lí task dự báo chuỗi thời gian.\n\n\n\n    \n    \n    Go to Next Page\n    \n\n\n    \n        \n            Go to Next Page\n            ➔"
  },
  {
    "objectID": "practice.html",
    "href": "practice.html",
    "title": "Time series forecasting",
    "section": "",
    "text": "Khi bạn cần xây dựng các mô hình Deep learning phức tạp hơn thì package torch trong R giống với PyTorch trong Python thường dùng để xây dựng mô hình machine learning và nó sẽ là công cụ mạnh mẽ có thể hỗ trợ bạn (Nếu bạn chưa biết thì hầu hết packages để train machine learning model trong R đều thực hiện thông qua Python và được bắt cầu nối bằng package reticulate).\nĐể học hết về torch thì bạn có thể tham khảo các link sau:\n\nSách Deep Learning and Scientific Computing with R torch của Sigrid Keydana.\nMột loạt bài post từ posit blog.\n\nCòn ở bài viết này, mình chỉ giới thiệu cơ bản cách sử dụng torch trong R để xây dựng mô hình.\n\n\n\n\n\n\nTip\n\n\n\nĐể tải torch vào máy local thì bạn sử dụng cú pháp install.package(\"torch\")\n\n\n\n\nỞ đây, mình sẽ giới thiệu cơ bản để mọi người có kiến thức cơ bản nhất về torch\nĐầu tiên, để dùng package torch trong R thì ta cần chuyển đổi object sang class tensor thì dùng hàm torch_tensor(). Đây là ví dụ về:\n\n\nCode\nlibrary(torch)\nm&lt;-torch_tensor(array(1:24, dim = c(4, 3, 2)))\nclass(m)\n\n\n[1] \"torch_tensor\" \"R7\"          \n\n\nNgoài ra, trong object tensor còn chứa thêm thông tin khác như là: \\(dtype* sẽ return data type (ví dụ như object dưới đây là dạng *long integer*), *\\)device return nơi tensor object được lưu trữ, $shape return dimensions của object.\n\n\nCode\nm$dtype\n\n\ntorch_Long\n\n\nCode\nm$device\n\n\ntorch_device(type='cpu') \n\n\nCode\nm$shape\n\n\n[1] 4 3 2\n\n\nVí dụ ta có thể simulate công thức đơn giản như sau bằng package torch: \\(f(x) = xw + b\\)\n\n\nCode\nx &lt;- torch_randn(100, 3)\nw &lt;- torch_randn(3, 1, requires_grad = TRUE)\nb &lt;- torch_zeros(1, 1, requires_grad = TRUE)\ny &lt;- x$matmul(w) + b\nhead(y)\n\n\ntorch_tensor\n 0.8659\n 1.1373\n-1.1229\n-0.8550\n 1.3113\n 0.4500\n[ CPUFloatType{6,1} ][ grad_fn = &lt;SliceBackward0&gt; ]\n\n\n\n\n\nTiếp theo, mình sẽ xây dựng mô hình Transformer để dự báo giá cổ phiếu của Google từ nguồn Yahoo Finance.\n\n\nCode\n#### Call packages-------------------------------------------------------------\npacman::p_load(quantmod,\n               torch,\n               dplyr,\n               dygraphs)\n#### Input---------------------------------------------------------------------\ngetSymbols(\"GOOG\", src = \"yahoo\", from = \"2020-01-01\", to = \"2022-01-01\")\n\n\n[1] \"GOOG\"\n\n\nCode\nprice_data &lt;- GOOG$GOOG.Close\nprice_data_xts &lt;- xts(price_data, \n                     order.by = index(price_data))\n\ncolors&lt;-RColorBrewer::brewer.pal(9, \"Blues\")[c(4, 6, 8)]\n\ndygraph(price_data_xts, main = \"Google Stock Price (2020 - 2022)\", ylab = \"Price ($)\") |&gt; \n  dyRangeSelector(height = 20) |&gt; \n  dyOptions(\n    fillGraph = TRUE,  \n    colors = colors,   \n    strokeWidth = 2,   \n    gridLineColor = \"gray\",  \n    gridLineWidth = 0.5,     \n    drawPoints = TRUE,   \n    pointSize = 4,       \n    pointShape = \"diamond\" \n  ) |&gt; \n  dyLegend(show = \"follow\") \n\n\n\n\n\n\nNhư biểu đồ, ta thấy giá cổ phiếu tăng cao chóng mặt và mức biến động khá mức tạp (lúc lên lúc xuống). Task này khá khó nên ta sẽ tìm hiểu xem performance của mô hình Transformer sẽ như thế nào.\nMô hình đầy đủ sẽ được code như sau:\n\n\nShow structure\n#### Transform input----------------------------------------------------------------\ncreate_supervised_data &lt;- function(series, n) {\n  series &lt;- as.vector(series)\n  data &lt;- data.frame(series)\n  \n  for (i in 1:n) {\n    lagged_column &lt;- lag(series, i)\n    data &lt;- cbind(data, lagged_column)\n  }\n  \n  colnames(data) &lt;- c('t',paste0('t', 1:n))\n\n  data &lt;- na.omit(data)\n  \n  return(data)\n}\n\nseq_leng &lt;- 50\ndim_model &lt;- 32\n\nsupervised_data &lt;- create_supervised_data(price_data, n = seq_leng)\n\nsupervised_data &lt;- scale(supervised_data)\n\n\nx_data &lt;- torch_tensor(as.matrix(supervised_data[, 2:(seq_leng+1)]), dtype = torch_float())  # Features (lags)\ny_data &lt;- torch_tensor(as.matrix(supervised_data[, 1]), dtype = torch_float())    # Target\n\n# Reshape x_data to match (batch_size, seq_leng, feature_size)\nx_data &lt;- x_data$view(c(nrow(x_data), seq_leng, 1))  # (batch_size, seq_leng, feature_size)\ny_data &lt;- y_data$view(c(nrow(y_data), 1, 1)) \n\n# Split the data into training and testing sets (80% for training, 20% for testing)\ntrain_size &lt;- round(0.8 * nrow(supervised_data))\n\nx_train &lt;- x_data[1:train_size, , drop = FALSE]  \ny_train &lt;- y_data[1:train_size]\n\nx_test &lt;- x_data[(train_size + 1):nrow(supervised_data), , drop = FALSE]\ny_test &lt;- y_data[(train_size + 1):nrow(supervised_data)]\n\n#### Build components of model----------------------------------------------------------------\n### Positional encoding:\npositional_encoding &lt;- function(seq_leng, d_model, n = 10000) {\n  if (missing(seq_leng) || missing(d_model)) {\n    stop(\"'seq_leng' and 'd_model' must be provided.\")\n  }\n  \n  P &lt;- matrix(0, nrow = seq_leng, ncol = d_model)  \n  \n  for (k in 1:seq_leng) {\n    for (i in 0:(d_model / 2 - 1)) {\n      denominator &lt;- n^(2 * i / d_model)\n      P[k, 2 * i + 1] &lt;- sin(k / denominator)\n      P[k, 2 * i + 2] &lt;- cos(k / denominator)\n    }\n  }\n  \n  return(P)\n}\n\nen_pe &lt;- positional_encoding(x_data$size(2),dim_model, n = 10000)\nde_pe &lt;- positional_encoding(y_data$size(2),dim_model, n = 10000)\n\n### Encoder block:\nencoder_layer &lt;- nn_module(\n  \"TransformerEncoderLayer\",\n  \n  initialize = function(d_model, num_heads, d_ff) {\n    \n    # Multi-Head Attention\n    self$multihead_attention &lt;- nn_multihead_attention(embed_dim = d_model, num_heads = num_heads)\n    \n    # Feedforward Network (Fully Connected)\n    self$feed_forward &lt;- nn_sequential(\n      nn_linear(d_model, d_ff),\n      nn_relu(),\n      nn_linear(d_ff, d_model)\n    )\n    \n    self$layer_norm &lt;- nn_layer_norm(d_model)\n  \n  },\n  \n  forward = function(x) {\n\n    attn_output &lt;- self$multihead_attention(x, x, x) \n    x &lt;- x + attn_output[[1]]\n    x &lt;- self$layer_norm(x) \n    \n    # Feedforward network with residual connection\n    ff_output &lt;- self$feed_forward(x)\n    x &lt;- x + ff_output\n    x &lt;- self$layer_norm(x)\n    \n    return(x)\n  }\n)\n\n### Mask function:\nmask_self_attention &lt;- nn_module(\n  initialize = function(embed_dim, num_heads) {\n    self$embed_dim &lt;- embed_dim\n    self$num_heads &lt;- num_heads\n    self$head_dim &lt;- embed_dim / num_heads\n    \n    # Ensure that self$head_dim is a scalar\n    if (self$head_dim %% 1 != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    if (embed_dim %% num_heads != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    # Linear layers for Q, K, V \n    self$query &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$key &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$value &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    \n    # Final linear layer after concatenating heads\n    self$out &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    \n  },\n  \n  forward = function(x) {\n    batch_size &lt;- x$size(1)\n    seq_leng &lt;- x$size(2)\n    \n    # Linear projections for Q, K, V\n    Q &lt;- self$query(x)  # (batch_size, seq_leng, embed_dim)\n    K &lt;- self$key(x)\n    V &lt;- self$value(x)\n    \n    # Reshape to separate heads: (batch_size, num_heads, seq_leng, head_dim)\n    Q &lt;- Q$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    K &lt;- K$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    V &lt;- V$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    \n    # Compute attention scores\n    d_k &lt;- self$head_dim\n    attention_scores &lt;- torch_matmul(Q, torch_transpose(K, -1, -2)) / sqrt(d_k)\n    \n    # Apply mask if provided\n    mask &lt;- torch_tril(torch_ones(c(seq_leng, seq_leng)))\n    \n    if (!is.null(mask)) {\n      \n      masked_attention_scores &lt;- attention_scores$masked_fill(mask == 0, -Inf)\n      \n    } else {\n      print(\"Warning: No mask provided\")\n    }\n    \n    # Compute attention weights\n    weights &lt;- nnf_softmax(masked_attention_scores, dim = -1)\n    \n    # Apply weights to V\n    attn_output &lt;- torch_matmul(weights, V)  # (batch_size, num_heads, seq_leng, head_dim)\n    \n    \n    attn_output &lt;- attn_output$transpose(2, 3)$contiguous()$view(c(batch_size, seq_leng, self$embed_dim))\n    \n    \n    output &lt;- self$out(attn_output)\n    return(output)\n  }\n)\n\n### Cross attention:\ncross_attention &lt;- nn_module(\n  initialize = function(embed_dim, num_heads) {\n    self$embed_dim &lt;- embed_dim\n    self$num_heads &lt;- num_heads\n    self$head_dim &lt;- embed_dim / num_heads\n    \n    if (self$head_dim %% 1 != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    self$query &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$key &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$value &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$out &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n  },\n  \n  forward = function(decoder_input, encoder_output, mask = NULL) {\n    batch_size &lt;- decoder_input$size(1)\n    seq_leng_dec &lt;- decoder_input$size(2)\n    seq_leng_enc &lt;- encoder_output$size(2)\n    \n    Q &lt;- self$query(decoder_input)\n    K &lt;- self$key(encoder_output)\n    V &lt;- self$value(encoder_output)\n    \n    Q &lt;- Q$view(c(batch_size, seq_leng_dec, self$num_heads, self$head_dim))$transpose(2, 3)\n    K &lt;- K$view(c(batch_size, seq_leng_enc, self$num_heads, self$head_dim))$transpose(2, 3)\n    V &lt;- V$view(c(batch_size, seq_leng_enc, self$num_heads, self$head_dim))$transpose(2, 3)\n    \n    d_k &lt;- self$head_dim\n    attention_scores &lt;- torch_matmul(Q, torch_transpose(K, -1, -2)) / sqrt(d_k)\n    \n    weights &lt;- nnf_softmax(attention_scores, dim = -1)\n    \n    attn_output &lt;- torch_matmul(weights, V)\n    \n    attn_output &lt;- attn_output$transpose(2, 3)$contiguous()$view(c(batch_size, seq_leng_dec, self$embed_dim))\n    \n    output &lt;- self$out(attn_output)\n    return(output)\n  }\n)\n\n### Decoder Layer\ndecoder_layer &lt;- nn_module(\n  \"TransformerDecoderLayer\",\n  \n  initialize = function(d_model, num_heads, d_ff) {\n    self$mask_self_attention &lt;- mask_self_attention(embed_dim = d_model, num_heads = num_heads)\n    self$cross_attention &lt;- cross_attention(embed_dim = d_model, num_heads = num_heads)\n    self$feed_forward &lt;- nn_sequential(\n      nn_linear(d_model, d_ff),\n      nn_relu(),\n      nn_linear(d_ff, d_model)\n    )\n    \n    self$layer_norm &lt;- nn_layer_norm(d_model)\n  },\n  \n  forward = function(x, encoder_output) {\n    # Masked Self-Attention\n    mask_output &lt;- self$mask_self_attention(x)\n    x &lt;- x + mask_output\n    x &lt;- self$layer_norm(x)\n    \n    # Encoder-Decoder Multi-Head Attention\n    cross_output &lt;- self$cross_attention(x, encoder_output)\n    x &lt;- x + cross_output\n    x &lt;- self$layer_norm(x)\n    \n    # Feedforward Network\n    ff_output &lt;- self$feed_forward(x)\n    x &lt;- x + ff_output\n    x &lt;- self$layer_norm(x)\n    \n    return(x)\n  }\n)\n\n### Final transformer model: \ntransformer &lt;- nn_module(\n  \"Transformer\",\n  \n  initialize = function(d_model, seq_leng, num_heads, d_ff, num_encoder_layers, num_decoder_layers) {\n    self$d_model &lt;- d_model\n    self$num_heads &lt;- num_heads\n    self$d_ff &lt;- d_ff\n    self$num_encoder_layers &lt;- num_encoder_layers\n    self$num_decoder_layers &lt;- num_decoder_layers\n    self$seq_leng &lt;- seq_leng\n    self$en_pe &lt;- en_pe\n    self$de_pe &lt;- de_pe\n    \n    # Encoder layers\n    self$encoder_layers &lt;- nn_module_list(\n      lapply(1:num_encoder_layers, function(i) {\n        encoder_layer(d_model, num_heads, d_ff)\n      })\n    )\n    \n    # Decoder layers\n    self$decoder_layers &lt;- nn_module_list(\n      lapply(1:num_decoder_layers, function(i) {\n        decoder_layer(d_model, num_heads, d_ff)\n      })\n    )\n    \n    # Final output layer\n    self$output_layer &lt;- nn_linear(d_model, 1)  # Output layer to predict a single value\n    \n  },\n  \n  forward = function(src, trg) {\n    \n    src &lt;- src + self$en_pe  \n    trg &lt;- trg + self$de_pe\n    \n    # Encoder forward pass\n    encoder_output &lt;- src\n    for (i in 1:self$num_encoder_layers) {\n      encoder_output &lt;- self$encoder_layers[[i]](encoder_output)\n    }\n    \n    # Decoder forward pass\n    decoder_output &lt;- trg\n    for (i in 1:self$num_decoder_layers) {\n      decoder_output &lt;- self$decoder_layers[[i]](decoder_output, encoder_output)\n    }\n  \n    # Apply final output layer\n    output &lt;- self$output_layer(decoder_output)\n    \n    return(output)\n  }\n)\n\n#### Training----------------------------------------------------------------\nmodel &lt;- transformer(\n  d_model = dim_model,         # Embedding dimension\n  seq_leng = seq_leng,        # Sequence length\n  num_heads = 8,        # Number of heads\n  d_ff = seq_leng,           # Dimension of the feedforward layer\n  num_encoder_layers = 6, \n  num_decoder_layers = 6\n)\n\n\n#### Training----------------------------------------------------------------\nepochs &lt;- 200\nloss_fn &lt;- nn_mse_loss()\noptimizer &lt;- optim_adam(model$parameters, lr = 1e-3)\n\nfor (epoch in 1:epochs) {\n  model$train()\n  optimizer$zero_grad()\n  \n  # Forward pass\n  y_train_pred &lt;- model(x_train, y_train) \n  \n  # Compute the loss\n  loss &lt;- loss_fn(y_train_pred, y_train)\n  \n  # Backpropagation and optimization\n  loss$backward()\n  optimizer$step()\n  \n  if (epoch %% 10 == 0) {\n    cat(\"Epoch: \", epoch, \" Loss: \", loss$item(), \"\\n\")\n  }\n}\n\n#### Predictions----------------------------------------------------------------\nmodel$eval()\n\n# Make predictions on the test data\ny_test_pred &lt;- model(x_test, y_test)  # Use the test data for both input and output during prediction\n\n# Convert tensors to numeric values for comparison\n\ny_test_pred&lt;- as.numeric(as.array(y_test_pred$cpu()))\n\n#### Evaluating----------------------------------------------------------------\nlibrary(highcharter)\ny_train_pred &lt;- as.numeric(as.array(y_train_pred$cpu()))\ny_train &lt;- as.numeric(as.array(y_train$cpu()))\ny_test &lt;- as.numeric(as.array(y_test$cpu()))\n\ncomparison &lt;- data.frame(\n  time = 1:nrow(supervised_data),\n  actual = c(y_train,y_test),\n  forecast = c(y_train_pred,y_test_pred)\n)\n\n# Compare only errors:\nerror&lt;-highchart() |&gt;\n  hc_title(text = \"Evaluating error of model\") |&gt;\n  hc_xAxis(\n    categories = time,\n    title = list(text = \"Time\")\n  ) |&gt;\n  hc_yAxis(\n    title = list(text = \"Value\"),\n    plotLines = list(list(\n      value = 0,\n      width = 1,\n      color = \"gray\"\n    ))\n  ) |&gt; \n  hc_add_series(\n    name = \"Error\",\n    data = (y_test_pred - y_test)/y_test,\n    type = \"line\",\n    color = \"red\"  # Blue color for actual data\n  ) |&gt;\n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) |&gt;\n  hc_legend(\n    enabled = TRUE\n  )\n\n\n# Compare all:\nall&lt;-highchart() |&gt;\n  hc_title(text = \"Model Predictions vs Actual Values\") |&gt;\n  hc_xAxis(\n    categories = time,\n    title = list(text = \"Time\")\n  ) |&gt;\n  hc_yAxis(\n    title = list(text = \"Value\"),\n    plotLines = list(list(\n      value = 0,\n      width = 1,\n      color = \"gray\"\n    ))\n  ) |&gt; \n  hc_add_series(\n    name = \"Actual Data\",\n    data = comparison$actual,\n    type = \"line\",\n    color = \"#1f77b4\"  # Blue color for actual data\n  ) |&gt;\n  hc_add_series(\n    name = \"Forecast\",\n    data = comparison$forecast,\n    type = \"line\",\n    color = \"#ff7f0e\"  # Orange color for forecast data\n  ) |&gt; \n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) |&gt;\n  hc_legend(\n    enabled = TRUE\n  )\n\n\n\n\n\nĐầu tiên ta sẽ đánh giá về sai số của mô hình khi dùng testing data. Kết quả khá ổn khi sai số khoảng (0.04,0.12).\n\n  \n\nVà còn nhìn tổng quan hết thì ta thấy mô hình dự đoán khá sát với training data nhưng với testing data thì vẫn chênh lệch thấp hơn thực tế (dấu hiệu cho thấy mô hình đang bị overfitting)."
  },
  {
    "objectID": "practice.html#thực-hành-trong-r",
    "href": "practice.html#thực-hành-trong-r",
    "title": "Time series forecasting",
    "section": "",
    "text": "Khi bạn cần xây dựng các mô hình Deep learning phức tạp hơn thì package torch trong R giống với PyTorch trong Python thường dùng để xây dựng mô hình machine learning và nó sẽ là công cụ mạnh mẽ có thể hỗ trợ bạn (Nếu bạn chưa biết thì hầu hết packages để train machine learning model trong R đều thực hiện thông qua Python và được bắt cầu nối bằng package reticulate).\nĐể học hết về torch thì bạn có thể tham khảo các link sau:\n\nSách Deep Learning and Scientific Computing with R torch của Sigrid Keydana.\nMột loạt bài post từ posit blog.\n\nCòn ở bài viết này, mình chỉ giới thiệu cơ bản cách sử dụng torch trong R để xây dựng mô hình.\n\n\n\n\n\n\nTip\n\n\n\nĐể tải torch vào máy local thì bạn sử dụng cú pháp install.package(\"torch\")\n\n\n\n\nỞ đây, mình sẽ giới thiệu cơ bản để mọi người có kiến thức cơ bản nhất về torch\nĐầu tiên, để dùng package torch trong R thì ta cần chuyển đổi object sang class tensor thì dùng hàm torch_tensor(). Đây là ví dụ về:\n\n\nCode\nlibrary(torch)\nm&lt;-torch_tensor(array(1:24, dim = c(4, 3, 2)))\nclass(m)\n\n\n[1] \"torch_tensor\" \"R7\"          \n\n\nNgoài ra, trong object tensor còn chứa thêm thông tin khác như là: \\(dtype* sẽ return data type (ví dụ như object dưới đây là dạng *long integer*), *\\)device return nơi tensor object được lưu trữ, $shape return dimensions của object.\n\n\nCode\nm$dtype\n\n\ntorch_Long\n\n\nCode\nm$device\n\n\ntorch_device(type='cpu') \n\n\nCode\nm$shape\n\n\n[1] 4 3 2\n\n\nVí dụ ta có thể simulate công thức đơn giản như sau bằng package torch: \\(f(x) = xw + b\\)\n\n\nCode\nx &lt;- torch_randn(100, 3)\nw &lt;- torch_randn(3, 1, requires_grad = TRUE)\nb &lt;- torch_zeros(1, 1, requires_grad = TRUE)\ny &lt;- x$matmul(w) + b\nhead(y)\n\n\ntorch_tensor\n 0.8659\n 1.1373\n-1.1229\n-0.8550\n 1.3113\n 0.4500\n[ CPUFloatType{6,1} ][ grad_fn = &lt;SliceBackward0&gt; ]\n\n\n\n\n\nTiếp theo, mình sẽ xây dựng mô hình Transformer để dự báo giá cổ phiếu của Google từ nguồn Yahoo Finance.\n\n\nCode\n#### Call packages-------------------------------------------------------------\npacman::p_load(quantmod,\n               torch,\n               dplyr,\n               dygraphs)\n#### Input---------------------------------------------------------------------\ngetSymbols(\"GOOG\", src = \"yahoo\", from = \"2020-01-01\", to = \"2022-01-01\")\n\n\n[1] \"GOOG\"\n\n\nCode\nprice_data &lt;- GOOG$GOOG.Close\nprice_data_xts &lt;- xts(price_data, \n                     order.by = index(price_data))\n\ncolors&lt;-RColorBrewer::brewer.pal(9, \"Blues\")[c(4, 6, 8)]\n\ndygraph(price_data_xts, main = \"Google Stock Price (2020 - 2022)\", ylab = \"Price ($)\") |&gt; \n  dyRangeSelector(height = 20) |&gt; \n  dyOptions(\n    fillGraph = TRUE,  \n    colors = colors,   \n    strokeWidth = 2,   \n    gridLineColor = \"gray\",  \n    gridLineWidth = 0.5,     \n    drawPoints = TRUE,   \n    pointSize = 4,       \n    pointShape = \"diamond\" \n  ) |&gt; \n  dyLegend(show = \"follow\") \n\n\n\n\n\n\nNhư biểu đồ, ta thấy giá cổ phiếu tăng cao chóng mặt và mức biến động khá mức tạp (lúc lên lúc xuống). Task này khá khó nên ta sẽ tìm hiểu xem performance của mô hình Transformer sẽ như thế nào.\nMô hình đầy đủ sẽ được code như sau:\n\n\nShow structure\n#### Transform input----------------------------------------------------------------\ncreate_supervised_data &lt;- function(series, n) {\n  series &lt;- as.vector(series)\n  data &lt;- data.frame(series)\n  \n  for (i in 1:n) {\n    lagged_column &lt;- lag(series, i)\n    data &lt;- cbind(data, lagged_column)\n  }\n  \n  colnames(data) &lt;- c('t',paste0('t', 1:n))\n\n  data &lt;- na.omit(data)\n  \n  return(data)\n}\n\nseq_leng &lt;- 50\ndim_model &lt;- 32\n\nsupervised_data &lt;- create_supervised_data(price_data, n = seq_leng)\n\nsupervised_data &lt;- scale(supervised_data)\n\n\nx_data &lt;- torch_tensor(as.matrix(supervised_data[, 2:(seq_leng+1)]), dtype = torch_float())  # Features (lags)\ny_data &lt;- torch_tensor(as.matrix(supervised_data[, 1]), dtype = torch_float())    # Target\n\n# Reshape x_data to match (batch_size, seq_leng, feature_size)\nx_data &lt;- x_data$view(c(nrow(x_data), seq_leng, 1))  # (batch_size, seq_leng, feature_size)\ny_data &lt;- y_data$view(c(nrow(y_data), 1, 1)) \n\n# Split the data into training and testing sets (80% for training, 20% for testing)\ntrain_size &lt;- round(0.8 * nrow(supervised_data))\n\nx_train &lt;- x_data[1:train_size, , drop = FALSE]  \ny_train &lt;- y_data[1:train_size]\n\nx_test &lt;- x_data[(train_size + 1):nrow(supervised_data), , drop = FALSE]\ny_test &lt;- y_data[(train_size + 1):nrow(supervised_data)]\n\n#### Build components of model----------------------------------------------------------------\n### Positional encoding:\npositional_encoding &lt;- function(seq_leng, d_model, n = 10000) {\n  if (missing(seq_leng) || missing(d_model)) {\n    stop(\"'seq_leng' and 'd_model' must be provided.\")\n  }\n  \n  P &lt;- matrix(0, nrow = seq_leng, ncol = d_model)  \n  \n  for (k in 1:seq_leng) {\n    for (i in 0:(d_model / 2 - 1)) {\n      denominator &lt;- n^(2 * i / d_model)\n      P[k, 2 * i + 1] &lt;- sin(k / denominator)\n      P[k, 2 * i + 2] &lt;- cos(k / denominator)\n    }\n  }\n  \n  return(P)\n}\n\nen_pe &lt;- positional_encoding(x_data$size(2),dim_model, n = 10000)\nde_pe &lt;- positional_encoding(y_data$size(2),dim_model, n = 10000)\n\n### Encoder block:\nencoder_layer &lt;- nn_module(\n  \"TransformerEncoderLayer\",\n  \n  initialize = function(d_model, num_heads, d_ff) {\n    \n    # Multi-Head Attention\n    self$multihead_attention &lt;- nn_multihead_attention(embed_dim = d_model, num_heads = num_heads)\n    \n    # Feedforward Network (Fully Connected)\n    self$feed_forward &lt;- nn_sequential(\n      nn_linear(d_model, d_ff),\n      nn_relu(),\n      nn_linear(d_ff, d_model)\n    )\n    \n    self$layer_norm &lt;- nn_layer_norm(d_model)\n  \n  },\n  \n  forward = function(x) {\n\n    attn_output &lt;- self$multihead_attention(x, x, x) \n    x &lt;- x + attn_output[[1]]\n    x &lt;- self$layer_norm(x) \n    \n    # Feedforward network with residual connection\n    ff_output &lt;- self$feed_forward(x)\n    x &lt;- x + ff_output\n    x &lt;- self$layer_norm(x)\n    \n    return(x)\n  }\n)\n\n### Mask function:\nmask_self_attention &lt;- nn_module(\n  initialize = function(embed_dim, num_heads) {\n    self$embed_dim &lt;- embed_dim\n    self$num_heads &lt;- num_heads\n    self$head_dim &lt;- embed_dim / num_heads\n    \n    # Ensure that self$head_dim is a scalar\n    if (self$head_dim %% 1 != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    if (embed_dim %% num_heads != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    # Linear layers for Q, K, V \n    self$query &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$key &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$value &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    \n    # Final linear layer after concatenating heads\n    self$out &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    \n  },\n  \n  forward = function(x) {\n    batch_size &lt;- x$size(1)\n    seq_leng &lt;- x$size(2)\n    \n    # Linear projections for Q, K, V\n    Q &lt;- self$query(x)  # (batch_size, seq_leng, embed_dim)\n    K &lt;- self$key(x)\n    V &lt;- self$value(x)\n    \n    # Reshape to separate heads: (batch_size, num_heads, seq_leng, head_dim)\n    Q &lt;- Q$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    K &lt;- K$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    V &lt;- V$view(c(batch_size, seq_leng, self$num_heads, self$head_dim))$transpose(2, 3)\n    \n    # Compute attention scores\n    d_k &lt;- self$head_dim\n    attention_scores &lt;- torch_matmul(Q, torch_transpose(K, -1, -2)) / sqrt(d_k)\n    \n    # Apply mask if provided\n    mask &lt;- torch_tril(torch_ones(c(seq_leng, seq_leng)))\n    \n    if (!is.null(mask)) {\n      \n      masked_attention_scores &lt;- attention_scores$masked_fill(mask == 0, -Inf)\n      \n    } else {\n      print(\"Warning: No mask provided\")\n    }\n    \n    # Compute attention weights\n    weights &lt;- nnf_softmax(masked_attention_scores, dim = -1)\n    \n    # Apply weights to V\n    attn_output &lt;- torch_matmul(weights, V)  # (batch_size, num_heads, seq_leng, head_dim)\n    \n    \n    attn_output &lt;- attn_output$transpose(2, 3)$contiguous()$view(c(batch_size, seq_leng, self$embed_dim))\n    \n    \n    output &lt;- self$out(attn_output)\n    return(output)\n  }\n)\n\n### Cross attention:\ncross_attention &lt;- nn_module(\n  initialize = function(embed_dim, num_heads) {\n    self$embed_dim &lt;- embed_dim\n    self$num_heads &lt;- num_heads\n    self$head_dim &lt;- embed_dim / num_heads\n    \n    if (self$head_dim %% 1 != 0) {\n      stop(\"embed_dim must be divisible by num_heads\")\n    }\n    \n    self$query &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$key &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$value &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n    self$out &lt;- nn_linear(embed_dim, embed_dim, bias = FALSE)\n  },\n  \n  forward = function(decoder_input, encoder_output, mask = NULL) {\n    batch_size &lt;- decoder_input$size(1)\n    seq_leng_dec &lt;- decoder_input$size(2)\n    seq_leng_enc &lt;- encoder_output$size(2)\n    \n    Q &lt;- self$query(decoder_input)\n    K &lt;- self$key(encoder_output)\n    V &lt;- self$value(encoder_output)\n    \n    Q &lt;- Q$view(c(batch_size, seq_leng_dec, self$num_heads, self$head_dim))$transpose(2, 3)\n    K &lt;- K$view(c(batch_size, seq_leng_enc, self$num_heads, self$head_dim))$transpose(2, 3)\n    V &lt;- V$view(c(batch_size, seq_leng_enc, self$num_heads, self$head_dim))$transpose(2, 3)\n    \n    d_k &lt;- self$head_dim\n    attention_scores &lt;- torch_matmul(Q, torch_transpose(K, -1, -2)) / sqrt(d_k)\n    \n    weights &lt;- nnf_softmax(attention_scores, dim = -1)\n    \n    attn_output &lt;- torch_matmul(weights, V)\n    \n    attn_output &lt;- attn_output$transpose(2, 3)$contiguous()$view(c(batch_size, seq_leng_dec, self$embed_dim))\n    \n    output &lt;- self$out(attn_output)\n    return(output)\n  }\n)\n\n### Decoder Layer\ndecoder_layer &lt;- nn_module(\n  \"TransformerDecoderLayer\",\n  \n  initialize = function(d_model, num_heads, d_ff) {\n    self$mask_self_attention &lt;- mask_self_attention(embed_dim = d_model, num_heads = num_heads)\n    self$cross_attention &lt;- cross_attention(embed_dim = d_model, num_heads = num_heads)\n    self$feed_forward &lt;- nn_sequential(\n      nn_linear(d_model, d_ff),\n      nn_relu(),\n      nn_linear(d_ff, d_model)\n    )\n    \n    self$layer_norm &lt;- nn_layer_norm(d_model)\n  },\n  \n  forward = function(x, encoder_output) {\n    # Masked Self-Attention\n    mask_output &lt;- self$mask_self_attention(x)\n    x &lt;- x + mask_output\n    x &lt;- self$layer_norm(x)\n    \n    # Encoder-Decoder Multi-Head Attention\n    cross_output &lt;- self$cross_attention(x, encoder_output)\n    x &lt;- x + cross_output\n    x &lt;- self$layer_norm(x)\n    \n    # Feedforward Network\n    ff_output &lt;- self$feed_forward(x)\n    x &lt;- x + ff_output\n    x &lt;- self$layer_norm(x)\n    \n    return(x)\n  }\n)\n\n### Final transformer model: \ntransformer &lt;- nn_module(\n  \"Transformer\",\n  \n  initialize = function(d_model, seq_leng, num_heads, d_ff, num_encoder_layers, num_decoder_layers) {\n    self$d_model &lt;- d_model\n    self$num_heads &lt;- num_heads\n    self$d_ff &lt;- d_ff\n    self$num_encoder_layers &lt;- num_encoder_layers\n    self$num_decoder_layers &lt;- num_decoder_layers\n    self$seq_leng &lt;- seq_leng\n    self$en_pe &lt;- en_pe\n    self$de_pe &lt;- de_pe\n    \n    # Encoder layers\n    self$encoder_layers &lt;- nn_module_list(\n      lapply(1:num_encoder_layers, function(i) {\n        encoder_layer(d_model, num_heads, d_ff)\n      })\n    )\n    \n    # Decoder layers\n    self$decoder_layers &lt;- nn_module_list(\n      lapply(1:num_decoder_layers, function(i) {\n        decoder_layer(d_model, num_heads, d_ff)\n      })\n    )\n    \n    # Final output layer\n    self$output_layer &lt;- nn_linear(d_model, 1)  # Output layer to predict a single value\n    \n  },\n  \n  forward = function(src, trg) {\n    \n    src &lt;- src + self$en_pe  \n    trg &lt;- trg + self$de_pe\n    \n    # Encoder forward pass\n    encoder_output &lt;- src\n    for (i in 1:self$num_encoder_layers) {\n      encoder_output &lt;- self$encoder_layers[[i]](encoder_output)\n    }\n    \n    # Decoder forward pass\n    decoder_output &lt;- trg\n    for (i in 1:self$num_decoder_layers) {\n      decoder_output &lt;- self$decoder_layers[[i]](decoder_output, encoder_output)\n    }\n  \n    # Apply final output layer\n    output &lt;- self$output_layer(decoder_output)\n    \n    return(output)\n  }\n)\n\n#### Training----------------------------------------------------------------\nmodel &lt;- transformer(\n  d_model = dim_model,         # Embedding dimension\n  seq_leng = seq_leng,        # Sequence length\n  num_heads = 8,        # Number of heads\n  d_ff = seq_leng,           # Dimension of the feedforward layer\n  num_encoder_layers = 6, \n  num_decoder_layers = 6\n)\n\n\n#### Training----------------------------------------------------------------\nepochs &lt;- 200\nloss_fn &lt;- nn_mse_loss()\noptimizer &lt;- optim_adam(model$parameters, lr = 1e-3)\n\nfor (epoch in 1:epochs) {\n  model$train()\n  optimizer$zero_grad()\n  \n  # Forward pass\n  y_train_pred &lt;- model(x_train, y_train) \n  \n  # Compute the loss\n  loss &lt;- loss_fn(y_train_pred, y_train)\n  \n  # Backpropagation and optimization\n  loss$backward()\n  optimizer$step()\n  \n  if (epoch %% 10 == 0) {\n    cat(\"Epoch: \", epoch, \" Loss: \", loss$item(), \"\\n\")\n  }\n}\n\n#### Predictions----------------------------------------------------------------\nmodel$eval()\n\n# Make predictions on the test data\ny_test_pred &lt;- model(x_test, y_test)  # Use the test data for both input and output during prediction\n\n# Convert tensors to numeric values for comparison\n\ny_test_pred&lt;- as.numeric(as.array(y_test_pred$cpu()))\n\n#### Evaluating----------------------------------------------------------------\nlibrary(highcharter)\ny_train_pred &lt;- as.numeric(as.array(y_train_pred$cpu()))\ny_train &lt;- as.numeric(as.array(y_train$cpu()))\ny_test &lt;- as.numeric(as.array(y_test$cpu()))\n\ncomparison &lt;- data.frame(\n  time = 1:nrow(supervised_data),\n  actual = c(y_train,y_test),\n  forecast = c(y_train_pred,y_test_pred)\n)\n\n# Compare only errors:\nerror&lt;-highchart() |&gt;\n  hc_title(text = \"Evaluating error of model\") |&gt;\n  hc_xAxis(\n    categories = time,\n    title = list(text = \"Time\")\n  ) |&gt;\n  hc_yAxis(\n    title = list(text = \"Value\"),\n    plotLines = list(list(\n      value = 0,\n      width = 1,\n      color = \"gray\"\n    ))\n  ) |&gt; \n  hc_add_series(\n    name = \"Error\",\n    data = (y_test_pred - y_test)/y_test,\n    type = \"line\",\n    color = \"red\"  # Blue color for actual data\n  ) |&gt;\n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) |&gt;\n  hc_legend(\n    enabled = TRUE\n  )\n\n\n# Compare all:\nall&lt;-highchart() |&gt;\n  hc_title(text = \"Model Predictions vs Actual Values\") |&gt;\n  hc_xAxis(\n    categories = time,\n    title = list(text = \"Time\")\n  ) |&gt;\n  hc_yAxis(\n    title = list(text = \"Value\"),\n    plotLines = list(list(\n      value = 0,\n      width = 1,\n      color = \"gray\"\n    ))\n  ) |&gt; \n  hc_add_series(\n    name = \"Actual Data\",\n    data = comparison$actual,\n    type = \"line\",\n    color = \"#1f77b4\"  # Blue color for actual data\n  ) |&gt;\n  hc_add_series(\n    name = \"Forecast\",\n    data = comparison$forecast,\n    type = \"line\",\n    color = \"#ff7f0e\"  # Orange color for forecast data\n  ) |&gt; \n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) |&gt;\n  hc_legend(\n    enabled = TRUE\n  )\n\n\n\n\n\nĐầu tiên ta sẽ đánh giá về sai số của mô hình khi dùng testing data. Kết quả khá ổn khi sai số khoảng (0.04,0.12).\n\n  \n\nVà còn nhìn tổng quan hết thì ta thấy mô hình dự đoán khá sát với training data nhưng với testing data thì vẫn chênh lệch thấp hơn thực tế (dấu hiệu cho thấy mô hình đang bị overfitting)."
  },
  {
    "objectID": "practice.html#kết-quả",
    "href": "practice.html#kết-quả",
    "title": "Thực hành trong R",
    "section": "2 Kết quả:",
    "text": "2 Kết quả:\n\n  \n\nNếu bạn có câu hỏi hay thắc mắc nào, đừng ngần ngại liên hệ với mình qua Gmail. Bên cạnh đó, nếu bạn muốn xem lại các bài viết trước đây của mình, hãy nhấn vào hai nút dưới đây để truy cập trang Rpubs hoặc mã nguồn trên Github. Rất vui được đồng hành cùng bạn, hẹn gặp lại! 😄😄😄\n\n\n\n    \n    \n    Contact Me\n    \n    \n    \n\n\n    \n        Contact Me\n        \n            Your Email:\n            \n            Please enter a valid email address.\n            Send Email\n        \n        \n            \n                \n                     View Code on GitHub\n                \n            \n        \n        \n            \n                \n                     Visit my RPubs"
  },
  {
    "objectID": "practice.html#kết-luận",
    "href": "practice.html#kết-luận",
    "title": "Time series forecasting",
    "section": "2 Kết luận:",
    "text": "2 Kết luận:\nNhư vậy ta đã thấy được sức mạnh của mô hình Transformer trong dự báo cho dữ liệu sequence (mặc dù mình mong muốn error rate &lt; 0.05 nhưng kết quả vẫn chấp nhận được).\nMột số suggestion của mình cho mô hình Transformer để improve performance như sau:\n\nThêm layer nn_dropout(p) vào mô hình: là một phương pháp regularization (chuẩn hóa) được sử dụng trong mạng nơ-ron để ngăn ngừa hiện tượng overfitting (quá khớp) bằng cách ngẫu nhiên “loại bỏ” một tỷ lệ phần trăm nơ-ron trong quá trình huấn luyện. Bạn chỉ cần thêm đối số p là tỷ lệ % dropout.\nDùng các variant của Transformer: thực chất mục đích ban đầu của Transformer là deal với các tasks liên quan về dịch thuật, xử lí văn bản, phân tích hình ảnh,… chứ không thiên về time series forecasting. Mô hình deep learning khác thiên về vấn đề này mà bạn có thể sử dụng là Informer.\n\nNếu bạn có câu hỏi hay thắc mắc nào, đừng ngần ngại liên hệ với mình qua Gmail. Bên cạnh đó, nếu bạn muốn xem lại các bài viết trước đây của mình, hãy nhấn vào hai nút dưới đây để truy cập trang Rpubs hoặc mã nguồn trên Github. Rất vui được đồng hành cùng bạn, hẹn gặp lại! 😄😄😄\n\n\n\n    \n    \n    Contact Me\n    \n    \n    \n\n\n    \n        Contact Me\n        \n            Your Email:\n            \n            Please enter a valid email address.\n            Send Email\n        \n        \n            \n                \n                     View Code on GitHub\n                \n            \n        \n        \n            \n                \n                     Visit my RPubs"
  }
]