[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cao XuÃ¢n Lá»™c",
    "section": "",
    "text": "Xin chÃ o, mÃ¬nh lÃ  Lá»™c, sinh nÄƒm 2003 vÃ  lÃ  má»™t chÃ ng trai Ä‘áº¿n tá»« máº£nh Ä‘áº¥t Ä‘áº§y náº¯ng vÃ  giÃ³ - PhÃº YÃªn, Viá»‡t Nam. MÃ¬nh cÃ³ báº±ng cá»­ nhÃ¢n trÆ°á»ng Äáº¡i há»c Kinh Táº¿ - TÃ i ChÃ­nh (UEF) vÃ  chuyÃªn ngÃ nh cá»§a mÃ¬nh lÃ  Logistics vÃ  quáº£n lÃ½ chuá»—i cung á»©ng.\nLÃ  ngÆ°á»i cÃ³ niá»m Ä‘am mÃª máº¡nh máº½ vá»›i R, mÃ¬nh cÃ³ sá»Ÿ thÃ­ch viáº¿t post vá» viá»‡c phÃ¢n tÃ­ch dá»¯ liá»‡u vá»›i R Ä‘á»ƒ á»©ng dá»¥ng vÃ o cÃ¡c cÃ´ng viá»‡c, bÃ i toÃ¡n thÆ°á»ng gáº·p trong Supply Chain. NgoÃ i ra, sá»Ÿ thÃ­ch cá»§a mÃ¬nh lÃ  nghe sÃ¡ch nÃ³i vÃ  Ä‘i bá»™!\nCÃ¢u slogan mÃ  mÃ¬nh thÃ­ch nháº¥t lÃ : â€œDonâ€™t fear the risk, fear the opportunity lost!â€ vÃ  Ä‘Ã³ cÅ©ng lÃ  cÃ¡ch mÃ¬nh sá»‘ng vÃ  lÃ m viá»‡c Ä‘áº¿n bÃ¢y giá» ğŸ’ğŸ’ğŸ’.\nHi vá»ng cÃ¡c báº¡n sáº½ thÃ­ch bÃ i viáº¿t cá»§a mÃ¬nh!\n    \n    \n    Go to Next Page\n    \n    \n        \n            Go to Next Page\n            â”"
  },
  {
    "objectID": "torch.html",
    "href": "torch.html",
    "title": "RNN and LSTM model",
    "section": "",
    "text": "á» Ä‘Ã¢y ta sáº½ há»c vá» mÃ´ hÃ¬nh machine learning Ä‘Æ°á»£c á»©ng dá»¥ng nhiá»u nháº¥t trong viá»‡c phÃ¢n tÃ­ch dá»¯ liá»‡u thá»i gian lÃ  RNN vÃ  LSTM."
  },
  {
    "objectID": "torch.html#Ä‘á»‹nh-nghÄ©a",
    "href": "torch.html#Ä‘á»‹nh-nghÄ©a",
    "title": "RNN and LSTM model",
    "section": "1 Äá»‹nh nghÄ©a:",
    "text": "1 Äá»‹nh nghÄ©a:\n\n1.1 MÃ´ hÃ¬nh RNN:\nÄiá»ƒm chung lÃ  cáº£ hai mÃ´ hÃ¬nh Ä‘á»u thuá»™c phÃ¢n lá»›p Deep learning - nghÄ©a lÃ  há»c mÃ¡y sÃ¢u vá»›i Ä‘áº·c Ä‘iá»ƒm chung lÃ  phÃ¢n chia dá»¯ liá»‡u thÃ nh nhiá»u lá»›p vÃ  báº¯t Ä‘áº§u â€œhá»câ€ dáº§n qua tá»«ng lá»›p Ä‘á»ƒ Ä‘Æ°a ra káº¿t quáº£ cuá»‘i cÃ¹ng. á» hÃ¬nh dÆ°á»›i Ä‘Ã¢y, \\(X_o\\) Ä‘áº¡i diá»‡n cho dá»¯ liá»‡u Ä‘áº§u vÃ o, \\(h_t\\) lÃ  output Ä‘áº§u ra cá»§a tá»«ng step vÃ  \\(A\\) lÃ  nhá»¯ng gÃ¬ Ä‘Ã£ â€œhá»câ€ Ä‘Æ°á»£c táº¡i step Ä‘Ã³ vÃ  Ä‘Æ°á»£c truyá»n cho step tiáº¿p theo. Trong tÃ i liá»‡u chuáº©n thÃ¬ há» thÆ°á»ng kÃ­ hiá»‡u lÃ  \\(X_t\\), \\(Y_t\\), \\(h_{t-1}\\).\n\n  \n  \n  \n  \n    HÃ¬nh 1: Minh há»a vá» sá»± phÃ¢n chia dá»¯ liá»‡u thÃ nh nhiá»u lá»›p\n  \n  \n  \n  \n    Source: Link to Image\n  \n\nKhi nhÃ¬n hÃ¬nh thÃ¬ báº¡n cÃ³ thá»ƒ bá»‘i rá»‘i chÆ°a hiá»ƒu cÃ¡c kÃ­ tá»± vÃ  hÃ¬nh áº£nh thÃ¬ báº¡n cÃ³ thá»ƒ tÆ°á»Ÿng tÆ°á»£ng há»c mÃ¡y nhÆ° 1 Ä‘á»©a tráº» vÃ  Ä‘á»ƒ nÃ³ cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c cÃ¢u: â€œHÃ´m nay con Ä‘i há»câ€ thÃ¬ nÃ³ pháº£i há»c tá»«ng chá»¯ cÃ¡i nhÆ°: a,b,c,â€¦ trÆ°á»›c rÃ²i má»›i ghÃ©p thÃ nh tá»« Ä‘Æ¡n nhÆ°: â€œHÃ´mâ€,â€œNayâ€,â€¦ rá»“i ghÃ©p thÃ nh cÃ¢u trÃªn. Váº­y giáº£ sá»­ nhÆ° hÃ´m nay há»c Ä‘Æ°á»£c tá»« â€œHÃ´mâ€ thÃ¬ nÃ³ sáº½ báº¯t Ä‘áº§u ghi nhá»› tá»« Ä‘Ã£ há»c vÃ o trong \\(A\\). Náº¿u sau nÃ y ta cáº§n há»c mÃ¡y hiá»ƒu cÃ¢u â€œHÃ´m sau con Ä‘i chÆ¡iâ€ thÃ¬ tá»‘c Ä‘á»™ há»c cá»§a há»c mÃ¡y sáº½ nhanh lÃªn vÃ¬ thay vÃ¬ nÃ³ pháº£i há»c 5 chá»¯ Ä‘Æ¡n nhÆ° thÃ´ng thÆ°á»ng thÃ¬ nÃ³ chá»‰ cáº§n há»c 4 chá»¯ cÃ²n láº¡i trá»« chá»¯ â€œhÃ´mâ€. Váº­y báº¡n Ä‘Ã£ hiá»ƒu Ã½ tÆ°á»Ÿng ná»n táº£ng cá»§a RNN rá»“i ha!\nNáº¿u muá»‘n hiá»ƒu thÃªm vá» RNN, báº¡n cÃ³ thá»ƒ tham kháº£o link nÃ y: Recurrent Neural Network: Tá»« RNN Ä‘áº¿n LSTM.\nVÃ  trong RNN cÃ³ 1 váº¥n Ä‘á» lá»›n lÃ  Vanishing Gradient nghÄ©a lÃ  mÃ´ hÃ¬nh sáº½ khÃ´ng cÃ²n â€œhá»câ€ thÃªm Ä‘Æ°á»£c ná»¯a cho dÃ¹ tÄƒng sá»‘ epochs. Theo pháº§n chá»©ng minh cá»§a anh Tuáº¥n cho tháº¥y RNN sáº½ luÃ´n xáº£y ra váº¥n Ä‘á» Ä‘Ã³ cho dÃ¹ báº¡n cÃ³ xÃ¢y dá»±ng mÃ´ hÃ¬nh tá»‘t nhÆ° tháº¿ nÃ o. Äiá»u nÃ y cÃ³ thá»ƒ hiá»ƒu Ä‘Æ¡n giáº£n nhÆ° viá»‡c báº¡n há»c liÃªn tá»¥c dáº«n tá»›i quÃ¡ táº£i. Do Ä‘Ã³, RNN chá»‰ há»c cÃ¡c thÃ´ng tin \\(A\\) tá»« cÃ¡c step gáº§n nháº¥t vÃ  Ä‘Ã³ lÃ  lÃ­ do ra Ä‘á»i LSTM - Long short term memory.\n\n\n\n\n\n\nWarning\n\n\n\nLÆ°u Ã½: Äiá»u nÃ y khÃ´ng cÃ³ nghÄ©a LSTM luÃ´n tá»‘t hÆ¡n RNN vÃ¬ cÃ³ nhá»¯ng bÃ i toÃ¡n vá»›i Ä‘áº§u vÃ o Ä‘Æ¡n giáº£n thÃ¬ mÃ´ hÃ¬nh chá»‰ cáº§n há»c cÃ¡c step Ä‘áº§u lÃ  Ä‘Ã£ â€œhá»câ€ Ä‘áº§y Ä‘á»§ thÃ´ng tin cáº§n thiáº¿t. MÃ´ hÃ¬nh LSTM phá»• biáº¿n vá»›i cÃ¡c bÃ i toÃ¡n phá»©c táº¡p nhÆ° tá»± Ä‘á»™ng dá»‹ch ngÃ´n ngá»¯, ghi chÃ©p láº¡i theo giá»ng nÃ³iâ€¦\n\n\n\n\n1.2 MÃ´ hÃ¬nh LSTM:\nCÃ³ thá»ƒ xem mÃ´ hÃ¬nh LSTM nhÆ° biáº¿n thá»ƒ cá»§a RNN. Vá» cáº¥u trÃºc, LSTM cÃ³ ba cá»•ng chÃ­nh giÃºp nÃ³ xá»­ lÃ½ vÃ  duy trÃ¬ thÃ´ng tin qua cÃ¡c bÆ°á»›c thá»i gian:\n\nCá»•ng quÃªn (Forget Gate): Quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o cáº§n bá»‹ quÃªn trong tráº¡ng thÃ¡i Ã´ nhá»›.\nCá»•ng nháº­p (Input Gate): XÃ¡c Ä‘á»‹nh thÃ´ng tin nÃ o cáº§n Ä‘Æ°á»£c ghi vÃ o tráº¡ng thÃ¡i Ã´ nhá»›.\nCá»•ng xuáº¥t (Output Gate): Quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o sáº½ Ä‘Æ°á»£c xuáº¥t ra tá»« tráº¡ng thÃ¡i Ã´ nhá»› Ä‘á»ƒ áº£nh hÆ°á»Ÿng Ä‘áº¿n dá»± Ä‘oÃ¡n tiáº¿p theo."
  },
  {
    "objectID": "torch.html#xÃ¢y-dá»±ng-mÃ´-hÃ¬nh",
    "href": "torch.html#xÃ¢y-dá»±ng-mÃ´-hÃ¬nh",
    "title": "RNN and LSTM model",
    "section": "2 XÃ¢y dá»±ng mÃ´ hÃ¬nh:",
    "text": "2 XÃ¢y dá»±ng mÃ´ hÃ¬nh:\n\n2.1 Load dá»¯ liá»‡u:\nÄáº§u tiÃªn ta sáº½ load dá»¯ liá»‡u láº¡i nhÆ° trÆ°á»›c. á» Ä‘Ã¢y, Ä‘á»ƒ Ä‘Æ¡n giáº£n, mÃ¬nh chá»‰ xÃ¢y dá»±ng mÃ´ hÃ¬nh cho product A thÃ´i.\nGiáº£ sá»­ cÃ´ng ty mÃ¬nh Ä‘ang kinh doanh 3 loáº¡i máº·t hÃ ng product A,product B,product C vÃ  Ä‘Ã¢y lÃ  biá»ƒu Ä‘á»“ thá»ƒ hiá»‡n nhu cáº§u cá»§a cáº£ 3 máº·t hÃ ng tá»« thÃ¡ng 5 tá»›i thÃ¡ng 10.\n\n\nCode\nlibrary(highcharter)\nsales_data |&gt; \n  select(-Weekday) |&gt; \n  pivot_longer(cols = c(Product_A, Product_B, Product_C),\n               names_to = \"Product\",\n               values_to = \"Sales\") |&gt; \n  hchart(\"line\", hcaes(x = Date, y = Sales, group = Product))\n\n\n\n\n\n\nNáº¿u ta phÃ¢n tich sÃ¢u vá» nhu cáº§u cá»§a tá»«ng máº·t hÃ ng theo thá»© trong tuáº§n, ta sáº½ tháº¥y ráº±ng máº·t hÃ ng A, B thÃ¬ bÃ¡n cháº¡y vÃ o thá»© 4 vÃ  thá»© 7, cÃ²n máº·t hÃ ng C thÃ¬ bÃ¡n cháº¡y vÃ o thá»© 2 vÃ  thá»© 3.\n\nProduct A:Product B:Product C:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThÃ´ng thÆ°á»ng dá»¯ liá»‡u Ä‘á»ƒ train model trong machine learning thÆ°á»ng cáº§n tráº£i qua bÆ°á»›c normalize data nghÄ©a lÃ  Ä‘Æ°a táº¥t cáº£ dá»¯ liá»‡u vá» chung 1 thÆ°á»›c Ä‘o vÃ  pháº¡m vi. NguyÃªn do vÃ¬ Ä‘iá»u nÃ y giÃºp nhiá»u thuáº­t toÃ¡n há»c mÃ¡y dá»… dÃ ng há»™i tá»¥ hÆ¡n. VÃ­ dá»¥, cÃ¡c thuáº­t toÃ¡n nhÆ° k-Nearest Neighbors (KNN) vÃ  Support Vector Machines (SVM) ráº¥t nháº¡y cáº£m vá»›i khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u nÃªn náº¿u dá»¯ liá»‡u khÃ´ng Ä‘Æ°á»£c chuáº©n hÃ³a, thuáº­t toÃ¡n cÃ³ thá»ƒ Æ°u tiÃªn cÃ¡c Ä‘áº·c trÆ°ng cÃ³ pháº¡m vi lá»›n hÆ¡n vÃ  bá» qua cÃ¡c Ä‘áº·c trÆ°ng cÃ³ pháº¡m vi nhá» hÆ¡n, dáº«n Ä‘áº¿n hiá»‡u suáº¥t kÃ©m. VÃ  cÃ´ng thá»©c phá»• biáº¿n nháº¥t cho chuáº©n hÃ³a lÃ :\n\\[\n\\text{Normalized Value} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n\\]\n\n\nCode\n# Create a data frame with the adjusted sales data\nsales_data &lt;- data.frame(\n  Date = dates,\n  Weekday = weekdays,\n  Product_A = product_a_sales,\n  Product_B = product_b_sales,\n  Product_C = product_c_sales\n)\n\n# Convert the sales data to a time series (ts) object for Product A\nproduct_a_ts &lt;- ts(sales_data$Product_A, start = c(2024, 5), \n                   frequency = 365)\n                   \n\n# Normalzie data:\ntime_series_data&lt;-scale(product_a_ts)\n\nlibrary(highcharter)\nhighchart() %&gt;%\n  hc_add_series(data = as.numeric(time_series_data), type = \"line\", name = \"Sales of Product A\") %&gt;%\n  hc_title(text = \"Normalized Time Series of Product A\") %&gt;%\n  hc_xAxis(title = list(text = \"Date\")) %&gt;%\n  hc_yAxis(title = list(text = \"Normalized Sales\")) %&gt;%\n  hc_tooltip(shared = TRUE) %&gt;%\n  hc_plotOptions(line = list(marker = list(enabled = FALSE)))\n\n\n\n\n\n\n\n\n2.2 Chia dá»¯ liá»‡u:\nVáº­y Ä‘á»ƒ train data, mÃ¬nh sáº½ chia bá»™ dá»¯ liá»‡u thÃ nh 3 pháº§n:\n\nTraining data: dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n vÃ  xÃ¢y dá»±ng mÃ´ hÃ¬nh.\nEvaluating data: Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh vá»«a huáº¥n luyá»‡n.\nTesting data: dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ láº¡i náº¿u muá»‘n mÃ´ hÃ¬nh há»c láº¡i dá»¯ liá»‡u\n\n\n\nCode\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(dplyr)\n\n# Function to create supervised learning format from time series\ncreate_supervised_data &lt;- function(series, n_in = 1, n_out = 1) {\n  series &lt;- as.vector(series)  # Convert time series object to vector\n  data &lt;- data.frame(series)\n  \n  # Use base R lag function for ts objects (lag() from stats package)\n  for (i in 1:n_in) {\n    data &lt;- cbind(data, stats::lag(series, -i))\n  }\n  \n  colnames(data) &lt;- c(paste0('t-', 1:n_in), 't+1')  # Correctly name columns\n  return(data)\n}\n\n# Prepare the data with 12 input lags and 1 output (next time step)\nsupervised_data &lt;- create_supervised_data(time_series_data,\n                                          n_in = 12, \n                                          n_out = 1)\n\n# Remove NA rows created by lag function\nsupervised_data &lt;- na.omit(supervised_data)\n\n# Step 2: Split data into training and test sets\ntrain_size &lt;- round(0.7 * nrow(supervised_data))   # 70% for training\nval_size &lt;- round(0.1 * nrow(supervised_data))     # 10% for validation\ntest_size &lt;- nrow(supervised_data) - train_size - val_size  # 20% for testing\n\ntrain_data &lt;- supervised_data[1:train_size, ]\nval_data &lt;- supervised_data[(train_size + 1):(train_size + val_size), ]\ntest_data &lt;- supervised_data[(train_size + val_size + 1):nrow(supervised_data), ]\n\n# Correct column selection\nx_train &lt;- as.matrix(train_data[, 1:12])  # Input features (12 lags)\ny_train &lt;- as.matrix(train_data[, 't+1'])  # Target output (next time step)\n\nx_val &lt;- as.matrix(val_data[, 1:12])  # Input features for validation\ny_val &lt;- as.matrix(val_data[, 't+1'])  # Actual output for validation\n\nx_test &lt;- as.matrix(test_data[, 1:12])  # Input features for testing\ny_test &lt;- as.matrix(test_data[, 't+1'])  # Actual output for testing\n\n\n## Plot the result:\nlibrary(xts)\nn&lt;-quantile(sales_data$Date, \n            probs = c(0, 0.7, 0.8,1), \n            type = 1)\n\nm1&lt;-sales_data %&gt;% \n  filter(Date &lt;= n[[2]])\nm2&lt;-sales_data %&gt;% \n  filter(Date &lt;= n[[3]] & Date &gt; n[[2]])\nm3&lt;-sales_data %&gt;% \n  filter(Date &lt;= n[[4]] & Date &gt; n[[3]])\n\ndemand_training&lt;-xts(x=m1$Product_A,\n                     order.by=m1$Date)\ndemand_testing&lt;-xts(x=m2$Product_A,\n                     order.by=m2$Date)\ndemand_forecasting&lt;-xts(x=m3$Product_A,\n                     order.by=m3$Date)\n\nlibrary(dygraphs)\nlines&lt;-cbind(demand_training,\n             demand_testing,\n             demand_forecasting)\ndygraph(lines,\n        main = \"Training and testing data\", \n        ylab = \"Quantity order (Unit: Millions)\") %&gt;% \n  dySeries(\"demand_training\", label = \"Training data\") %&gt;%\n  dySeries(\"demand_testing\", label = \"Testing data\") %&gt;%\n  dySeries(\"demand_forecasting\", label = \"Forecasting data\") %&gt;%\n  dyOptions(fillGraph = TRUE, fillAlpha = 0.4) %&gt;% \n  dyRangeSelector(height = 20)\n\n\n\n\n\n\n\n\n2.3 MÃ´ hÃ¬nh RNN:\nSau Ä‘Ã³, ta sáº½ báº¯t Ä‘áº§u train model báº±ng cÃ¡ch táº¡o thÃªm 12 cá»™t giÃ¡ trá»‹ lÃ  giÃ¡ trá»‹ quÃ¡ khá»© cá»§a demand. Báº¡n sáº½ báº¯t Ä‘áº§u Ä‘á»‹nh nghÄ©a mÃ´ hÃ¬nh gá»“m:\n\nInput: dÃ¹ng hÃ m layer_input(shape = input_shape) vá»›i input_shape lÃ  sá»‘ lÆ°á»£ng predictor.\nLayer: lÃ  cÃ¡c hidden layer trong mÃ´ hÃ¬nh thÃªm vÃ o báº±ng hÃ m layer_dense(x, units = 64, activation = 'relu') vá»›i Ä‘á»‘i sá»‘ units thÆ°á»ng lÃ  bá»™i sá»‘ cá»§a 32 nhÆ° 32,64,256,â€¦\nOutput: dÃ¹ng hÃ m layer_dense(x, units = 1) Ä‘á»ƒ Ä‘á»‹nh nghÄ©a lÃ  Ä‘áº§u ra chá»‰ cÃ³ 1 giÃ¡ trá»‹.\n\n\n\nCode\n# Step 3: Build a simple transformer-like model\nRNN_model &lt;- function(input_shape) {\n  inputs &lt;- layer_input(shape = input_shape)\n\n  # Transformer Encoder Layer (simplified)\n  x &lt;- inputs\n  x &lt;- layer_dense(x, units = 64, activation = 'relu')  # Dense layer\n  x &lt;- layer_dense(x, units = 32, activation = 'relu')  # Another dense layer\n\n  # Output layer\n  x &lt;- layer_dense(x, units = 1)\n  \n  model &lt;- keras_model(inputs, x)\n  return(model)\n}\n\n# Example input shape (12 time steps input per sample)\ninput_shape &lt;- c(12)\n\nRNN_model &lt;- RNN_model(input_shape)\n\n\nÄá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh truyá»n thá»‘ng nhÆ° linear regression thÃ¬ báº¡n Ä‘Ã£ quen vá»›i thÃ´ng sá»‘ \\(R^2\\) Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh, cÃ²n vá»›i mÃ´ hÃ¬nh Machine learning thÃ¬ dÃ¹ng khÃ¡i niá»‡m loss function - hÃ m máº¥t mÃ¡t. Vá» khÃ¡i niá»‡m, loss function sáº½ Ä‘o lÆ°á»ng chÃªnh lá»‡ch giá»¯a predicted vÃ  actual trong bá»™ training data nÃªn khi cÃ ng tÄƒng epochs nghÄ©a lÃ  tÄƒng sá»‘ láº§n há»c láº¡i dá»¯ liá»‡u thÃ¬ loss function sáº½ tÃ­nh ra giÃ¡ trá»‹ cÃ ng tháº¥p. NhÆ° mÃ´ hÃ¬nh trÃªn thÃ¬ mÃ¬nh Ä‘áº·t Ä‘á»‘i sá»‘ loss = mse nghÄ©a lÃ  sá»­ dá»¥ng Mean Squared Error Ä‘á»ƒ tá»‘i Æ°u quy trÃ¬nh há»c cá»§a há»c mÃ¡y. CÃ´ng thá»©c nhÆ° sau:\n\\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{pred}}(i) - y_{\\text{true}}(i))^2\n\\]\nCÃ²n Ä‘á»‘i sá»‘ metrics = c('mae') nghÄ©a lÃ  tiÃªu chÃ­ khÃ¡c Ä‘á»ƒ theo dÃµi vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh. Váº­y táº¡i sao cáº§n cÃ³ 2 tham sá»‘ Ä‘Ã¡nh giÃ¡ song song nhÆ° váº­y lÃ  vÃ¬ nhÆ° Ä‘Ã£ nÃ³i, náº¿u báº¡n cÃ ng tÄƒng epochs thÃ¬ giÃ¡ trá»‹ loss cÃ ng tháº¥p trong khi dÃ¹ng metrics sáº½ Ä‘Æ°a ra Ä‘Ã¡nh giÃ¡ khÃ¡ch quan hÆ¡n vá» mÃ´ hÃ¬nh mÃ  khÃ´ng phá»¥ thuá»™c vÃ o sá»‘ láº§n epochs. CÃ´ng thá»©c nhÆ° sau:\n\\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{pred}}(i) - y_{\\text{true}}(i)|\n\\]\nVáº­y khi cháº¡y code, R sáº½ return output nhÆ° biá»ƒu Ä‘á»“ dÆ°á»›i Ä‘Ã¢y lÃ  so sÃ¡nh tham sá»‘ cá»§a mse vÃ  mae giá»¯a training data vÃ  evaluating data. Ã tÆ°á»Ÿng lÃ  Ä‘Ã¡nh giÃ¡ thá»­ mÃ´ hÃ¬nh cÃ³ dá»± Ä‘oÃ¡n tá»‘t khÃ´ng khi cÃ³ dá»¯ liá»‡u má»›i vÃ o.\nTiáº¿p theo, ta sáº½ dÃ¹ng test data Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh vá»«a xÃ¢y dá»±ng. Káº¿t quáº£ cÃ³ váº» khÃ¡ tuyá»‡t vÃ¬ mÃ´ hÃ¬nh gáº§n nhÆ° theo sÃ¡t Ä‘Æ°á»£c dá»¯ liá»‡u cá»§a test data.\n\n\nCode\n# Step 6: Make predictions\nRNN_forecast &lt;- RNN_model %&gt;% \n  predict(x_test)\n\n\n2/2 - 0s - 85ms/epoch - 43ms/step\n\n\nCode\n# Step 7: Combine predicted and observed\nplot_data &lt;- data.frame(\n  time = c(min(m3$Date)-days(1),m3$Date),  # Time for the test set\n  actual = y_test,  # Actual values from the test set\n  forecast = RNN_forecast  # Forecasted values\n)\n\n# Step 8: Plot using Highcharts\nhighchart() %&gt;%\n  hc_title(text = \"Time Series Forecasting with Highcharts\") %&gt;%\n  hc_xAxis(\n    categories = plot_data$time,\n    title = list(text = \"Time\")\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Value\"),\n    plotLines = list(list(\n      value = 0,\n      width = 1,\n      color = \"gray\"\n    ))\n  ) %&gt;%\n  hc_add_series(\n    name = \"Actual Data\",\n    data = plot_data$actual,\n    type = \"line\",\n    color = \"#1f77b4\"  # Blue color for actual data\n  ) %&gt;%\n  hc_add_series(\n    name = \"Forecast\",\n    data = plot_data$forecast,\n    type = \"line\",\n    color = \"#ff7f0e\"  # Orange color for forecast data\n  ) %&gt;%\n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) %&gt;%\n  hc_legend(\n    enabled = TRUE\n  )\n\n\n\n\n\n\n\n\n2.4 MÃ´ hÃ¬nh LSTM:\nTiáº¿p theo, ta sáº½ xÃ¢y dá»±ng thá»­ mÃ´ hÃ¬nh LSTM. MÃ´ hÃ¬nh LSTM thÆ°á»ng bao gá»“m cÃ¡c lá»›p sau:\n\nLá»›p LSTM: ÄÃ¢y lÃ  lá»›p chÃ­nh, cÃ³ thá»ƒ cÃ³ má»™t hoáº·c nhiá»u lá»›p LSTM chá»“ng lÃªn nhau. Má»—i lá»›p LSTM cÃ³ thá»ƒ tráº£ vá» toÃ n bá»™ chuá»—i báº±ng return_sequences = TRUE hoáº·c chá»‰ tráº£ vá» giÃ¡ trá»‹ cuá»‘i cÃ¹ng báº±ng return_sequences = FALSE.\nLá»›p Dense: Sau khi thÃ´ng tin Ä‘Æ°á»£c xá»­ lÃ½ qua cÃ¡c lá»›p LSTM, nÃ³ sáº½ Ä‘Æ°á»£c Ä‘Æ°a qua cÃ¡c lá»›p Dense (lá»›p fully connected) Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng.\nLá»›p Dropout (tÃ¹y chá»n): Äá»ƒ trÃ¡nh overfitting, cÃ³ thá»ƒ thÃªm lá»›p dropout Ä‘á»ƒ táº¯t ngáº«u nhiÃªn má»™t sá»‘ nÆ¡-ron trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.\n\nVáº­y giá» ta sáº½ so sÃ¡nh vá»›i mÃ´ hÃ¬nh RNN trÆ°á»›c vá»›i mÃ´ hÃ¬nh LSTM qua 2 thÃ´ng sá»‘ Ä‘Ã£ chá»n mse vÃ  mae.\n\n\nCode\n# Extract metrics into a data frame\nresults_df &lt;- data.frame(\n  Model = c(\"RNN\", \"LSTM\"),\n  Metric = c(\"Loss\", \"metric\"),\n  MSE = c(RNN_result[[1]],RNN_result[[2]]),\n  MAE = c(LSTM_result[[1]], LSTM_result[[2]])\n)\n\nlibrary(gt)\n# Create a gt table\nresults_df %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Model Performance Metrics\",\n    subtitle = \"Comparison of MSE and MAE for RNN and LSTM\"\n  ) %&gt;%\n  fmt_number(\n    columns = vars(MSE, MAE),\n    decimals = 4\n  ) %&gt;%\n  cols_label(\n    Model = \"Model Type\",\n    MSE = \"Mean Squared Error\",\n    MAE = \"Mean Absolute Error\"\n  ) %&gt;%\n  tab_options(\n    table.font.size = 14,\n    heading.title.font.size = 16,\n    heading.subtitle.font.size = 14\n  )\n\n\n\n\n\n\n\n\nModel Performance Metrics\n\n\nComparison of MSE and MAE for RNN and LSTM\n\n\nModel Type\nMetric\nMean Squared Error\nMean Absolute Error\n\n\n\n\nRNN\nLoss\n0.0000\n0.0001\n\n\nLSTM\nmetric\n0.0020\n0.0067\n\n\n\n\n\n\n\nKáº¿t quáº£ cho tháº¥y mÃ´ hÃ¬nh RNN truyá»n thá»‘ng Ä‘Æ°a ra káº¿t quáº£ tá»‘t hÆ¡n LSTM máº·c dÃ¹ sai sá»‘ cá»§a LSTM Ä‘á»u &lt; 0.03 lÃ  khÃ´ng quÃ¡ tá»‡ nhÆ°ng tiÃªu chÃ­ váº«n lÃ  mÃ´ hÃ¬nh nÃ o hiá»‡u quáº£ nháº¥t.\n\n\nCode\nLSTM_forecast &lt;- LSTM_model %&gt;% \n  predict(x_test)\n\n\n2/2 - 1s - 626ms/epoch - 313ms/step\n\n\nCode\ncompare&lt;-data.frame(Date = c(min(m3$Date)-days(1),m3$Date),\n                    LSTM = round(LSTM_forecast - y_test,3),\n                    RNN = round(RNN_forecast - y_test,3)\n)\n\n# Create the highchart plot\nhighchart() %&gt;%\n  hc_chart(type = \"line\") %&gt;%\n  hc_title(text = \"Residual Comparison: LSTM vs RNN\") %&gt;%\n  hc_xAxis(\n    categories = compare$Date,\n    title = list(text = \"Date\")\n  ) %&gt;%\n  hc_yAxis(\n    title = list(text = \"Residuals\"),\n    plotLines = list(\n      list(value = 0, color = \"gray\", width = 1, dashStyle = \"Dash\")\n    )\n  ) %&gt;%\n  hc_add_series(\n    name = \"LSTM Residuals\",\n    data = compare$LSTM,\n    color = \"#1f77b4\"\n  ) %&gt;%\n  hc_add_series(\n    name = \"RNN Residuals\",\n    data = compare$RNN,\n    color = \"#ff7f0e\"\n  ) %&gt;%\n  hc_tooltip(shared = TRUE) %&gt;%\n  hc_legend(enabled = TRUE)"
  },
  {
    "objectID": "torch.html#káº¿t-luáº­n",
    "href": "torch.html#káº¿t-luáº­n",
    "title": "RNN and LSTM model",
    "section": "3 Káº¿t luáº­n:",
    "text": "3 Káº¿t luáº­n:\nNhÆ° váº­y, chÃºng ta Ä‘Ã£ Ä‘Æ°á»£c há»c vá» thuáº­t toÃ¡n Genetic vÃ  mÃ´ hÃ¬nh MILP cÅ©ng nhÆ° cÃ¡ch thá»±c hiá»‡n trong Rstudio.\nNáº¿u báº¡n cÃ³ cÃ¢u há»i hay tháº¯c máº¯c nÃ o, Ä‘á»«ng ngáº§n ngáº¡i liÃªn há»‡ vá»›i mÃ¬nh qua Gmail. BÃªn cáº¡nh Ä‘Ã³, náº¿u báº¡n muá»‘n xem láº¡i cÃ¡c bÃ i viáº¿t trÆ°á»›c Ä‘Ã¢y cá»§a mÃ¬nh, hÃ£y nháº¥n vÃ o hai nÃºt dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ truy cáº­p trang Rpubs hoáº·c mÃ£ nguá»“n trÃªn Github. Ráº¥t vui Ä‘Æ°á»£c Ä‘á»“ng hÃ nh cÃ¹ng báº¡n, háº¹n gáº·p láº¡i! ğŸ˜„ğŸ˜„ğŸ˜„\n\n\n\n    \n    \n    Contact Me\n    \n    \n    \n\n\n    \n        Contact Me\n        \n            Your Email:\n            \n            Please enter a valid email address.\n            Send Email\n        \n        \n            \n                \n                     View Code on GitHub\n                \n            \n        \n        \n            \n                \n                     Visit my RPubs"
  }
]