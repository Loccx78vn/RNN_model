{"title":"RNN and LSTM model","markdown":{"yaml":{"title":"RNN and LSTM model","subtitle":"Việt Nam, 2024","categories":["Machine Learning","Forecasting"],"format":{"html":{"code-fold":true,"code-tools":true}},"number-sections":true},"headingText":"Định nghĩa:","containsRefs":false,"markdown":"\n\nỞ đây ta sẽ học về mô hình machine learning được ứng dụng nhiều nhất trong việc phân tích dữ liệu thời gian là *RNN* và *LSTM*.\n\n\n### Mô hình RNN:\n\nĐiểm chung là cả hai mô hình đều thuộc phân lớp *Deep learning* - nghĩa là học máy sâu với đặc điểm chung là phân chia dữ liệu thành nhiều lớp và bắt đầu \"học\" dần qua từng lớp để đưa ra kết quả cuối cùng. Ở hình dưới đây, $X_o$ đại diện cho dữ liệu đầu vào, $h_t$ là output đầu ra của từng step và $A$ là những gì đã \"học\" được tại step đó và được truyền cho step tiếp theo. Trong tài liệu chuẩn thì họ thường kí hiệu là $X_t$, $Y_t$, $h_{t-1}$.\n\n```{=html}\n<div style=\"text-align: center; margin-bottom: 20px;\">\n  <img src=\"img/RNN.png\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\">\n  \n  <!-- Picture Name -->\n  <div style=\"text-align: left; margin-top: 10px;\">\n    Hình 1: Minh họa về sự phân chia dữ liệu thành nhiều lớp\n  </div>\n  \n  <!-- Source Link -->\n  <div style=\"text-align: right; font-style: italic; margin-top: 5px;\">\n    Source: <a href=\"https://dominhhai.github.io/vi/2017/10/what-is-lstm/\" target=\"_blank\">Link to Image</a>\n  </div>\n</div>\n```\nKhi nhìn hình thì bạn có thể bối rối chưa hiểu các kí tự và hình ảnh thì bạn có thể tưởng tượng **học máy** như 1 đứa trẻ và để nó có thể hiểu được câu: \"Hôm nay con đi học\" thì nó phải học từng chữ cái như: a,b,c,... trước ròi mới ghép thành từ đơn như: \"Hôm\",\"Nay\",... rồi ghép thành câu trên. Vậy giả sử như hôm nay học được từ \"Hôm\" thì nó sẽ bắt đầu ghi nhớ từ đã học vào trong $A$. Nếu sau này ta cần **học máy** hiểu câu \"Hôm sau con đi chơi\" thì tốc độ học của **học máy** sẽ nhanh lên vì thay vì nó phải học 5 chữ đơn như thông thường thì nó chỉ cần học 4 chữ còn lại trừ chữ \"hôm\". Vậy bạn đã hiểu ý tưởng nền tảng của *RNN* rồi ha!\n\nNếu muốn hiểu thêm về *RNN*, bạn có thể tham khảo link này: [Recurrent Neural Network: Từ RNN đến LSTM](https://viblo.asia/p/recurrent-neural-network-tu-rnn-den-lstm-gGJ597z1ZX2).\n\nVà trong *RNN* có 1 vấn đề lớn là *Vanishing Gradient* nghĩa là mô hình sẽ không còn \"học\" thêm được nữa cho dù tăng số `epochs`. Theo phần chứng minh của [anh Tuấn](https://nttuan8.com/bai-14-long-short-term-memory-lstm/) cho thấy *RNN* sẽ luôn xảy ra vấn đề đó cho dù bạn có xây dựng mô hình tốt như thế nào. Điều này có thể hiểu đơn giản như việc bạn học liên tục dẫn tới quá tải. Do đó, *RNN* chỉ học các thông tin $A$ từ các step gần nhất và đó là lí do ra đời *LSTM - Long short term memory.*\n\n::: callout-warning\n<u>Lưu ý</u>: Điều này không có nghĩa *LSTM* luôn tốt hơn *RNN* vì có những bài toán với đầu vào đơn giản thì mô hình chỉ cần học các step đầu là đã \"học\" đầy đủ thông tin cần thiết. \nMô hình *LSTM* phổ biến với các bài toán phức tạp như tự động dịch ngôn ngữ, ghi chép lại theo giọng nói...\n:::\n\n### Mô hình LSTM:\n\nCó thể xem mô hình *LSTM* như biến thể của *RNN*. Về cấu trúc, *LSTM* có ba cổng chính giúp nó xử lý và duy trì thông tin qua các bước thời gian:\n\n-   Cổng quên (Forget Gate): Quyết định thông tin nào cần bị quên trong trạng thái ô nhớ.\n\n-   Cổng nhập (Input Gate): Xác định thông tin nào cần được ghi vào trạng thái ô nhớ.\n\n-   Cổng xuất (Output Gate): Quyết định thông tin nào sẽ được xuất ra từ trạng thái ô nhớ để ảnh hưởng đến dự đoán tiếp theo.\n\n## Xây dựng mô hình:\n\n### Load dữ liệu:\n\nĐầu tiên ta sẽ load dữ liệu lại như trước. Ở đây, để đơn giản, mình chỉ xây dựng mô hình cho *product A* thôi.\n\n```{r}\n#| include: false\n#| warning: false\n#| message: false\npacman::p_load(\njanitor,\ntidyverse,\ndplyr,\ntidyr,\nmagrittr,\nggplot2)\n```\n\n```{r}\n#| include: false\n# Set the start and end date for the 6-month period\nstart_date <- as.Date(\"2024-05-01\")\nend_date <- as.Date(\"2024-10-31\")\n\n# Generate date range\ndates <- seq.Date(start_date, \n                  end_date, \n                  by = \"day\")\n\n# Set a random seed for reproducibility\nset.seed(42)\n\n# Create a vector of weekdays for each date\nweekdays <- weekdays(dates)\n\n# Simulate sales data for Product A, B, and C based on weekday patterns\nproduct_a_sales <- sample(5:50, length(dates), replace = TRUE)\nproduct_b_sales <- sample(3:40, length(dates), replace = TRUE)\nproduct_c_sales <- sample(2:30, length(dates), replace = TRUE)\n\n# Adjust sales based on the weekday\nfor (i in 1:length(dates)) {\n  if (weekdays[i] == \"Wednesday\" | weekdays[i] == \"Saturday\") {\n    # High demand for Product A and B on Wednesday and Saturday\n    product_a_sales[i] <- sample(40:70, 1)\n    product_b_sales[i] <- sample(30:60, 1)\n  } else if (weekdays[i] == \"Monday\" | weekdays[i] == \"Tuesday\") {\n    # High demand for Product C on Monday and Tuesday\n    product_c_sales[i] <- sample(20:40, 1)\n  }\n}\n\n# Create a data frame with the adjusted sales data\nsales_data <- data.frame(\n  Date = dates,\n  Weekday = weekdays,\n  Product_A = product_a_sales,\n  Product_B = product_b_sales,\n  Product_C = product_c_sales\n)\n```\n\nGiả sử công ty mình đang kinh doanh 3 loại mặt hàng *product A*,*product B*,*product C* và đây là biểu đồ thể hiện nhu cầu của cả 3 mặt hàng từ tháng 5 tới tháng 10.\n\n```{r}\n#| warning: false\n#| message: false\nlibrary(highcharter)\nsales_data |> \n  select(-Weekday) |> \n  pivot_longer(cols = c(Product_A, Product_B, Product_C),\n               names_to = \"Product\",\n               values_to = \"Sales\") |> \n  hchart(\"line\", hcaes(x = Date, y = Sales, group = Product))\n```\n\nNếu ta phân tich sâu về nhu cầu của từng mặt hàng theo thứ trong tuần, ta sẽ thấy rằng mặt hàng A, B thì bán chạy vào thứ 4 và thứ 7, còn mặt hàng C thì bán chạy vào thứ 2 và thứ 3.\n\n::: panel-tabset\n\n##### Product A:\n```{r}\n#| warning: false\n#| message: false\n#| echo: false\nmA<-sales_data |> \n  select(Date, \n         Weekday,\n         Product_A)  \n\n# Ensure 'Weekday' is a factor with the correct order\nmA$Weekday <- factor(mA$Weekday, \n                     levels = c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"))\n\nhcboxplot(\n    x = mA$Product_A,\n    var = mA$Weekday,\n    name = \"Weekday sales\") |> \n  hc_title(text = \"Comparing sales data between weekday\") |> \n  hc_yAxis(title = list(text = \"No.product\")) |> \n  hc_chart(type = \"column\")\n```\n\n##### Product B:\n```{r}\n#| warning: false\n#| message: false\n#| echo: false\nmB<-sales_data |> \n  select(Date, \n         Weekday,\n         Product_B)  \n\n# Ensure 'Weekday' is a factor with the correct order\nmA$Weekday <- factor(mB$Weekday, \n                     levels = c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"))\n\nhcboxplot(\n    x = mB$Product_B,\n    var = mB$Weekday,\n    name = \"Weekday sales\") |> \n  hc_title(text = \"Comparing sales data between weekday\") |> \n  hc_yAxis(title = list(text = \"No.product\")) |> \n   hc_chart(type = \"column\")\n```\n\n##### Product C:\n```{r}\n#| warning: false\n#| message: false\n#| echo: false\nmC<-sales_data |> \n  select(Date, \n         Weekday,\n         Product_C)  \n\n# Ensure 'Weekday' is a factor with the correct order\nmA$Weekday <- factor(mC$Weekday, \n                     levels = c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"))\n\nhcboxplot(\n    x = mC$Product_C,\n    var = mC$Weekday,\n    name = \"Weekday sales\") |> \n  hc_title(text = \"Comparing sales data between weekday\") |> \n  hc_yAxis(title = list(text = \"No.product\")) |> \n   hc_chart(type = \"column\")\n```\n:::\n\n```{r}\n#| warning: false\n#| message: false\n#| include: false\nlibrary(tidyverse)\n# Set the start and end date for the 6-month period\nstart_date <- as.Date(\"2024-05-01\")\nend_date <- as.Date(\"2024-10-31\")\n\n# Generate date range\ndates <- seq.Date(start_date, \n                  end_date, \n                  by = \"day\")\n\n# Set a random seed for reproducibility\nset.seed(42)\n\n# Create a vector of weekdays for each date\nweekdays <- weekdays(dates)\n\n# Simulate sales data for Product A, B, and C based on weekday patterns\nproduct_a_sales <- sample(5:50, length(dates), replace = TRUE)\nproduct_b_sales <- sample(3:40, length(dates), replace = TRUE)\nproduct_c_sales <- sample(2:30, length(dates), replace = TRUE)\n\n# Adjust sales based on the weekday\nfor (i in 1:length(dates)) {\n  if (weekdays[i] == \"Wednesday\" | weekdays[i] == \"Saturday\") {\n    # High demand for Product A and B on Wednesday and Saturday\n    product_a_sales[i] <- sample(40:70, 1)\n    product_b_sales[i] <- sample(30:60, 1)\n  } else if (weekdays[i] == \"Monday\" | weekdays[i] == \"Tuesday\") {\n    # High demand for Product C on Monday and Tuesday\n    product_c_sales[i] <- sample(20:40, 1)\n  }\n}\n```\n\nThông thường dữ liệu để *train model* trong *machine learning* thường cần trải qua bước *normalize data* nghĩa là đưa tất cả dữ liệu về chung 1 thước đo và phạm vi. Nguyên do vì điều này giúp nhiều thuật toán học máy dễ dàng hội tụ hơn. Ví dụ, các thuật toán như *k-Nearest Neighbors (KNN)* và *Support Vector Machines (SVM)* rất nhạy cảm với khoảng cách giữa các điểm dữ liệu nên nếu dữ liệu không được chuẩn hóa, thuật toán có thể ưu tiên các đặc trưng có phạm vi lớn hơn và bỏ qua các đặc trưng có phạm vi nhỏ hơn, dẫn đến hiệu suất kém. Và công thức phổ biến nhất cho chuẩn hóa là:\n\n$$\n\\text{Normalized Value} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n$$\n\n```{r}\n#| warning: false\n#| message: false\n# Create a data frame with the adjusted sales data\nsales_data <- data.frame(\n  Date = dates,\n  Weekday = weekdays,\n  Product_A = product_a_sales,\n  Product_B = product_b_sales,\n  Product_C = product_c_sales\n)\n\n# Convert the sales data to a time series (ts) object for Product A\nproduct_a_ts <- ts(sales_data$Product_A, start = c(2024, 5), \n                   frequency = 365)\n                   \n\n# Normalzie data:\ntime_series_data<-scale(product_a_ts)\n\nlibrary(highcharter)\nhighchart() %>%\n  hc_add_series(data = as.numeric(time_series_data), type = \"line\", name = \"Sales of Product A\") %>%\n  hc_title(text = \"Normalized Time Series of Product A\") %>%\n  hc_xAxis(title = list(text = \"Date\")) %>%\n  hc_yAxis(title = list(text = \"Normalized Sales\")) %>%\n  hc_tooltip(shared = TRUE) %>%\n  hc_plotOptions(line = list(marker = list(enabled = FALSE)))\n```\n\n### Chia dữ liệu:\n\nVậy để *train data*, mình sẽ chia bộ dữ liệu thành 3 phần:\n\n-   *Training data*: dùng để huấn luyện và xây dựng mô hình.\n\n-   *Evaluating data*: đánh giá mô hình vừa huấn luyện.\n\n-   *Testing data*: dùng để đánh giá lại nếu muốn mô hình học lại dữ liệu\n\n```{r}\n#| warning: false\n#| message: false\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(dplyr)\n\n# Function to create supervised learning format from time series\ncreate_supervised_data <- function(series, n_in = 1, n_out = 1) {\n  series <- as.vector(series)  # Convert time series object to vector\n  data <- data.frame(series)\n  \n  # Use base R lag function for ts objects (lag() from stats package)\n  for (i in 1:n_in) {\n    data <- cbind(data, stats::lag(series, -i))\n  }\n  \n  colnames(data) <- c(paste0('t-', 1:n_in), 't+1')  # Correctly name columns\n  return(data)\n}\n\n# Prepare the data with 12 input lags and 1 output (next time step)\nsupervised_data <- create_supervised_data(time_series_data,\n                                          n_in = 12, \n                                          n_out = 1)\n\n# Remove NA rows created by lag function\nsupervised_data <- na.omit(supervised_data)\n\n# Step 2: Split data into training and test sets\ntrain_size <- round(0.7 * nrow(supervised_data))   # 70% for training\nval_size <- round(0.1 * nrow(supervised_data))     # 10% for validation\ntest_size <- nrow(supervised_data) - train_size - val_size  # 20% for testing\n\ntrain_data <- supervised_data[1:train_size, ]\nval_data <- supervised_data[(train_size + 1):(train_size + val_size), ]\ntest_data <- supervised_data[(train_size + val_size + 1):nrow(supervised_data), ]\n\n# Correct column selection\nx_train <- as.matrix(train_data[, 1:12])  # Input features (12 lags)\ny_train <- as.matrix(train_data[, 't+1'])  # Target output (next time step)\n\nx_val <- as.matrix(val_data[, 1:12])  # Input features for validation\ny_val <- as.matrix(val_data[, 't+1'])  # Actual output for validation\n\nx_test <- as.matrix(test_data[, 1:12])  # Input features for testing\ny_test <- as.matrix(test_data[, 't+1'])  # Actual output for testing\n\n\n## Plot the result:\nlibrary(xts)\nn<-quantile(sales_data$Date, \n            probs = c(0, 0.7, 0.8,1), \n            type = 1)\n\nm1<-sales_data %>% \n  filter(Date <= n[[2]])\nm2<-sales_data %>% \n  filter(Date <= n[[3]] & Date > n[[2]])\nm3<-sales_data %>% \n  filter(Date <= n[[4]] & Date > n[[3]])\n\ndemand_training<-xts(x=m1$Product_A,\n                     order.by=m1$Date)\ndemand_testing<-xts(x=m2$Product_A,\n                     order.by=m2$Date)\ndemand_forecasting<-xts(x=m3$Product_A,\n                     order.by=m3$Date)\n\nlibrary(dygraphs)\nlines<-cbind(demand_training,\n             demand_testing,\n             demand_forecasting)\ndygraph(lines,\n        main = \"Training and testing data\", \n        ylab = \"Quantity order (Unit: Millions)\") %>% \n  dySeries(\"demand_training\", label = \"Training data\") %>%\n  dySeries(\"demand_testing\", label = \"Testing data\") %>%\n  dySeries(\"demand_forecasting\", label = \"Forecasting data\") %>%\n  dyOptions(fillGraph = TRUE, fillAlpha = 0.4) %>% \n  dyRangeSelector(height = 20)\n```\n\n### Mô hình RNN:\n\nSau đó, ta sẽ bắt đầu *train model* bằng cách tạo thêm 12 cột giá trị là giá trị quá khứ của *demand*. Bạn sẽ bắt đầu định nghĩa mô hình gồm:\n\n-   *Input*: dùng hàm `layer_input(shape = input_shape)` với `input_shape` là số lượng *predictor*.\n\n-   *Layer*: là các hidden layer trong mô hình thêm vào bằng hàm `layer_dense(x, units = 64, activation = 'relu')` với đối số `units` thường là bội số của 32 như 32,64,256,...\n\n-   *Output*: dùng hàm `layer_dense(x, units = 1)` để định nghĩa là đầu ra chỉ có 1 giá trị.\n\n```{r}\n# Step 3: Build a simple transformer-like model\nRNN_model <- function(input_shape) {\n  inputs <- layer_input(shape = input_shape)\n\n  # Transformer Encoder Layer (simplified)\n  x <- inputs\n  x <- layer_dense(x, units = 64, activation = 'relu')  # Dense layer\n  x <- layer_dense(x, units = 32, activation = 'relu')  # Another dense layer\n\n  # Output layer\n  x <- layer_dense(x, units = 1)\n  \n  model <- keras_model(inputs, x)\n  return(model)\n}\n\n# Example input shape (12 time steps input per sample)\ninput_shape <- c(12)\n\nRNN_model <- RNN_model(input_shape)\n```\n\n```{r}\n#| warning: false\n#| message: false\n#| include: false\n# Step 4: Compile the model\nRNN_model %>% compile(\n  loss = 'mse',\n  optimizer = optimizer_adam(),\n  metrics = c('mae')\n)\n\n# Step 5: Train the model\nRNN_history <- RNN_model %>% fit(\n  x_train, \n  y_train,\n  epochs = 50, \n  batch_size = 32,\n  validation_data = list(x_val, y_val)\n)\n\nRNN_result <- RNN_model %>% \n    evaluate(x_test, y_test)\n```\n\nĐối với các mô hình truyền thống như *linear regression* thì bạn đã quen với thông số $R^2$ để đánh giá mô hình, còn với mô hình *Machine learning* thì dùng khái niệm *loss function - hàm mất mát*. Về khái niệm, *loss function* sẽ đo lường chênh lệch giữa *predicted* và *actual* trong bộ *training data* nên khi càng tăng `epochs` nghĩa là tăng số lần học lại dữ liệu thì *loss function* sẽ tính ra giá trị càng thấp. Như mô hình trên thì mình đặt đối số `loss = mse` nghĩa là sử dụng *Mean Squared Error* để tối ưu quy trình học của học máy. Công thức như sau:\n\n$$\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{pred}}(i) - y_{\\text{true}}(i))^2\n$$\n\nCòn đối số `metrics = c('mae')` nghĩa là tiêu chí khác để theo dõi và đánh giá mô hình. Vậy tại sao cần có 2 tham số đánh giá song song như vậy là vì như đã nói, nếu bạn càng tăng `epochs` thì giá trị *loss* càng thấp trong khi dùng `metrics` sẽ đưa ra đánh giá khách quan hơn về mô hình mà không phụ thuộc vào số lần `epochs`. Công thức như sau:\n\n$$\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{pred}}(i) - y_{\\text{true}}(i)|\n$$\n\nVậy khi chạy code, R sẽ return output như biểu đồ dưới đây là so sánh tham số của *mse* và *mae* giữa *training data* và *evaluating data*. Ý tưởng là đánh giá thử mô hình có dự đoán tốt không khi có dữ liệu mới vào.\n\nTiếp theo, ta sẽ dùng *test data* để đánh giá mô hình vừa xây dựng. Kết quả có vẻ khá tuyệt vì mô hình gần như theo sát được dữ liệu của *test data*.\n\n```{r}\n# Step 6: Make predictions\nRNN_forecast <- RNN_model %>% \n  predict(x_test)\n\n# Step 7: Combine predicted and observed\nplot_data <- data.frame(\n  time = c(min(m3$Date)-days(1),m3$Date),  # Time for the test set\n  actual = y_test,  # Actual values from the test set\n  forecast = RNN_forecast  # Forecasted values\n)\n\n# Step 8: Plot using Highcharts\nhighchart() %>%\n  hc_title(text = \"Time Series Forecasting with Highcharts\") %>%\n  hc_xAxis(\n    categories = plot_data$time,\n    title = list(text = \"Time\")\n  ) %>%\n  hc_yAxis(\n    title = list(text = \"Value\"),\n    plotLines = list(list(\n      value = 0,\n      width = 1,\n      color = \"gray\"\n    ))\n  ) %>%\n  hc_add_series(\n    name = \"Actual Data\",\n    data = plot_data$actual,\n    type = \"line\",\n    color = \"#1f77b4\"  # Blue color for actual data\n  ) %>%\n  hc_add_series(\n    name = \"Forecast\",\n    data = plot_data$forecast,\n    type = \"line\",\n    color = \"#ff7f0e\"  # Orange color for forecast data\n  ) %>%\n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) %>%\n  hc_legend(\n    enabled = TRUE\n  )\n```\n\n### Mô hình LSTM:\n\nTiếp theo, ta sẽ xây dựng thử mô hình *LSTM*. Mô hình LSTM thường bao gồm các lớp sau:\n\n-   Lớp LSTM: Đây là lớp chính, có thể có một hoặc nhiều lớp LSTM chồng lên nhau. Mỗi lớp LSTM có thể trả về toàn bộ chuỗi bằng `return_sequences = TRUE` hoặc chỉ trả về giá trị cuối cùng bằng `return_sequences = FALSE`.\n\n-   Lớp Dense: Sau khi thông tin được xử lý qua các lớp LSTM, nó sẽ được đưa qua các lớp Dense (lớp fully connected) để đưa ra dự đoán cuối cùng.\n\n-   Lớp Dropout (tùy chọn): Để tránh overfitting, có thể thêm lớp dropout để tắt ngẫu nhiên một số nơ-ron trong quá trình huấn luyện.\n\n```{r}\n#| warning: false\n#| message: false\n#| include: false\n# Step 3: Build an enhanced LSTM model\nLSTM_model <- function(input_shape) {\n  # Define input layer\n  inputs <- layer_input(shape = input_shape)\n  \n  # First LSTM layer (returns the full sequence for the next layer)\n  x <- layer_lstm(units = 50, return_sequences = TRUE, activation = 'tanh')(inputs)\n  \n  # Second LSTM layer (returns the full sequence, could also be 'return_sequences = FALSE' for output prediction)\n  x <- layer_lstm(units = 50, return_sequences = FALSE, activation = 'tanh')(x)\n  \n  # Output layer (prediction for the next time step)\n  outputs <- layer_dense(units = 1)(x)\n  \n  # Create the model\n  model <- keras_model(inputs, outputs)\n  \n  # Return the model\n  return(model)\n}\n\n\n# Update input shape for LSTM (timesteps = 12, features = 1)\ninput_shape <- c(12, 1)\n\n# Reshape the input data for LSTM (add a feature dimension)\nx_train <- array_reshape(x_train, dim = c(nrow(x_train), 12, 1))\nx_test <- array_reshape(x_test, dim = c(nrow(x_test), 12, 1))\nx_val <- array_reshape(x_val, dim = c(nrow(x_val), 12, 1))\n\n# Build the enhanced LSTM model\nLSTM_model <- LSTM_model(input_shape)\n\n# Step 4: Compile the model\nLSTM_model %>% compile(\n  loss = 'mse',\n  optimizer = optimizer_adam(),\n  metrics = c('mae')\n)\n\n# Step 5: Train the model\nLSTM_history <- LSTM_model %>% fit(\n  x_train, y_train,\n  epochs = 50, batch_size = 32,\n  validation_data = list(x_val, y_val)\n)\n\nLSTM_result <- LSTM_model %>% \n    evaluate(x_test, y_test)\n```\n\nVậy giờ ta sẽ so sánh với mô hình *RNN* trước với mô hình *LSTM* qua 2 thông số đã chọn *mse* và *mae*.\n\n```{r}\n#| warning: false\n#| message: false\n# Extract metrics into a data frame\nresults_df <- data.frame(\n  Model = c(\"RNN\", \"LSTM\"),\n  Metric = c(\"Loss\", \"metric\"),\n  MSE = c(RNN_result[[1]],RNN_result[[2]]),\n  MAE = c(LSTM_result[[1]], LSTM_result[[2]])\n)\n\nlibrary(gt)\n# Create a gt table\nresults_df %>%\n  gt() %>%\n  tab_header(\n    title = \"Model Performance Metrics\",\n    subtitle = \"Comparison of MSE and MAE for RNN and LSTM\"\n  ) %>%\n  fmt_number(\n    columns = vars(MSE, MAE),\n    decimals = 4\n  ) %>%\n  cols_label(\n    Model = \"Model Type\",\n    MSE = \"Mean Squared Error\",\n    MAE = \"Mean Absolute Error\"\n  ) %>%\n  tab_options(\n    table.font.size = 14,\n    heading.title.font.size = 16,\n    heading.subtitle.font.size = 14\n  )\n```\n\nKết quả cho thấy mô hình *RNN* truyền thống đưa ra kết quả tốt hơn *LSTM* mặc dù sai số của *LSTM* đều \\< 0.03 là không quá tệ nhưng tiêu chí vẫn là mô hình nào hiệu quả nhất.\n\n```{r}\nLSTM_forecast <- LSTM_model %>% \n  predict(x_test)\n\ncompare<-data.frame(Date = c(min(m3$Date)-days(1),m3$Date),\n                    LSTM = round(LSTM_forecast - y_test,3),\n                    RNN = round(RNN_forecast - y_test,3)\n)\n\n# Create the highchart plot\nhighchart() %>%\n  hc_chart(type = \"line\") %>%\n  hc_title(text = \"Residual Comparison: LSTM vs RNN\") %>%\n  hc_xAxis(\n    categories = compare$Date,\n    title = list(text = \"Date\")\n  ) %>%\n  hc_yAxis(\n    title = list(text = \"Residuals\"),\n    plotLines = list(\n      list(value = 0, color = \"gray\", width = 1, dashStyle = \"Dash\")\n    )\n  ) %>%\n  hc_add_series(\n    name = \"LSTM Residuals\",\n    data = compare$LSTM,\n    color = \"#1f77b4\"\n  ) %>%\n  hc_add_series(\n    name = \"RNN Residuals\",\n    data = compare$RNN,\n    color = \"#ff7f0e\"\n  ) %>%\n  hc_tooltip(shared = TRUE) %>%\n  hc_legend(enabled = TRUE)\n```\n\n\n## Kết luận:\n\nNhư vậy, chúng ta đã được học về thuật toán Genetic và mô hình MILP cũng như cách thực hiện trong Rstudio.\n\nNếu bạn có câu hỏi hay thắc mắc nào, đừng ngần ngại liên hệ với mình qua Gmail. Bên cạnh đó, nếu bạn muốn xem lại các bài viết trước đây của mình, hãy nhấn vào hai nút dưới đây để truy cập trang **Rpubs** hoặc mã nguồn trên **Github**. Rất vui được đồng hành cùng bạn, hẹn gặp lại! 😄😄😄\n\n```{=html}\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Contact Me</title>\n    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css\">\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/simple-icons@v6.0.0/svgs/rstudio.svg\">\n    <style>\n        body { font-family: Arial, sans-serif; background-color: $secondary-color; }\n        .container { max-width: 400px; margin: auto; padding: 20px; background: white; border-radius: 8px; box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1); }\n        label { display: block; margin: 10px 0 5px; }\n        input[type=\"email\"] { width: 100%; padding: 10px; margin-bottom: 15px; border: 1px solid #ccc; border-radius: 4px; }\n        .github-button, .rpubs-button { margin-top: 20px; text-align: center; }\n        .github-button button, .rpubs-button button { background-color: #333; color: white; border: none; padding: 10px; cursor: pointer; border-radius: 4px; width: 100%; }\n        .github-button button:hover, .rpubs-button button:hover { background-color: #555; }\n        .rpubs-button button { background-color: #75AADB; }\n        .rpubs-button button:hover { background-color: #5A9BC2; }\n        .rpubs-icon { margin-right: 5px; width: 20px; vertical-align: middle; filter: brightness(0) invert(1); }\n        .error-message { color: red; font-size: 0.9em; margin-top: 5px; }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h2>Contact Me</h2>\n        <form id=\"emailForm\">\n            <label for=\"email\">Your Email:</label>\n            <input type=\"email\" id=\"email\" name=\"email\" required aria-label=\"Email Address\">\n            <div class=\"error-message\" id=\"error-message\" style=\"display: none;\">Please enter a valid email address.</div>\n            <button type=\"submit\">Send Email</button>\n        </form>\n        <div class=\"github-button\">\n            <button>\n                <a href=\"https://github.com/Loccx78vn/Time_series_forcasting\" target=\"_blank\" style=\"color: white; text-decoration: none;\">\n                    <i class=\"fab fa-github\"></i> View Code on GitHub\n                </a>\n            </button>\n        </div>\n        <div class=\"rpubs-button\">\n            <button>\n                <a href=\"https://rpubs.com/loccx\" target=\"_blank\" style=\"color: white; text-decoration: none;\">\n                    <img src=\"https://cdn.jsdelivr.net/npm/simple-icons@v6.0.0/icons/rstudio.svg\" alt=\"RStudio icon\" class=\"rpubs-icon\"> Visit my RPubs\n                </a>\n            </button>\n        </div>\n    </div>\n\n    <script>\n        document.getElementById('emailForm').addEventListener('submit', function(event) {\n            event.preventDefault(); // Prevent default form submission\n            const emailInput = document.getElementById('email');\n            const email = emailInput.value;\n            const errorMessage = document.getElementById('error-message');\n\n            // Simple email validation regex\n            const emailPattern = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n\n            if (emailPattern.test(email)) {\n                errorMessage.style.display = 'none'; // Hide error message\n                const yourEmail = 'loccaoxuan103@gmail.com'; // Your email\n                const gmailLink = `https://mail.google.com/mail/?view=cm&fs=1&to=${yourEmail}&su=Help%20Request%20from%20${encodeURIComponent(email)}`;\n                window.open(gmailLink, '_blank'); // Open in new tab\n            } else {\n                errorMessage.style.display = 'block'; // Show error message\n            }\n        });\n    </script>\n</body>\n</html>\n```","srcMarkdownNoYaml":"\n\nỞ đây ta sẽ học về mô hình machine learning được ứng dụng nhiều nhất trong việc phân tích dữ liệu thời gian là *RNN* và *LSTM*.\n\n## Định nghĩa:\n\n### Mô hình RNN:\n\nĐiểm chung là cả hai mô hình đều thuộc phân lớp *Deep learning* - nghĩa là học máy sâu với đặc điểm chung là phân chia dữ liệu thành nhiều lớp và bắt đầu \"học\" dần qua từng lớp để đưa ra kết quả cuối cùng. Ở hình dưới đây, $X_o$ đại diện cho dữ liệu đầu vào, $h_t$ là output đầu ra của từng step và $A$ là những gì đã \"học\" được tại step đó và được truyền cho step tiếp theo. Trong tài liệu chuẩn thì họ thường kí hiệu là $X_t$, $Y_t$, $h_{t-1}$.\n\n```{=html}\n<div style=\"text-align: center; margin-bottom: 20px;\">\n  <img src=\"img/RNN.png\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\">\n  \n  <!-- Picture Name -->\n  <div style=\"text-align: left; margin-top: 10px;\">\n    Hình 1: Minh họa về sự phân chia dữ liệu thành nhiều lớp\n  </div>\n  \n  <!-- Source Link -->\n  <div style=\"text-align: right; font-style: italic; margin-top: 5px;\">\n    Source: <a href=\"https://dominhhai.github.io/vi/2017/10/what-is-lstm/\" target=\"_blank\">Link to Image</a>\n  </div>\n</div>\n```\nKhi nhìn hình thì bạn có thể bối rối chưa hiểu các kí tự và hình ảnh thì bạn có thể tưởng tượng **học máy** như 1 đứa trẻ và để nó có thể hiểu được câu: \"Hôm nay con đi học\" thì nó phải học từng chữ cái như: a,b,c,... trước ròi mới ghép thành từ đơn như: \"Hôm\",\"Nay\",... rồi ghép thành câu trên. Vậy giả sử như hôm nay học được từ \"Hôm\" thì nó sẽ bắt đầu ghi nhớ từ đã học vào trong $A$. Nếu sau này ta cần **học máy** hiểu câu \"Hôm sau con đi chơi\" thì tốc độ học của **học máy** sẽ nhanh lên vì thay vì nó phải học 5 chữ đơn như thông thường thì nó chỉ cần học 4 chữ còn lại trừ chữ \"hôm\". Vậy bạn đã hiểu ý tưởng nền tảng của *RNN* rồi ha!\n\nNếu muốn hiểu thêm về *RNN*, bạn có thể tham khảo link này: [Recurrent Neural Network: Từ RNN đến LSTM](https://viblo.asia/p/recurrent-neural-network-tu-rnn-den-lstm-gGJ597z1ZX2).\n\nVà trong *RNN* có 1 vấn đề lớn là *Vanishing Gradient* nghĩa là mô hình sẽ không còn \"học\" thêm được nữa cho dù tăng số `epochs`. Theo phần chứng minh của [anh Tuấn](https://nttuan8.com/bai-14-long-short-term-memory-lstm/) cho thấy *RNN* sẽ luôn xảy ra vấn đề đó cho dù bạn có xây dựng mô hình tốt như thế nào. Điều này có thể hiểu đơn giản như việc bạn học liên tục dẫn tới quá tải. Do đó, *RNN* chỉ học các thông tin $A$ từ các step gần nhất và đó là lí do ra đời *LSTM - Long short term memory.*\n\n::: callout-warning\n<u>Lưu ý</u>: Điều này không có nghĩa *LSTM* luôn tốt hơn *RNN* vì có những bài toán với đầu vào đơn giản thì mô hình chỉ cần học các step đầu là đã \"học\" đầy đủ thông tin cần thiết. \nMô hình *LSTM* phổ biến với các bài toán phức tạp như tự động dịch ngôn ngữ, ghi chép lại theo giọng nói...\n:::\n\n### Mô hình LSTM:\n\nCó thể xem mô hình *LSTM* như biến thể của *RNN*. Về cấu trúc, *LSTM* có ba cổng chính giúp nó xử lý và duy trì thông tin qua các bước thời gian:\n\n-   Cổng quên (Forget Gate): Quyết định thông tin nào cần bị quên trong trạng thái ô nhớ.\n\n-   Cổng nhập (Input Gate): Xác định thông tin nào cần được ghi vào trạng thái ô nhớ.\n\n-   Cổng xuất (Output Gate): Quyết định thông tin nào sẽ được xuất ra từ trạng thái ô nhớ để ảnh hưởng đến dự đoán tiếp theo.\n\n## Xây dựng mô hình:\n\n### Load dữ liệu:\n\nĐầu tiên ta sẽ load dữ liệu lại như trước. Ở đây, để đơn giản, mình chỉ xây dựng mô hình cho *product A* thôi.\n\n```{r}\n#| include: false\n#| warning: false\n#| message: false\npacman::p_load(\njanitor,\ntidyverse,\ndplyr,\ntidyr,\nmagrittr,\nggplot2)\n```\n\n```{r}\n#| include: false\n# Set the start and end date for the 6-month period\nstart_date <- as.Date(\"2024-05-01\")\nend_date <- as.Date(\"2024-10-31\")\n\n# Generate date range\ndates <- seq.Date(start_date, \n                  end_date, \n                  by = \"day\")\n\n# Set a random seed for reproducibility\nset.seed(42)\n\n# Create a vector of weekdays for each date\nweekdays <- weekdays(dates)\n\n# Simulate sales data for Product A, B, and C based on weekday patterns\nproduct_a_sales <- sample(5:50, length(dates), replace = TRUE)\nproduct_b_sales <- sample(3:40, length(dates), replace = TRUE)\nproduct_c_sales <- sample(2:30, length(dates), replace = TRUE)\n\n# Adjust sales based on the weekday\nfor (i in 1:length(dates)) {\n  if (weekdays[i] == \"Wednesday\" | weekdays[i] == \"Saturday\") {\n    # High demand for Product A and B on Wednesday and Saturday\n    product_a_sales[i] <- sample(40:70, 1)\n    product_b_sales[i] <- sample(30:60, 1)\n  } else if (weekdays[i] == \"Monday\" | weekdays[i] == \"Tuesday\") {\n    # High demand for Product C on Monday and Tuesday\n    product_c_sales[i] <- sample(20:40, 1)\n  }\n}\n\n# Create a data frame with the adjusted sales data\nsales_data <- data.frame(\n  Date = dates,\n  Weekday = weekdays,\n  Product_A = product_a_sales,\n  Product_B = product_b_sales,\n  Product_C = product_c_sales\n)\n```\n\nGiả sử công ty mình đang kinh doanh 3 loại mặt hàng *product A*,*product B*,*product C* và đây là biểu đồ thể hiện nhu cầu của cả 3 mặt hàng từ tháng 5 tới tháng 10.\n\n```{r}\n#| warning: false\n#| message: false\nlibrary(highcharter)\nsales_data |> \n  select(-Weekday) |> \n  pivot_longer(cols = c(Product_A, Product_B, Product_C),\n               names_to = \"Product\",\n               values_to = \"Sales\") |> \n  hchart(\"line\", hcaes(x = Date, y = Sales, group = Product))\n```\n\nNếu ta phân tich sâu về nhu cầu của từng mặt hàng theo thứ trong tuần, ta sẽ thấy rằng mặt hàng A, B thì bán chạy vào thứ 4 và thứ 7, còn mặt hàng C thì bán chạy vào thứ 2 và thứ 3.\n\n::: panel-tabset\n\n##### Product A:\n```{r}\n#| warning: false\n#| message: false\n#| echo: false\nmA<-sales_data |> \n  select(Date, \n         Weekday,\n         Product_A)  \n\n# Ensure 'Weekday' is a factor with the correct order\nmA$Weekday <- factor(mA$Weekday, \n                     levels = c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"))\n\nhcboxplot(\n    x = mA$Product_A,\n    var = mA$Weekday,\n    name = \"Weekday sales\") |> \n  hc_title(text = \"Comparing sales data between weekday\") |> \n  hc_yAxis(title = list(text = \"No.product\")) |> \n  hc_chart(type = \"column\")\n```\n\n##### Product B:\n```{r}\n#| warning: false\n#| message: false\n#| echo: false\nmB<-sales_data |> \n  select(Date, \n         Weekday,\n         Product_B)  \n\n# Ensure 'Weekday' is a factor with the correct order\nmA$Weekday <- factor(mB$Weekday, \n                     levels = c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"))\n\nhcboxplot(\n    x = mB$Product_B,\n    var = mB$Weekday,\n    name = \"Weekday sales\") |> \n  hc_title(text = \"Comparing sales data between weekday\") |> \n  hc_yAxis(title = list(text = \"No.product\")) |> \n   hc_chart(type = \"column\")\n```\n\n##### Product C:\n```{r}\n#| warning: false\n#| message: false\n#| echo: false\nmC<-sales_data |> \n  select(Date, \n         Weekday,\n         Product_C)  \n\n# Ensure 'Weekday' is a factor with the correct order\nmA$Weekday <- factor(mC$Weekday, \n                     levels = c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"))\n\nhcboxplot(\n    x = mC$Product_C,\n    var = mC$Weekday,\n    name = \"Weekday sales\") |> \n  hc_title(text = \"Comparing sales data between weekday\") |> \n  hc_yAxis(title = list(text = \"No.product\")) |> \n   hc_chart(type = \"column\")\n```\n:::\n\n```{r}\n#| warning: false\n#| message: false\n#| include: false\nlibrary(tidyverse)\n# Set the start and end date for the 6-month period\nstart_date <- as.Date(\"2024-05-01\")\nend_date <- as.Date(\"2024-10-31\")\n\n# Generate date range\ndates <- seq.Date(start_date, \n                  end_date, \n                  by = \"day\")\n\n# Set a random seed for reproducibility\nset.seed(42)\n\n# Create a vector of weekdays for each date\nweekdays <- weekdays(dates)\n\n# Simulate sales data for Product A, B, and C based on weekday patterns\nproduct_a_sales <- sample(5:50, length(dates), replace = TRUE)\nproduct_b_sales <- sample(3:40, length(dates), replace = TRUE)\nproduct_c_sales <- sample(2:30, length(dates), replace = TRUE)\n\n# Adjust sales based on the weekday\nfor (i in 1:length(dates)) {\n  if (weekdays[i] == \"Wednesday\" | weekdays[i] == \"Saturday\") {\n    # High demand for Product A and B on Wednesday and Saturday\n    product_a_sales[i] <- sample(40:70, 1)\n    product_b_sales[i] <- sample(30:60, 1)\n  } else if (weekdays[i] == \"Monday\" | weekdays[i] == \"Tuesday\") {\n    # High demand for Product C on Monday and Tuesday\n    product_c_sales[i] <- sample(20:40, 1)\n  }\n}\n```\n\nThông thường dữ liệu để *train model* trong *machine learning* thường cần trải qua bước *normalize data* nghĩa là đưa tất cả dữ liệu về chung 1 thước đo và phạm vi. Nguyên do vì điều này giúp nhiều thuật toán học máy dễ dàng hội tụ hơn. Ví dụ, các thuật toán như *k-Nearest Neighbors (KNN)* và *Support Vector Machines (SVM)* rất nhạy cảm với khoảng cách giữa các điểm dữ liệu nên nếu dữ liệu không được chuẩn hóa, thuật toán có thể ưu tiên các đặc trưng có phạm vi lớn hơn và bỏ qua các đặc trưng có phạm vi nhỏ hơn, dẫn đến hiệu suất kém. Và công thức phổ biến nhất cho chuẩn hóa là:\n\n$$\n\\text{Normalized Value} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n$$\n\n```{r}\n#| warning: false\n#| message: false\n# Create a data frame with the adjusted sales data\nsales_data <- data.frame(\n  Date = dates,\n  Weekday = weekdays,\n  Product_A = product_a_sales,\n  Product_B = product_b_sales,\n  Product_C = product_c_sales\n)\n\n# Convert the sales data to a time series (ts) object for Product A\nproduct_a_ts <- ts(sales_data$Product_A, start = c(2024, 5), \n                   frequency = 365)\n                   \n\n# Normalzie data:\ntime_series_data<-scale(product_a_ts)\n\nlibrary(highcharter)\nhighchart() %>%\n  hc_add_series(data = as.numeric(time_series_data), type = \"line\", name = \"Sales of Product A\") %>%\n  hc_title(text = \"Normalized Time Series of Product A\") %>%\n  hc_xAxis(title = list(text = \"Date\")) %>%\n  hc_yAxis(title = list(text = \"Normalized Sales\")) %>%\n  hc_tooltip(shared = TRUE) %>%\n  hc_plotOptions(line = list(marker = list(enabled = FALSE)))\n```\n\n### Chia dữ liệu:\n\nVậy để *train data*, mình sẽ chia bộ dữ liệu thành 3 phần:\n\n-   *Training data*: dùng để huấn luyện và xây dựng mô hình.\n\n-   *Evaluating data*: đánh giá mô hình vừa huấn luyện.\n\n-   *Testing data*: dùng để đánh giá lại nếu muốn mô hình học lại dữ liệu\n\n```{r}\n#| warning: false\n#| message: false\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(dplyr)\n\n# Function to create supervised learning format from time series\ncreate_supervised_data <- function(series, n_in = 1, n_out = 1) {\n  series <- as.vector(series)  # Convert time series object to vector\n  data <- data.frame(series)\n  \n  # Use base R lag function for ts objects (lag() from stats package)\n  for (i in 1:n_in) {\n    data <- cbind(data, stats::lag(series, -i))\n  }\n  \n  colnames(data) <- c(paste0('t-', 1:n_in), 't+1')  # Correctly name columns\n  return(data)\n}\n\n# Prepare the data with 12 input lags and 1 output (next time step)\nsupervised_data <- create_supervised_data(time_series_data,\n                                          n_in = 12, \n                                          n_out = 1)\n\n# Remove NA rows created by lag function\nsupervised_data <- na.omit(supervised_data)\n\n# Step 2: Split data into training and test sets\ntrain_size <- round(0.7 * nrow(supervised_data))   # 70% for training\nval_size <- round(0.1 * nrow(supervised_data))     # 10% for validation\ntest_size <- nrow(supervised_data) - train_size - val_size  # 20% for testing\n\ntrain_data <- supervised_data[1:train_size, ]\nval_data <- supervised_data[(train_size + 1):(train_size + val_size), ]\ntest_data <- supervised_data[(train_size + val_size + 1):nrow(supervised_data), ]\n\n# Correct column selection\nx_train <- as.matrix(train_data[, 1:12])  # Input features (12 lags)\ny_train <- as.matrix(train_data[, 't+1'])  # Target output (next time step)\n\nx_val <- as.matrix(val_data[, 1:12])  # Input features for validation\ny_val <- as.matrix(val_data[, 't+1'])  # Actual output for validation\n\nx_test <- as.matrix(test_data[, 1:12])  # Input features for testing\ny_test <- as.matrix(test_data[, 't+1'])  # Actual output for testing\n\n\n## Plot the result:\nlibrary(xts)\nn<-quantile(sales_data$Date, \n            probs = c(0, 0.7, 0.8,1), \n            type = 1)\n\nm1<-sales_data %>% \n  filter(Date <= n[[2]])\nm2<-sales_data %>% \n  filter(Date <= n[[3]] & Date > n[[2]])\nm3<-sales_data %>% \n  filter(Date <= n[[4]] & Date > n[[3]])\n\ndemand_training<-xts(x=m1$Product_A,\n                     order.by=m1$Date)\ndemand_testing<-xts(x=m2$Product_A,\n                     order.by=m2$Date)\ndemand_forecasting<-xts(x=m3$Product_A,\n                     order.by=m3$Date)\n\nlibrary(dygraphs)\nlines<-cbind(demand_training,\n             demand_testing,\n             demand_forecasting)\ndygraph(lines,\n        main = \"Training and testing data\", \n        ylab = \"Quantity order (Unit: Millions)\") %>% \n  dySeries(\"demand_training\", label = \"Training data\") %>%\n  dySeries(\"demand_testing\", label = \"Testing data\") %>%\n  dySeries(\"demand_forecasting\", label = \"Forecasting data\") %>%\n  dyOptions(fillGraph = TRUE, fillAlpha = 0.4) %>% \n  dyRangeSelector(height = 20)\n```\n\n### Mô hình RNN:\n\nSau đó, ta sẽ bắt đầu *train model* bằng cách tạo thêm 12 cột giá trị là giá trị quá khứ của *demand*. Bạn sẽ bắt đầu định nghĩa mô hình gồm:\n\n-   *Input*: dùng hàm `layer_input(shape = input_shape)` với `input_shape` là số lượng *predictor*.\n\n-   *Layer*: là các hidden layer trong mô hình thêm vào bằng hàm `layer_dense(x, units = 64, activation = 'relu')` với đối số `units` thường là bội số của 32 như 32,64,256,...\n\n-   *Output*: dùng hàm `layer_dense(x, units = 1)` để định nghĩa là đầu ra chỉ có 1 giá trị.\n\n```{r}\n# Step 3: Build a simple transformer-like model\nRNN_model <- function(input_shape) {\n  inputs <- layer_input(shape = input_shape)\n\n  # Transformer Encoder Layer (simplified)\n  x <- inputs\n  x <- layer_dense(x, units = 64, activation = 'relu')  # Dense layer\n  x <- layer_dense(x, units = 32, activation = 'relu')  # Another dense layer\n\n  # Output layer\n  x <- layer_dense(x, units = 1)\n  \n  model <- keras_model(inputs, x)\n  return(model)\n}\n\n# Example input shape (12 time steps input per sample)\ninput_shape <- c(12)\n\nRNN_model <- RNN_model(input_shape)\n```\n\n```{r}\n#| warning: false\n#| message: false\n#| include: false\n# Step 4: Compile the model\nRNN_model %>% compile(\n  loss = 'mse',\n  optimizer = optimizer_adam(),\n  metrics = c('mae')\n)\n\n# Step 5: Train the model\nRNN_history <- RNN_model %>% fit(\n  x_train, \n  y_train,\n  epochs = 50, \n  batch_size = 32,\n  validation_data = list(x_val, y_val)\n)\n\nRNN_result <- RNN_model %>% \n    evaluate(x_test, y_test)\n```\n\nĐối với các mô hình truyền thống như *linear regression* thì bạn đã quen với thông số $R^2$ để đánh giá mô hình, còn với mô hình *Machine learning* thì dùng khái niệm *loss function - hàm mất mát*. Về khái niệm, *loss function* sẽ đo lường chênh lệch giữa *predicted* và *actual* trong bộ *training data* nên khi càng tăng `epochs` nghĩa là tăng số lần học lại dữ liệu thì *loss function* sẽ tính ra giá trị càng thấp. Như mô hình trên thì mình đặt đối số `loss = mse` nghĩa là sử dụng *Mean Squared Error* để tối ưu quy trình học của học máy. Công thức như sau:\n\n$$\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{pred}}(i) - y_{\\text{true}}(i))^2\n$$\n\nCòn đối số `metrics = c('mae')` nghĩa là tiêu chí khác để theo dõi và đánh giá mô hình. Vậy tại sao cần có 2 tham số đánh giá song song như vậy là vì như đã nói, nếu bạn càng tăng `epochs` thì giá trị *loss* càng thấp trong khi dùng `metrics` sẽ đưa ra đánh giá khách quan hơn về mô hình mà không phụ thuộc vào số lần `epochs`. Công thức như sau:\n\n$$\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{pred}}(i) - y_{\\text{true}}(i)|\n$$\n\nVậy khi chạy code, R sẽ return output như biểu đồ dưới đây là so sánh tham số của *mse* và *mae* giữa *training data* và *evaluating data*. Ý tưởng là đánh giá thử mô hình có dự đoán tốt không khi có dữ liệu mới vào.\n\nTiếp theo, ta sẽ dùng *test data* để đánh giá mô hình vừa xây dựng. Kết quả có vẻ khá tuyệt vì mô hình gần như theo sát được dữ liệu của *test data*.\n\n```{r}\n# Step 6: Make predictions\nRNN_forecast <- RNN_model %>% \n  predict(x_test)\n\n# Step 7: Combine predicted and observed\nplot_data <- data.frame(\n  time = c(min(m3$Date)-days(1),m3$Date),  # Time for the test set\n  actual = y_test,  # Actual values from the test set\n  forecast = RNN_forecast  # Forecasted values\n)\n\n# Step 8: Plot using Highcharts\nhighchart() %>%\n  hc_title(text = \"Time Series Forecasting with Highcharts\") %>%\n  hc_xAxis(\n    categories = plot_data$time,\n    title = list(text = \"Time\")\n  ) %>%\n  hc_yAxis(\n    title = list(text = \"Value\"),\n    plotLines = list(list(\n      value = 0,\n      width = 1,\n      color = \"gray\"\n    ))\n  ) %>%\n  hc_add_series(\n    name = \"Actual Data\",\n    data = plot_data$actual,\n    type = \"line\",\n    color = \"#1f77b4\"  # Blue color for actual data\n  ) %>%\n  hc_add_series(\n    name = \"Forecast\",\n    data = plot_data$forecast,\n    type = \"line\",\n    color = \"#ff7f0e\"  # Orange color for forecast data\n  ) %>%\n  hc_tooltip(\n    shared = TRUE,\n    crosshairs = TRUE\n  ) %>%\n  hc_legend(\n    enabled = TRUE\n  )\n```\n\n### Mô hình LSTM:\n\nTiếp theo, ta sẽ xây dựng thử mô hình *LSTM*. Mô hình LSTM thường bao gồm các lớp sau:\n\n-   Lớp LSTM: Đây là lớp chính, có thể có một hoặc nhiều lớp LSTM chồng lên nhau. Mỗi lớp LSTM có thể trả về toàn bộ chuỗi bằng `return_sequences = TRUE` hoặc chỉ trả về giá trị cuối cùng bằng `return_sequences = FALSE`.\n\n-   Lớp Dense: Sau khi thông tin được xử lý qua các lớp LSTM, nó sẽ được đưa qua các lớp Dense (lớp fully connected) để đưa ra dự đoán cuối cùng.\n\n-   Lớp Dropout (tùy chọn): Để tránh overfitting, có thể thêm lớp dropout để tắt ngẫu nhiên một số nơ-ron trong quá trình huấn luyện.\n\n```{r}\n#| warning: false\n#| message: false\n#| include: false\n# Step 3: Build an enhanced LSTM model\nLSTM_model <- function(input_shape) {\n  # Define input layer\n  inputs <- layer_input(shape = input_shape)\n  \n  # First LSTM layer (returns the full sequence for the next layer)\n  x <- layer_lstm(units = 50, return_sequences = TRUE, activation = 'tanh')(inputs)\n  \n  # Second LSTM layer (returns the full sequence, could also be 'return_sequences = FALSE' for output prediction)\n  x <- layer_lstm(units = 50, return_sequences = FALSE, activation = 'tanh')(x)\n  \n  # Output layer (prediction for the next time step)\n  outputs <- layer_dense(units = 1)(x)\n  \n  # Create the model\n  model <- keras_model(inputs, outputs)\n  \n  # Return the model\n  return(model)\n}\n\n\n# Update input shape for LSTM (timesteps = 12, features = 1)\ninput_shape <- c(12, 1)\n\n# Reshape the input data for LSTM (add a feature dimension)\nx_train <- array_reshape(x_train, dim = c(nrow(x_train), 12, 1))\nx_test <- array_reshape(x_test, dim = c(nrow(x_test), 12, 1))\nx_val <- array_reshape(x_val, dim = c(nrow(x_val), 12, 1))\n\n# Build the enhanced LSTM model\nLSTM_model <- LSTM_model(input_shape)\n\n# Step 4: Compile the model\nLSTM_model %>% compile(\n  loss = 'mse',\n  optimizer = optimizer_adam(),\n  metrics = c('mae')\n)\n\n# Step 5: Train the model\nLSTM_history <- LSTM_model %>% fit(\n  x_train, y_train,\n  epochs = 50, batch_size = 32,\n  validation_data = list(x_val, y_val)\n)\n\nLSTM_result <- LSTM_model %>% \n    evaluate(x_test, y_test)\n```\n\nVậy giờ ta sẽ so sánh với mô hình *RNN* trước với mô hình *LSTM* qua 2 thông số đã chọn *mse* và *mae*.\n\n```{r}\n#| warning: false\n#| message: false\n# Extract metrics into a data frame\nresults_df <- data.frame(\n  Model = c(\"RNN\", \"LSTM\"),\n  Metric = c(\"Loss\", \"metric\"),\n  MSE = c(RNN_result[[1]],RNN_result[[2]]),\n  MAE = c(LSTM_result[[1]], LSTM_result[[2]])\n)\n\nlibrary(gt)\n# Create a gt table\nresults_df %>%\n  gt() %>%\n  tab_header(\n    title = \"Model Performance Metrics\",\n    subtitle = \"Comparison of MSE and MAE for RNN and LSTM\"\n  ) %>%\n  fmt_number(\n    columns = vars(MSE, MAE),\n    decimals = 4\n  ) %>%\n  cols_label(\n    Model = \"Model Type\",\n    MSE = \"Mean Squared Error\",\n    MAE = \"Mean Absolute Error\"\n  ) %>%\n  tab_options(\n    table.font.size = 14,\n    heading.title.font.size = 16,\n    heading.subtitle.font.size = 14\n  )\n```\n\nKết quả cho thấy mô hình *RNN* truyền thống đưa ra kết quả tốt hơn *LSTM* mặc dù sai số của *LSTM* đều \\< 0.03 là không quá tệ nhưng tiêu chí vẫn là mô hình nào hiệu quả nhất.\n\n```{r}\nLSTM_forecast <- LSTM_model %>% \n  predict(x_test)\n\ncompare<-data.frame(Date = c(min(m3$Date)-days(1),m3$Date),\n                    LSTM = round(LSTM_forecast - y_test,3),\n                    RNN = round(RNN_forecast - y_test,3)\n)\n\n# Create the highchart plot\nhighchart() %>%\n  hc_chart(type = \"line\") %>%\n  hc_title(text = \"Residual Comparison: LSTM vs RNN\") %>%\n  hc_xAxis(\n    categories = compare$Date,\n    title = list(text = \"Date\")\n  ) %>%\n  hc_yAxis(\n    title = list(text = \"Residuals\"),\n    plotLines = list(\n      list(value = 0, color = \"gray\", width = 1, dashStyle = \"Dash\")\n    )\n  ) %>%\n  hc_add_series(\n    name = \"LSTM Residuals\",\n    data = compare$LSTM,\n    color = \"#1f77b4\"\n  ) %>%\n  hc_add_series(\n    name = \"RNN Residuals\",\n    data = compare$RNN,\n    color = \"#ff7f0e\"\n  ) %>%\n  hc_tooltip(shared = TRUE) %>%\n  hc_legend(enabled = TRUE)\n```\n\n\n## Kết luận:\n\nNhư vậy, chúng ta đã được học về thuật toán Genetic và mô hình MILP cũng như cách thực hiện trong Rstudio.\n\nNếu bạn có câu hỏi hay thắc mắc nào, đừng ngần ngại liên hệ với mình qua Gmail. Bên cạnh đó, nếu bạn muốn xem lại các bài viết trước đây của mình, hãy nhấn vào hai nút dưới đây để truy cập trang **Rpubs** hoặc mã nguồn trên **Github**. Rất vui được đồng hành cùng bạn, hẹn gặp lại! 😄😄😄\n\n```{=html}\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Contact Me</title>\n    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css\">\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/simple-icons@v6.0.0/svgs/rstudio.svg\">\n    <style>\n        body { font-family: Arial, sans-serif; background-color: $secondary-color; }\n        .container { max-width: 400px; margin: auto; padding: 20px; background: white; border-radius: 8px; box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1); }\n        label { display: block; margin: 10px 0 5px; }\n        input[type=\"email\"] { width: 100%; padding: 10px; margin-bottom: 15px; border: 1px solid #ccc; border-radius: 4px; }\n        .github-button, .rpubs-button { margin-top: 20px; text-align: center; }\n        .github-button button, .rpubs-button button { background-color: #333; color: white; border: none; padding: 10px; cursor: pointer; border-radius: 4px; width: 100%; }\n        .github-button button:hover, .rpubs-button button:hover { background-color: #555; }\n        .rpubs-button button { background-color: #75AADB; }\n        .rpubs-button button:hover { background-color: #5A9BC2; }\n        .rpubs-icon { margin-right: 5px; width: 20px; vertical-align: middle; filter: brightness(0) invert(1); }\n        .error-message { color: red; font-size: 0.9em; margin-top: 5px; }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h2>Contact Me</h2>\n        <form id=\"emailForm\">\n            <label for=\"email\">Your Email:</label>\n            <input type=\"email\" id=\"email\" name=\"email\" required aria-label=\"Email Address\">\n            <div class=\"error-message\" id=\"error-message\" style=\"display: none;\">Please enter a valid email address.</div>\n            <button type=\"submit\">Send Email</button>\n        </form>\n        <div class=\"github-button\">\n            <button>\n                <a href=\"https://github.com/Loccx78vn/Time_series_forcasting\" target=\"_blank\" style=\"color: white; text-decoration: none;\">\n                    <i class=\"fab fa-github\"></i> View Code on GitHub\n                </a>\n            </button>\n        </div>\n        <div class=\"rpubs-button\">\n            <button>\n                <a href=\"https://rpubs.com/loccx\" target=\"_blank\" style=\"color: white; text-decoration: none;\">\n                    <img src=\"https://cdn.jsdelivr.net/npm/simple-icons@v6.0.0/icons/rstudio.svg\" alt=\"RStudio icon\" class=\"rpubs-icon\"> Visit my RPubs\n                </a>\n            </button>\n        </div>\n    </div>\n\n    <script>\n        document.getElementById('emailForm').addEventListener('submit', function(event) {\n            event.preventDefault(); // Prevent default form submission\n            const emailInput = document.getElementById('email');\n            const email = emailInput.value;\n            const errorMessage = document.getElementById('error-message');\n\n            // Simple email validation regex\n            const emailPattern = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n\n            if (emailPattern.test(email)) {\n                errorMessage.style.display = 'none'; // Hide error message\n                const yourEmail = 'loccaoxuan103@gmail.com'; // Your email\n                const gmailLink = `https://mail.google.com/mail/?view=cm&fs=1&to=${yourEmail}&su=Help%20Request%20from%20${encodeURIComponent(email)}`;\n                window.open(gmailLink, '_blank'); // Open in new tab\n            } else {\n                errorMessage.style.display = 'block'; // Show error message\n            }\n        });\n    </script>\n</body>\n</html>\n```"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":true,"output-file":"torch.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.30","editor":"source","theme":{"light":"theme-light.scss","dark":"theme-dark.scss"},"title":"RNN and LSTM model","subtitle":"Việt Nam, 2024","categories":["Machine Learning","Forecasting"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}