---
title: "RNN and LSTM model"
subtitle: "Viá»‡t Nam, 2024"
categories: ["Machine Learning", "Forecasting"]
format: 
  html:
    code-fold: true
    code-tools: true
number-sections: true
---

á» Ä‘Ã¢y ta sáº½ há»c vá» mÃ´ hÃ¬nh machine learning Ä‘Æ°á»£c á»©ng dá»¥ng nhiá»u nháº¥t trong viá»‡c phÃ¢n tÃ­ch dá»¯ liá»‡u thá»i gian lÃ  *RNN* vÃ  *LSTM*.

## MÃ´ hÃ¬nh RNN:

### Äá»‹nh nghÄ©a:

Äiá»ƒm chung lÃ  cáº£ hai mÃ´ hÃ¬nh Ä‘á»u thuá»™c phÃ¢n lá»›p *Deep learning* - nghÄ©a lÃ  há»c mÃ¡y sÃ¢u vá»›i Ä‘áº·c Ä‘iá»ƒm chung lÃ  phÃ¢n chia dá»¯ liá»‡u thÃ nh nhiá»u lá»›p vÃ  báº¯t Ä‘áº§u "há»c" dáº§n qua tá»«ng lá»›p Ä‘á»ƒ Ä‘Æ°a ra káº¿t quáº£ cuá»‘i cÃ¹ng. á» hÃ¬nh dÆ°á»›i Ä‘Ã¢y, $X_o$ Ä‘áº¡i diá»‡n cho dá»¯ liá»‡u Ä‘áº§u vÃ o, $h_t$ lÃ  output Ä‘áº§u ra cá»§a tá»«ng step vÃ  $A$ lÃ  nhá»¯ng gÃ¬ Ä‘Ã£ "há»c" Ä‘Æ°á»£c táº¡i step Ä‘Ã³ vÃ  Ä‘Æ°á»£c truyá»n cho step tiáº¿p theo. Trong tÃ i liá»‡u chuáº©n thÃ¬ há» thÆ°á»ng kÃ­ hiá»‡u lÃ  $X_t$, $Y_t$, $h_{t-1}$.

```{=html}
<div style="text-align: center; margin-bottom: 20px;">
  <img src="img/RNN.png" style="max-width: 100%; height: auto; display: block; margin: 0 auto;">
  
  <!-- Picture Name -->
  <div style="text-align: left; margin-top: 10px;">
    HÃ¬nh 1: Minh há»a vá» sá»± phÃ¢n chia dá»¯ liá»‡u thÃ nh nhiá»u lá»›p
  </div>
  
  <!-- Source Link -->
  <div style="text-align: right; font-style: italic; margin-top: 5px;">
    Source: <a href="https://dominhhai.github.io/vi/2017/10/what-is-lstm/" target="_blank">Link to Image</a>
  </div>
</div>
```
Khi nhÃ¬n hÃ¬nh thÃ¬ báº¡n cÃ³ thá»ƒ bá»‘i rá»‘i chÆ°a hiá»ƒu cÃ¡c kÃ­ tá»± vÃ  hÃ¬nh áº£nh thÃ¬ báº¡n cÃ³ thá»ƒ tÆ°á»Ÿng tÆ°á»£ng **há»c mÃ¡y** nhÆ° 1 Ä‘á»©a tráº» vÃ  Ä‘á»ƒ nÃ³ cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c cÃ¢u: "HÃ´m nay con Ä‘i há»c" thÃ¬ nÃ³ pháº£i há»c tá»«ng chá»¯ cÃ¡i nhÆ°: a,b,c,... trÆ°á»›c rÃ²i má»›i ghÃ©p thÃ nh tá»« Ä‘Æ¡n nhÆ°: "HÃ´m","Nay",... rá»“i ghÃ©p thÃ nh cÃ¢u trÃªn.

Váº­y giáº£ sá»­ nhÆ° hÃ´m nay há»c Ä‘Æ°á»£c tá»« "HÃ´m" thÃ¬ nÃ³ sáº½ báº¯t Ä‘áº§u ghi nhá»› tá»« Ä‘Ã£ há»c vÃ o trong $A$. Náº¿u sau nÃ y ta cáº§n **há»c mÃ¡y** hiá»ƒu cÃ¢u "HÃ´m sau con Ä‘i chÆ¡i" thÃ¬ tá»‘c Ä‘á»™ há»c cá»§a **há»c mÃ¡y** sáº½ nhanh lÃªn vÃ¬ thay vÃ¬ nÃ³ pháº£i há»c 5 chá»¯ Ä‘Æ¡n nhÆ° thÃ´ng thÆ°á»ng thÃ¬ nÃ³ chá»‰ cáº§n há»c 4 chá»¯ cÃ²n láº¡i trá»« chá»¯ "hÃ´m". Váº­y báº¡n Ä‘Ã£ hiá»ƒu Ã½ tÆ°á»Ÿng ná»n táº£ng cá»§a *RNN* rá»“i ha!

### NguyÃªn lÃ­ hoáº¡t Ä‘á»™ng:

Äáº§u tiÃªn, *RNN* sáº½ tÃ­nh toÃ¡n *hidden state* lÃ  $h_t$ vá»›i cÃ´ng thá»©c lÃ :

$$
   \mathbf{h}_t = \text{activation}(\mathbf{W}_\text{hh} \mathbf{h}_{t-1} + \mathbf{W}_\text{xh} \mathbf{x}_t + \mathbf{b}_\text{h})
$$ Sau Ä‘Ã³, $h_t$ sáº½ Ä‘Æ°á»£c lÃ m input cho cÃ¡c *state* sau vÃ  dá»±a vÃ o Ä‘Ã³ Ä‘á»ƒ tÃ­nh output vá»›i cÃ´ng thá»©c lÃ :

$$
y_t = W_y \cdot h_t + b_y
$$

**VÃ­ dá»¥:** MÃ¬nh muá»‘n dá»± Ä‘oÃ¡n hÃ nh Ä‘á»™ng trong cÃ¢u nÃ³i "I am reading book" báº±ng mÃ´ hÃ¬nh *RNN* nhÆ° sau:

-   *BÆ°á»›c 1*: Chuyá»ƒn Ä‘á»•i thÃ nh dáº¡ng sá»‘ báº±ng *embedding layer*:

MÃ¬nh sáº½ gÃ¡n tá»«ng tá»« Ä‘Æ¡n sang dáº¡ng sá»‘ nhÆ°:

-   "I" -\> $x_1$

-   "am" -\> $x_2$

-   "reading" -\> $x_3$

-   "book" -\> $x_4$

-   *BÆ°á»›c 2:* ThÃªm hidden layer vÃ  báº¯t Ä‘áº§u tÃ­nh toÃ¡n:

Cho input: "I" $$
   h_1 = \tanh(W_x \cdot x_1 + W_h \cdot h_0 + b)
$$

Cho input: "am" $$
   h_2 = \tanh(W_x \cdot x_2 + W_h \cdot h_1 + b)
$$ Cho input: "reading" $$
   h_3 = \tanh(W_x \cdot x_3 + W_h \cdot h_2 + b)
$$

Cho input: "book" $$
   h_4 = \tanh(W_x \cdot x_4 + W_h \cdot h_3 + b)
$$

-   *BÆ°á»›c 3:* TÃ­nh toÃ¡n output: DÃ¹ng hÃ m activation **softmax** Ä‘á»ƒ phÃ¢n lá»›p theo xÃ¡c suáº¥t.

$$
\hat{y} = \text{softmax}(W_y \cdot h_4 + b_y)
$$ Náº¿u muá»‘n hiá»ƒu thÃªm vá» cÃ¡ch hoáº¡t Ä‘á»™ng *RNN*, báº¡n cÃ³ thá»ƒ tham kháº£o link nÃ y: [Recurrent Neural Network: Tá»« RNN Ä‘áº¿n LSTM](https://viblo.asia/p/recurrent-neural-network-tu-rnn-den-lstm-gGJ597z1ZX2).

### Váº¥n Ä‘á» lá»›n cá»§a RNN:

*RNN* cÃ³ 1 váº¥n Ä‘á» lá»›n lÃ  *Vanishing Gradient* nghÄ©a lÃ  mÃ´ hÃ¬nh sáº½ khÃ´ng cÃ²n "há»c" thÃªm Ä‘Æ°á»£c ná»¯a cho dÃ¹ tÄƒng sá»‘ `epochs`. NguyÃªn nhÃ¢n vÃ¬ sao nhÆ° váº­y thÃ¬ báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n chá»©ng minh cá»§a [anh Tuáº¥n](https://nttuan8.com/bai-14-long-short-term-memory-lstm/).

```{=html}
<div style="text-align: center; margin-bottom: 20px;">
  <img src="img/vanishing.png" style="max-width: 100%; height: auto; display: block; margin: 0 auto;">
  
  <!-- Picture Name -->
  <div style="text-align: left; margin-top: 10px;">
    HÃ¬nh 2: Vanishing Gradient Problem
  </div>
  
  <!-- Source Link -->
  <div style="text-align: right; font-style: italic; margin-top: 5px;">
    Source: <a href="https://www.superdatascience.com/blogs/recurrent-neural-networks-rnn-the-vanishing-gradient-problem" target="_blank">Link to Image</a>
  </div>
</div>
```
Váº¥n Ä‘á» nÃ y sáº½ lÃ m network khÃ³ update *weight* dáº«n tá»›i thá»i gian há»c lÃ¢u vÃ  khÃ³ Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c output. Báº¡n cÃ³ thá»ƒ hiá»ƒu Ä‘Æ¡n giáº£n nhÆ° viá»‡c báº¡n há»c liÃªn tá»¥c dáº«n tá»›i quÃ¡ táº£i vÃ  *RNN* cÅ©ng khÃ´ng nhÆ° váº­y. Do Ä‘Ã³, *RNN* chá»‰ há»c cÃ¡c thÃ´ng tin tá»« *state* gáº§n vÃ  Ä‘Ã³ lÃ  lÃ­ do ra Ä‘á»i *LSTM - Long short term memory.*

::: callout-warning
<u>LÆ°u Ã½</u>: Äiá»u nÃ y khÃ´ng cÃ³ nghÄ©a *LSTM* luÃ´n tá»‘t hÆ¡n *RNN* vÃ¬ cÃ³ nhá»¯ng bÃ i toÃ¡n vá»›i Ä‘áº§u vÃ o Ä‘Æ¡n giáº£n thÃ¬ mÃ´ hÃ¬nh chá»‰ cáº§n há»c cÃ¡c step Ä‘áº§u lÃ  Ä‘Ã£ "há»c" Ä‘áº§y Ä‘á»§ thÃ´ng tin cáº§n thiáº¿t. MÃ´ hÃ¬nh *LSTM* phá»• biáº¿n vá»›i cÃ¡c bÃ i toÃ¡n phá»©c táº¡p nhÆ° tá»± Ä‘á»™ng dá»‹ch ngÃ´n ngá»¯, ghi chÃ©p láº¡i theo giá»ng nÃ³i...
:::

### MÃ´ hÃ¬nh LSTM:

CÃ³ thá»ƒ xem mÃ´ hÃ¬nh *LSTM* nhÆ° biáº¿n thá»ƒ cá»§a *RNN*. Vá» cáº¥u trÃºc, *LSTM* phá»©c táº¡p hÆ¡n *RNN*:

```{=html}
<div style="text-align: center; margin-bottom: 20px;">
  <img src="img/compare.png" style="max-width: 100%; height: auto; display: block; margin: 0 auto;">
  
  <!-- Picture Name -->
  <div style="text-align: left; margin-top: 10px;">
    HÃ¬nh 3: So sÃ¡nh mÃ´ hÃ¬nh RNN vÃ  LSTM
  </div>
  
  <!-- Source Link -->
  <div style="text-align: right; font-style: italic; margin-top: 5px;">
    Source: <a href="https://dominhhai.github.io/vi/2017/10/what-is-lstm/" target="_blank">Link to Image</a>
  </div>
</div>
```
Cáº¥u trÃºc cÆ¡ báº£n gá»“m:

-   Cá»•ng quÃªn (Forget Gate): cÃ³ tÃ¡c dá»¥ng quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o cáº§n bá»‹ quÃªn trong tráº¡ng thÃ¡i Ã´ nhá»›.

-   Cá»•ng nháº­p (Input Gate): XÃ¡c Ä‘á»‹nh thÃ´ng tin nÃ o cáº§n Ä‘Æ°á»£c ghi vÃ o tráº¡ng thÃ¡i Ã´ nhá»›.

-   Cá»•ng xuáº¥t (Output Gate): Quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o sáº½ Ä‘Æ°á»£c xuáº¥t ra tá»« tráº¡ng thÃ¡i Ã´ nhá»› Ä‘á»ƒ áº£nh hÆ°á»Ÿng Ä‘áº¿n dá»± Ä‘oÃ¡n tiáº¿p theo.

Báº¡n cÃ³ thá»ƒ kham kháº£o thÃªm bÃ i viáº¿t cá»§a [dominhhai](https://dominhhai.github.io/vi/2017/10/what-is-lstm/) vá» cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a *RNN* vÃ  *LSTM* Ä‘á»ƒ hiá»ƒu thÃªm.

Tiáº¿p theo, ta sáº½ báº¯t Ä‘áº§u xÃ¢y dá»±ng thá»­ mÃ´ hÃ¬nh trong R.

## XÃ¢y dá»±ng mÃ´ hÃ¬nh:

### Load dá»¯ liá»‡u:

Äáº§u tiÃªn ta sáº½ load dá»¯ liá»‡u láº¡i nhÆ° trÆ°á»›c. á» Ä‘Ã¢y, Ä‘á»ƒ Ä‘Æ¡n giáº£n, mÃ¬nh chá»‰ xÃ¢y dá»±ng mÃ´ hÃ¬nh cho *product A* thÃ´i.

```{r}
#| include: false
#| warning: false
#| message: false
pacman::p_load(
janitor,
tidyverse,
dplyr,
tidyr,
magrittr,
ggplot2)
```

```{r}
#| include: false
# Set the start and end date for the 6-month period
start_date <- as.Date("2024-05-01")
end_date <- as.Date("2024-10-31")

# Generate date range
dates <- seq.Date(start_date, 
                  end_date, 
                  by = "day")

# Set a random seed for reproducibility
set.seed(42)

# Create a vector of weekdays for each date
weekdays <- weekdays(dates)

# Simulate sales data for Product A, B, and C based on weekday patterns
product_a_sales <- sample(5:50, length(dates), replace = TRUE)
product_b_sales <- sample(3:40, length(dates), replace = TRUE)
product_c_sales <- sample(2:30, length(dates), replace = TRUE)

# Adjust sales based on the weekday
for (i in 1:length(dates)) {
  if (weekdays[i] == "Wednesday" | weekdays[i] == "Saturday") {
    # High demand for Product A and B on Wednesday and Saturday
    product_a_sales[i] <- sample(40:70, 1)
    product_b_sales[i] <- sample(30:60, 1)
  } else if (weekdays[i] == "Monday" | weekdays[i] == "Tuesday") {
    # High demand for Product C on Monday and Tuesday
    product_c_sales[i] <- sample(20:40, 1)
  }
}

# Create a data frame with the adjusted sales data
sales_data <- data.frame(
  Date = dates,
  Weekday = weekdays,
  Product_A = product_a_sales,
  Product_B = product_b_sales,
  Product_C = product_c_sales
)
```

Giáº£ sá»­ cÃ´ng ty mÃ¬nh Ä‘ang kinh doanh 3 loáº¡i máº·t hÃ ng *product A*,*product B*,*product C* vÃ  Ä‘Ã¢y lÃ  biá»ƒu Ä‘á»“ thá»ƒ hiá»‡n nhu cáº§u cá»§a cáº£ 3 máº·t hÃ ng tá»« thÃ¡ng 5 tá»›i thÃ¡ng 10.

```{r}
#| warning: false
#| message: false
library(highcharter)
sales_data |> 
  select(-Weekday) |> 
  pivot_longer(cols = c(Product_A, Product_B, Product_C),
               names_to = "Product",
               values_to = "Sales") |> 
  hchart("line", hcaes(x = Date, y = Sales, group = Product))
```

Náº¿u ta phÃ¢n tich sÃ¢u vá» nhu cáº§u cá»§a tá»«ng máº·t hÃ ng theo thá»© trong tuáº§n, ta sáº½ tháº¥y ráº±ng máº·t hÃ ng A, B thÃ¬ bÃ¡n cháº¡y vÃ o thá»© 4 vÃ  thá»© 7, cÃ²n máº·t hÃ ng C thÃ¬ bÃ¡n cháº¡y vÃ o thá»© 2 vÃ  thá»© 3.

::: panel-tabset
##### Product A:

```{r}
#| warning: false
#| message: false
#| echo: false
mA<-sales_data |> 
  select(Date, 
         Weekday,
         Product_A)  

# Ensure 'Weekday' is a factor with the correct order
mA$Weekday <- factor(mA$Weekday, 
                     levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))

hcboxplot(
    x = mA$Product_A,
    var = mA$Weekday,
    name = "Weekday sales") |> 
  hc_title(text = "Comparing sales data between weekday") |> 
  hc_yAxis(title = list(text = "No.product")) |> 
  hc_chart(type = "column")
```

##### Product B:

```{r}
#| warning: false
#| message: false
#| echo: false
mB<-sales_data |> 
  select(Date, 
         Weekday,
         Product_B)  

# Ensure 'Weekday' is a factor with the correct order
mA$Weekday <- factor(mB$Weekday, 
                     levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))

hcboxplot(
    x = mB$Product_B,
    var = mB$Weekday,
    name = "Weekday sales") |> 
  hc_title(text = "Comparing sales data between weekday") |> 
  hc_yAxis(title = list(text = "No.product")) |> 
   hc_chart(type = "column")
```

##### Product C:

```{r}
#| warning: false
#| message: false
#| echo: false
mC<-sales_data |> 
  select(Date, 
         Weekday,
         Product_C)  

# Ensure 'Weekday' is a factor with the correct order
mA$Weekday <- factor(mC$Weekday, 
                     levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))

hcboxplot(
    x = mC$Product_C,
    var = mC$Weekday,
    name = "Weekday sales") |> 
  hc_title(text = "Comparing sales data between weekday") |> 
  hc_yAxis(title = list(text = "No.product")) |> 
   hc_chart(type = "column")
```
:::

```{r}
#| warning: false
#| message: false
#| include: false
library(tidyverse)
# Set the start and end date for the 6-month period
start_date <- as.Date("2024-05-01")
end_date <- as.Date("2024-10-31")

# Generate date range
dates <- seq.Date(start_date, 
                  end_date, 
                  by = "day")

# Set a random seed for reproducibility
set.seed(42)

# Create a vector of weekdays for each date
weekdays <- weekdays(dates)

# Simulate sales data for Product A, B, and C based on weekday patterns
product_a_sales <- sample(5:50, length(dates), replace = TRUE)
product_b_sales <- sample(3:40, length(dates), replace = TRUE)
product_c_sales <- sample(2:30, length(dates), replace = TRUE)

# Adjust sales based on the weekday
for (i in 1:length(dates)) {
  if (weekdays[i] == "Wednesday" | weekdays[i] == "Saturday") {
    # High demand for Product A and B on Wednesday and Saturday
    product_a_sales[i] <- sample(40:70, 1)
    product_b_sales[i] <- sample(30:60, 1)
  } else if (weekdays[i] == "Monday" | weekdays[i] == "Tuesday") {
    # High demand for Product C on Monday and Tuesday
    product_c_sales[i] <- sample(20:40, 1)
  }
}
```

ThÃ´ng thÆ°á»ng dá»¯ liá»‡u Ä‘á»ƒ *train model* trong *machine learning* thÆ°á»ng cáº§n tráº£i qua bÆ°á»›c *normalize data* nghÄ©a lÃ  Ä‘Æ°a táº¥t cáº£ dá»¯ liá»‡u vá» chung 1 thÆ°á»›c Ä‘o vÃ  pháº¡m vi. NguyÃªn do vÃ¬ Ä‘iá»u nÃ y giÃºp nhiá»u thuáº­t toÃ¡n há»c mÃ¡y dá»… dÃ ng há»™i tá»¥ hÆ¡n. VÃ­ dá»¥, cÃ¡c thuáº­t toÃ¡n nhÆ° *k-Nearest Neighbors (KNN)* vÃ  *Support Vector Machines (SVM)* ráº¥t nháº¡y cáº£m vá»›i khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u nÃªn náº¿u dá»¯ liá»‡u khÃ´ng Ä‘Æ°á»£c chuáº©n hÃ³a, thuáº­t toÃ¡n cÃ³ thá»ƒ Æ°u tiÃªn cÃ¡c Ä‘áº·c trÆ°ng cÃ³ pháº¡m vi lá»›n hÆ¡n vÃ  bá» qua cÃ¡c Ä‘áº·c trÆ°ng cÃ³ pháº¡m vi nhá» hÆ¡n, dáº«n Ä‘áº¿n hiá»‡u suáº¥t kÃ©m. VÃ  cÃ´ng thá»©c phá»• biáº¿n nháº¥t cho chuáº©n hÃ³a lÃ :

$$
\text{Normalized Value} = \frac{x - \min(x)}{\max(x) - \min(x)}
$$

```{r}
#| warning: false
#| message: false
# Create a data frame with the adjusted sales data
sales_data <- data.frame(
  Date = dates,
  Weekday = weekdays,
  Product_A = product_a_sales,
  Product_B = product_b_sales,
  Product_C = product_c_sales
)

# Convert the sales data to a time series (ts) object for Product A
product_a_ts <- ts(sales_data$Product_A, start = c(2024, 5), 
                   frequency = 365)
                   

# Normalzie data:
time_series_data<-scale(product_a_ts)

library(highcharter)
highchart() %>%
  hc_add_series(data = as.numeric(time_series_data), type = "line", name = "Sales of Product A") %>%
  hc_title(text = "Normalized Time Series of Product A") %>%
  hc_xAxis(title = list(text = "Date")) %>%
  hc_yAxis(title = list(text = "Normalized Sales")) %>%
  hc_tooltip(shared = TRUE) %>%
  hc_plotOptions(line = list(marker = list(enabled = FALSE)))
```

### Chia dá»¯ liá»‡u:

Váº­y Ä‘á»ƒ *train data*, mÃ¬nh sáº½ chia bá»™ dá»¯ liá»‡u thÃ nh 3 pháº§n:

-   *Training data*: dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n vÃ  xÃ¢y dá»±ng mÃ´ hÃ¬nh.

-   *Evaluating data*: Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh vá»«a huáº¥n luyá»‡n.

-   *Testing data*: dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ láº¡i náº¿u muá»‘n mÃ´ hÃ¬nh há»c láº¡i dá»¯ liá»‡u

```{r}
#| warning: false
#| message: false
library(keras)
library(tensorflow)
library(dplyr)

# Function to create supervised learning format from time series
create_supervised_data <- function(series, n_in = 1, n_out = 1) {
  series <- as.vector(series)  # Convert time series object to vector
  data <- data.frame(series)
  
  # Use base R lag function for ts objects (lag() from stats package)
  for (i in 1:n_in) {
    data <- cbind(data, stats::lag(series, -i))
  }
  
  colnames(data) <- c(paste0('t-', 1:n_in), 't+1')  # Correctly name columns
  return(data)
}

# Prepare the data with 12 input lags and 1 output (next time step)
supervised_data <- create_supervised_data(time_series_data,
                                          n_in = 12, 
                                          n_out = 1)

# Remove NA rows created by lag function
supervised_data <- na.omit(supervised_data)

# Step 2: Split data into training and test sets
train_size <- round(0.7 * nrow(supervised_data))   # 70% for training
val_size <- round(0.1 * nrow(supervised_data))     # 10% for validation
test_size <- nrow(supervised_data) - train_size - val_size  # 20% for testing

train_data <- supervised_data[1:train_size, ]
val_data <- supervised_data[(train_size + 1):(train_size + val_size), ]
test_data <- supervised_data[(train_size + val_size + 1):nrow(supervised_data), ]

# Correct column selection
x_train <- as.matrix(train_data[, 1:12])  # Input features (12 lags)
y_train <- as.matrix(train_data[, 't+1'])  # Target output (next time step)

x_val <- as.matrix(val_data[, 1:12])  # Input features for validation
y_val <- as.matrix(val_data[, 't+1'])  # Actual output for validation

x_test <- as.matrix(test_data[, 1:12])  # Input features for testing
y_test <- as.matrix(test_data[, 't+1'])  # Actual output for testing


## Plot the result:
library(xts)
n<-quantile(sales_data$Date, 
            probs = c(0, 0.7, 0.8,1), 
            type = 1)

m1<-sales_data %>% 
  filter(Date <= n[[2]])
m2<-sales_data %>% 
  filter(Date <= n[[3]] & Date > n[[2]])
m3<-sales_data %>% 
  filter(Date <= n[[4]] & Date > n[[3]])

demand_training<-xts(x=m1$Product_A,
                     order.by=m1$Date)
demand_testing<-xts(x=m2$Product_A,
                     order.by=m2$Date)
demand_forecasting<-xts(x=m3$Product_A,
                     order.by=m3$Date)

library(dygraphs)
lines<-cbind(demand_training,
             demand_testing,
             demand_forecasting)
dygraph(lines,
        main = "Training and testing data", 
        ylab = "Quantity order (Unit: Millions)") %>% 
  dySeries("demand_training", label = "Training data") %>%
  dySeries("demand_testing", label = "Testing data") %>%
  dySeries("demand_forecasting", label = "Forecasting data") %>%
  dyOptions(fillGraph = TRUE, fillAlpha = 0.4) %>% 
  dyRangeSelector(height = 20)
```

### MÃ´ hÃ¬nh RNN:

Sau Ä‘Ã³, ta sáº½ báº¯t Ä‘áº§u *train model* báº±ng cÃ¡ch táº¡o thÃªm 12 cá»™t giÃ¡ trá»‹ lÃ  giÃ¡ trá»‹ quÃ¡ khá»© cá»§a *demand*. Báº¡n sáº½ báº¯t Ä‘áº§u Ä‘á»‹nh nghÄ©a mÃ´ hÃ¬nh gá»“m:

-   *Input*: dÃ¹ng hÃ m `layer_input(shape = input_shape)` vá»›i `input_shape` lÃ  sá»‘ lÆ°á»£ng *predictor*.

-   *Layer*: lÃ  cÃ¡c hidden layer trong mÃ´ hÃ¬nh thÃªm vÃ o báº±ng hÃ m `layer_dense(x, units = 64, activation = 'relu')` vá»›i Ä‘á»‘i sá»‘ `units` thÆ°á»ng lÃ  bá»™i sá»‘ cá»§a 32 nhÆ° 32,64,256,...

-   *Output*: dÃ¹ng hÃ m `layer_dense(x, units = 1)` Ä‘á»ƒ Ä‘á»‹nh nghÄ©a lÃ  Ä‘áº§u ra chá»‰ cÃ³ 1 giÃ¡ trá»‹.

```{r}
# Step 3: Build a simple transformer-like model
RNN_model <- function(input_shape) {
  inputs <- layer_input(shape = input_shape)

  # Transformer Encoder Layer (simplified)
  x <- inputs
  x <- layer_dense(x, units = 64, activation = 'relu')  # Dense layer
  x <- layer_dense(x, units = 32, activation = 'relu')  # Another dense layer

  # Output layer
  x <- layer_dense(x, units = 1)
  
  model <- keras_model(inputs, x)
  return(model)
}

# Example input shape (12 time steps input per sample)
input_shape <- c(12)

RNN_model <- RNN_model(input_shape)
```

```{r}
#| warning: false
#| message: false
#| include: false
# Step 4: Compile the model
RNN_model %>% compile(
  loss = 'mse',
  optimizer = optimizer_adam(),
  metrics = c('mae')
)

# Step 5: Train the model
RNN_history <- RNN_model %>% fit(
  x_train, 
  y_train,
  epochs = 50, 
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

RNN_result <- RNN_model %>% 
    evaluate(x_test, y_test)
```

Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh truyá»n thá»‘ng nhÆ° *linear regression* thÃ¬ báº¡n Ä‘Ã£ quen vá»›i thÃ´ng sá»‘ $R^2$ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh, cÃ²n vá»›i mÃ´ hÃ¬nh *Machine learning* thÃ¬ dÃ¹ng khÃ¡i niá»‡m *loss function - hÃ m máº¥t mÃ¡t*. Vá» khÃ¡i niá»‡m, *loss function* sáº½ Ä‘o lÆ°á»ng chÃªnh lá»‡ch giá»¯a *predicted* vÃ  *actual* trong bá»™ *training data* nÃªn khi cÃ ng tÄƒng `epochs` nghÄ©a lÃ  tÄƒng sá»‘ láº§n há»c láº¡i dá»¯ liá»‡u thÃ¬ *loss function* sáº½ tÃ­nh ra giÃ¡ trá»‹ cÃ ng tháº¥p. NhÆ° mÃ´ hÃ¬nh trÃªn thÃ¬ mÃ¬nh Ä‘áº·t Ä‘á»‘i sá»‘ `loss = mse` nghÄ©a lÃ  sá»­ dá»¥ng *Mean Squared Error* Ä‘á»ƒ tá»‘i Æ°u quy trÃ¬nh há»c cá»§a há»c mÃ¡y. CÃ´ng thá»©c nhÆ° sau:

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_{\text{pred}}(i) - y_{\text{true}}(i))^2
$$

CÃ²n Ä‘á»‘i sá»‘ `metrics = c('mae')` nghÄ©a lÃ  tiÃªu chÃ­ khÃ¡c Ä‘á»ƒ theo dÃµi vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh. Váº­y táº¡i sao cáº§n cÃ³ 2 tham sá»‘ Ä‘Ã¡nh giÃ¡ song song nhÆ° váº­y lÃ  vÃ¬ nhÆ° Ä‘Ã£ nÃ³i, náº¿u báº¡n cÃ ng tÄƒng `epochs` thÃ¬ giÃ¡ trá»‹ *loss* cÃ ng tháº¥p trong khi dÃ¹ng `metrics` sáº½ Ä‘Æ°a ra Ä‘Ã¡nh giÃ¡ khÃ¡ch quan hÆ¡n vá» mÃ´ hÃ¬nh mÃ  khÃ´ng phá»¥ thuá»™c vÃ o sá»‘ láº§n `epochs`. CÃ´ng thá»©c nhÆ° sau:

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_{\text{pred}}(i) - y_{\text{true}}(i)|
$$

Váº­y khi cháº¡y code, R sáº½ return output nhÆ° biá»ƒu Ä‘á»“ dÆ°á»›i Ä‘Ã¢y lÃ  so sÃ¡nh tham sá»‘ cá»§a *mse* vÃ  *mae* giá»¯a *training data* vÃ  *evaluating data*. Ã tÆ°á»Ÿng lÃ  Ä‘Ã¡nh giÃ¡ thá»­ mÃ´ hÃ¬nh cÃ³ dá»± Ä‘oÃ¡n tá»‘t khÃ´ng khi cÃ³ dá»¯ liá»‡u má»›i vÃ o.

Tiáº¿p theo, ta sáº½ dÃ¹ng *test data* Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh vá»«a xÃ¢y dá»±ng. Káº¿t quáº£ cÃ³ váº» khÃ¡ tuyá»‡t vÃ¬ mÃ´ hÃ¬nh gáº§n nhÆ° theo sÃ¡t Ä‘Æ°á»£c dá»¯ liá»‡u cá»§a *test data*.

```{r}
# Step 6: Make predictions
RNN_forecast <- RNN_model %>% 
  predict(x_test)

# Step 7: Combine predicted and observed
plot_data <- data.frame(
  time = c(min(m3$Date)-days(1),m3$Date),  # Time for the test set
  actual = y_test,  # Actual values from the test set
  forecast = RNN_forecast  # Forecasted values
)

# Step 8: Plot using Highcharts
highchart() %>%
  hc_title(text = "Time Series Forecasting with Highcharts") %>%
  hc_xAxis(
    categories = plot_data$time,
    title = list(text = "Time")
  ) %>%
  hc_yAxis(
    title = list(text = "Value"),
    plotLines = list(list(
      value = 0,
      width = 1,
      color = "gray"
    ))
  ) %>%
  hc_add_series(
    name = "Actual Data",
    data = plot_data$actual,
    type = "line",
    color = "#1f77b4"  # Blue color for actual data
  ) %>%
  hc_add_series(
    name = "Forecast",
    data = plot_data$forecast,
    type = "line",
    color = "#ff7f0e"  # Orange color for forecast data
  ) %>%
  hc_tooltip(
    shared = TRUE,
    crosshairs = TRUE
  ) %>%
  hc_legend(
    enabled = TRUE
  )
```

### MÃ´ hÃ¬nh LSTM:

Tiáº¿p theo, ta sáº½ xÃ¢y dá»±ng thá»­ mÃ´ hÃ¬nh *LSTM*. MÃ´ hÃ¬nh LSTM thÆ°á»ng bao gá»“m cÃ¡c lá»›p sau:

-   Lá»›p LSTM: ÄÃ¢y lÃ  lá»›p chÃ­nh, cÃ³ thá»ƒ cÃ³ má»™t hoáº·c nhiá»u lá»›p LSTM chá»“ng lÃªn nhau. Má»—i lá»›p LSTM cÃ³ thá»ƒ tráº£ vá» toÃ n bá»™ chuá»—i báº±ng `return_sequences = TRUE` hoáº·c chá»‰ tráº£ vá» giÃ¡ trá»‹ cuá»‘i cÃ¹ng báº±ng `return_sequences = FALSE`.

-   Lá»›p Dense: Sau khi thÃ´ng tin Ä‘Æ°á»£c xá»­ lÃ½ qua cÃ¡c lá»›p LSTM, nÃ³ sáº½ Ä‘Æ°á»£c Ä‘Æ°a qua cÃ¡c lá»›p Dense (lá»›p fully connected) Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng.

-   Lá»›p Dropout (tÃ¹y chá»n): Äá»ƒ trÃ¡nh overfitting, cÃ³ thá»ƒ thÃªm lá»›p dropout Ä‘á»ƒ táº¯t ngáº«u nhiÃªn má»™t sá»‘ nÆ¡-ron trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.

```{r}
#| warning: false
#| message: false
#| include: false
# Step 3: Build an enhanced LSTM model
LSTM_model <- function(input_shape) {
  # Define input layer
  inputs <- layer_input(shape = input_shape)
  
  # First LSTM layer (returns the full sequence for the next layer)
  x <- layer_lstm(units = 50, return_sequences = TRUE, activation = 'tanh')(inputs)
  
  # Second LSTM layer (returns the full sequence, could also be 'return_sequences = FALSE' for output prediction)
  x <- layer_lstm(units = 50, return_sequences = FALSE, activation = 'tanh')(x)
  
  # Output layer (prediction for the next time step)
  outputs <- layer_dense(units = 1)(x)
  
  # Create the model
  model <- keras_model(inputs, outputs)
  
  # Return the model
  return(model)
}


# Update input shape for LSTM (timesteps = 12, features = 1)
input_shape <- c(12, 1)

# Reshape the input data for LSTM (add a feature dimension)
x_train <- array_reshape(x_train, dim = c(nrow(x_train), 12, 1))
x_test <- array_reshape(x_test, dim = c(nrow(x_test), 12, 1))
x_val <- array_reshape(x_val, dim = c(nrow(x_val), 12, 1))

# Build the enhanced LSTM model
LSTM_model <- LSTM_model(input_shape)

# Step 4: Compile the model
LSTM_model %>% compile(
  loss = 'mse',
  optimizer = optimizer_adam(),
  metrics = c('mae')
)

# Step 5: Train the model
LSTM_history <- LSTM_model %>% fit(
  x_train, y_train,
  epochs = 50, batch_size = 32,
  validation_data = list(x_val, y_val)
)

LSTM_result <- LSTM_model %>% 
    evaluate(x_test, y_test)
```

Váº­y giá» ta sáº½ so sÃ¡nh vá»›i mÃ´ hÃ¬nh *RNN* trÆ°á»›c vá»›i mÃ´ hÃ¬nh *LSTM* qua 2 thÃ´ng sá»‘ Ä‘Ã£ chá»n *mse* vÃ  *mae*.

```{r}
#| warning: false
#| message: false
# Extract metrics into a data frame
results_df <- data.frame(
  Model = c("RNN", "LSTM"),
  Metric = c("Loss", "metric"),
  MSE = c(RNN_result[[1]],RNN_result[[2]]),
  MAE = c(LSTM_result[[1]], LSTM_result[[2]])
)

library(gt)
# Create a gt table
results_df %>%
  gt() %>%
  tab_header(
    title = "Model Performance Metrics",
    subtitle = "Comparison of MSE and MAE for RNN and LSTM"
  ) %>%
  fmt_number(
    columns = vars(MSE, MAE),
    decimals = 4
  ) %>%
  cols_label(
    Model = "Model Type",
    MSE = "Mean Squared Error",
    MAE = "Mean Absolute Error"
  ) %>%
  tab_options(
    table.font.size = 14,
    heading.title.font.size = 16,
    heading.subtitle.font.size = 14
  )
```

Káº¿t quáº£ cho tháº¥y mÃ´ hÃ¬nh *RNN* truyá»n thá»‘ng Ä‘Æ°a ra káº¿t quáº£ tá»‘t hÆ¡n *LSTM* máº·c dÃ¹ sai sá»‘ cá»§a *LSTM* Ä‘á»u \< 0.03 lÃ  khÃ´ng quÃ¡ tá»‡ nhÆ°ng tiÃªu chÃ­ váº«n lÃ  mÃ´ hÃ¬nh nÃ o hiá»‡u quáº£ nháº¥t.

```{r}
LSTM_forecast <- LSTM_model %>% 
  predict(x_test)

compare<-data.frame(Date = c(min(m3$Date)-days(1),m3$Date),
                    LSTM = round(LSTM_forecast - y_test,3),
                    RNN = round(RNN_forecast - y_test,3)
)

# Create the highchart plot
highchart() %>%
  hc_chart(type = "line") %>%
  hc_title(text = "Residual Comparison: LSTM vs RNN") %>%
  hc_xAxis(
    categories = compare$Date,
    title = list(text = "Date")
  ) %>%
  hc_yAxis(
    title = list(text = "Residuals"),
    plotLines = list(
      list(value = 0, color = "gray", width = 1, dashStyle = "Dash")
    )
  ) %>%
  hc_add_series(
    name = "LSTM Residuals",
    data = compare$LSTM,
    color = "#1f77b4"
  ) %>%
  hc_add_series(
    name = "RNN Residuals",
    data = compare$RNN,
    color = "#ff7f0e"
  ) %>%
  hc_tooltip(shared = TRUE) %>%
  hc_legend(enabled = TRUE)
```

### XÃ¡c Ä‘á»‹nh cáº¥u trÃºc mÃ´ hÃ¬nh:

Náº¿u báº¡n Ä‘á»ƒ Ã½, thá»±c cháº¥t code cho mÃ´ hÃ¬nh cho nhÆ° mÃ¬nh Ä‘Ã£ trÃ¬nh bÃ y thÃ¬ khÃ¡ Ä‘Æ¡n giáº£n vÃ  Ä‘iá»u khÃ³ nháº¥t trong mÃ´ hÃ¬nh lÃ  xÃ¡c Ä‘á»‹nh sá»‘ lá»›p *layer* trong mÃ´ hÃ¬nh. NhÆ° bÃ i toÃ¡n *time series forecasting* thÃ¬ mÃ¬nh chá»‰ cáº§n 2,3 lá»›p layer Ä‘Æ¡n giáº£n lÃ  Ä‘Ã£ Ä‘áº¡t káº¿t quáº£ tá»‘t vá»›i sai sá»‘ ráº¥t tháº¥p (\< 0.03), cÃ²n vá»›i cÃ¡c bÃ i toÃ¡n phá»©c táº¡p hÆ¡n thÃ¬ sá»‘ *layer* sáº½ nhiá»u hÆ¡n.

Váº­y quy táº¯c xÃ¡c Ä‘á»‹nh mÃ´ hÃ¬nh lÃ  nhÆ° tháº¿ nÃ o ? CÃ¢u tráº£ lá»i lÃ  **khÃ´ng cÃ³ quy táº¯c nÃ o cáº£** vÃ  chá»‰ cÃ³ cÃ¡c tips mÃ  mÃ¬nh lá»¥m nháº·t trÃªn máº¡ng nhÆ° sau:

-   *Number of layer* nÃªn náº±m giá»¯a sá»‘ input vÃ  sá»‘ output. NhÆ° bÃ i thá»±c hÃ nh trÃªn thÃ¬ sá»‘ *layer* nÃªn náº±m trong khoáº£ng (1,12). Hoáº·c báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng hÃ m dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ xÃ¡c Ä‘á»‹nh.

$$
N_h = \frac{N_s}{\alpha \cdot (N_i + N_o)}
$$

Vá»›i cÃ¡c tham sá»‘ gá»“m:

1.  $N_h$ lÃ  sá»‘ lÆ°á»£ng *hidden neurons*.

2.  $N_s$ lÃ  sá»‘ lÆ°á»£ng máº«u trong *training data*.

3.  $\alpha$ lÃ  yáº¿u tá»‘ tá»· lá»‡ tÃ¹y Ã½ (thÆ°á»ng tá»« 2-10).

4.  $N_i$ lÃ  sá»‘ lÆ°á»£ng nÆ¡-ron input

5.  $N_o$ lÃ  sá»‘ lÆ°á»£ng nÆ¡-ron output.

- CÃ¡c hÃ m *acvtivation* nhÆ° **Tanh** thÃ¬ phÃ¹ há»£p cho dá»± bÃ¡o giÃ¡ trá»‹ liÃªn tá»¥c tá»« dá»¯ liá»‡u chuá»—i, **ReLU** giÃºp cho quÃ¡ trÃ¬nh training nhanh hÆ¡n vÃ  khÃ´ng gÃ¢y ra *vanishing problem* do khÃ´ng bá»‹ cháº·n, **Softmax** thÆ°á»ng dÃ¹ng á»Ÿ *final layer* cho bÃ i toÃ¡n classification, **Sigmoid** thÆ°á»ng dÃ¹ng cho há»“i quy logic.

- *Number of neurons*: Sá»‘ lÆ°á»£ng nÆ¡-ron trong má»™t lá»›p quyáº¿t Ä‘á»‹nh lÆ°á»£ng thÃ´ng tin mÃ  máº¡ng cÃ³ thá»ƒ lÆ°u trá»¯. Nhiá»u nÆ¡-ron giÃºp máº¡ng há»c Ä‘Æ°á»£c cÃ¡c máº«u phá»©c táº¡p hÆ¡n, nhÆ°ng cÅ©ng lÃ m tÄƒng nguy cÆ¡ overfitting (quÃ¡ khá»›p) vÃ  yÃªu cáº§u nhiá»u tÃ i nguyÃªn tÃ­nh toÃ¡n hÆ¡n. Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u vá»›i má»™t sá»‘ lÆ°á»£ng nÆ¡-ron tÆ°Æ¡ng Ä‘á»‘i nhá», nhÆ° 128 hoáº·c 256...

## Káº¿t luáº­n:

NhÆ° váº­y, chÃºng ta Ä‘Ã£ Ä‘Æ°á»£c há»c vá» mÃ´ hÃ¬nh *RNN* vÃ  *LSTM* vÃ  cÃ¡ch xÃ¢y dá»±ng chÃºng trong R.

Náº¿u báº¡n cÃ³ cÃ¢u há»i hay tháº¯c máº¯c nÃ o, Ä‘á»«ng ngáº§n ngáº¡i liÃªn há»‡ vá»›i mÃ¬nh qua Gmail. BÃªn cáº¡nh Ä‘Ã³, náº¿u báº¡n muá»‘n xem láº¡i cÃ¡c bÃ i viáº¿t trÆ°á»›c Ä‘Ã¢y cá»§a mÃ¬nh, hÃ£y nháº¥n vÃ o hai nÃºt dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ truy cáº­p trang **Rpubs** hoáº·c mÃ£ nguá»“n trÃªn **Github**. Ráº¥t vui Ä‘Æ°á»£c Ä‘á»“ng hÃ nh cÃ¹ng báº¡n, háº¹n gáº·p láº¡i! ğŸ˜„ğŸ˜„ğŸ˜„

```{=html}
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Contact Me</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/simple-icons@v6.0.0/svgs/rstudio.svg">
    <style>
        body { font-family: Arial, sans-serif; background-color: $secondary-color; }
        .container { max-width: 400px; margin: auto; padding: 20px; background: white; border-radius: 8px; box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1); }
        label { display: block; margin: 10px 0 5px; }
        input[type="email"] { width: 100%; padding: 10px; margin-bottom: 15px; border: 1px solid #ccc; border-radius: 4px; }
        .github-button, .rpubs-button { margin-top: 20px; text-align: center; }
        .github-button button, .rpubs-button button { background-color: #333; color: white; border: none; padding: 10px; cursor: pointer; border-radius: 4px; width: 100%; }
        .github-button button:hover, .rpubs-button button:hover { background-color: #555; }
        .rpubs-button button { background-color: #75AADB; }
        .rpubs-button button:hover { background-color: #5A9BC2; }
        .rpubs-icon { margin-right: 5px; width: 20px; vertical-align: middle; filter: brightness(0) invert(1); }
        .error-message { color: red; font-size: 0.9em; margin-top: 5px; }
    </style>
</head>
<body>
    <div class="container">
        <h2>Contact Me</h2>
        <form id="emailForm">
            <label for="email">Your Email:</label>
            <input type="email" id="email" name="email" required aria-label="Email Address">
            <div class="error-message" id="error-message" style="display: none;">Please enter a valid email address.</div>
            <button type="submit">Send Email</button>
        </form>
        <div class="github-button">
            <button>
                <a href="https://github.com/Loccx78vn/RNN_model" target="_blank" style="color: white; text-decoration: none;">
                    <i class="fab fa-github"></i> View Code on GitHub
                </a>
            </button>
        </div>
        <div class="rpubs-button">
            <button>
                <a href="https://rpubs.com/loccx" target="_blank" style="color: white; text-decoration: none;">
                    <img src="https://cdn.jsdelivr.net/npm/simple-icons@v6.0.0/icons/rstudio.svg" alt="RStudio icon" class="rpubs-icon"> Visit my RPubs
                </a>
            </button>
        </div>
    </div>

    <script>
        document.getElementById('emailForm').addEventListener('submit', function(event) {
            event.preventDefault(); // Prevent default form submission
            const emailInput = document.getElementById('email');
            const email = emailInput.value;
            const errorMessage = document.getElementById('error-message');

            // Simple email validation regex
            const emailPattern = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;

            if (emailPattern.test(email)) {
                errorMessage.style.display = 'none'; // Hide error message
                const yourEmail = 'loccaoxuan103@gmail.com'; // Your email
                const gmailLink = `https://mail.google.com/mail/?view=cm&fs=1&to=${yourEmail}&su=Help%20Request%20from%20${encodeURIComponent(email)}`;
                window.open(gmailLink, '_blank'); // Open in new tab
            } else {
                errorMessage.style.display = 'block'; // Show error message
            }
        });
    </script>
</body>
</html>
```
